pwc_paper_url,pwc_paper_arxiv_id,pwc_paper_url_abs,pwc_paper_url_pdf,pwc_repo_url,pwc_mentioned_in_paper,pwc_mentioned_in_github,pwc_abstract,pwc_proceeding,pwc_authors,pwc_tasks,pwc_date,pwc_methods,s2ag_id,s2ag_authors,s2ag_inCitations,s2ag_outCitations,s2ag_year,s2ag_s2Url,s2ag_sources,s2ag_pdfUrls,s2ag_venue,s2ag_fieldOfStudy,s2ag_magId,GHT_pj_id,GHT_url,GHT_owner_id,GHT_name,GH_description,GHT_language,GHT_created_at,GHT_forked_from,GHT_delete,GHT_updated_at,GHT_forked_commit_id,GHUrl,GHT_total_star,gh_api_forks,gh_api_repo_language,gh_api_repo_created_at,gh_api_repo_pushed_at,gh_api_open_issues,gh_api_has_issues,gh_api_has_wiki,gh_api_homepage_url,gh_api_subscribers,gh_api_topics,gh_api_readme,gh_api_commit,gh_api_repo_age,gh_api_repo_last_push,gh_api_user_followers,gh_api_account_age,gh_api_user_created_at,gh_api_repo_url,gh_api_repo_networkCount,gh_api_repo_size,gh_api_repo_is_fork,gh_api_repo_mirror,gh_api_repo_contributors,gh_api_repo_num_tags,gh_api_repo_num_release,gh_api_user_account_type,gh_api_user_company,gh_api_user_email,gh_api_user_twitter_username,gh_api_user_public_repo,gh_api_user_updated_at,gh_api_repo_lisence,gh_api_pull_request,number_of_year_repo,number_of_year_paper,int_number_of_year_repo,int_number_of_year_paper,GHT_repo_name,pwc_framework,readme_tag,yuanrui_num_list,yuanrui_num_code_blk,yuanrui_num_inline_code,yuanrui_num_img,yuanrui_num_ani_img,yuanrui_has_video,yuanrui_num_table,yuanrui_num_ghlink,yuanrui_has_project_page,yuanrui_has_license,yuanrui_has_model,yuanrui_num_module,yuanrui_num_root_file,yuanrui_has_data,yuanrui_contain_docker,yuanrui_has_shell,yuanrui_num_code_file,yuanrui_num_code_line,yuanrui_num_copy_paste,repo_readme_length,repo_description_length,user_company_bool,user_email_bool,gh_api_homepage_url_bool,pwc_paper_title,class,title_paper,Total_Authors,AllAuthor_PaperMin,AllAuthor_PaperMax,AllAuthor_PaperMean,AllAuthor_PaperMedian,AllAuthor_PaperTotal,AllAuthor_PaperStd,AllAuthor_CitationTotal,AllAuthor_CitationMin,AllAuthor_CitationMax,AllAuthor_CitationMean,AllAuthor_CitationMedian,AllAuthor_CitationStd,AllAuthor_HindexTotal,AllAuthor_HindexMin,AllAuthor_HindexMax,AllAuthor_HindexMean,AllAuthor_HindexMedian,AllAuthor_HindexStd,HighestHindexAuthor_Paper,HighestHindexAuthor_Citation,HighestHindexAuthor_Hindex,HighestHindexAuthor_haveAffiliation,HighestHindexAuthor_haveHomepage,FirstAuthor_Paper,FirstAuthor_Citation,FirstAuthor_Hindex,FirstAuthor_haveAffiliation,FirstAuthor_haveHomepage,paper_before_repo,paper_equal_repo,gh_api_star,total_ref,total_cite,total_tasks,total_methods,total_fieldOfStudy,total_sources,total_s2ag_pdfUrls,FirstAuthor_TotalAffiliation,HighestHindexAuthor_TotalAffiliation,conf_rank,title_length,abstract_length
https://paperswithcode.com/paper/robust-lane-detection-from-continuous-driving,1903.02193,https://arxiv.org/abs/1903.02193v2,https://arxiv.org/pdf/1903.02193v2.pdf,https://github.com/qinnzou/Robust-Lane-Detection,TRUE,FALSE,"Lane detection in driving scenes is an important module for autonomous vehicles and advanced driver assistance systems. In recent years, many sophisticated lane detection methods have been proposed. However, most methods focus on detecting the lane from one single image, and often lead to unsatisfactory performance in handling some extremely-bad situations such as heavy shadow, severe mark degradation, serious vehicle occlusion, and so on. In fact, lanes are continuous line structures on the road. Consequently, the lane that cannot be accurately detected in one current frame may potentially be inferred out by incorporating information of previous frames. To this end, we investigate lane detection by using multiple frames of a continuous driving scene, and propose a hybrid deep architecture by combining the convolutional neural network (CNN) and the recurrent neural network (RNN). Specifically, information of each frame is abstracted by a CNN block, and the CNN features of multiple continuous frames, holding the property of time-series, are then fed into the RNN block for feature learning and lane prediction. Extensive experiments on two large-scale datasets demonstrate that, the proposed method outperforms the competing methods in lane detection, especially in handling difficult situations.",no proceeding,"['Qin Zou', 'Hanwen Jiang', 'Qiyu Dai', 'Yuanhao Yue', 'Long Chen', 'Qian Wang']","['Autonomous Vehicles', 'Frame', 'Lane Detection', 'Time Series']",2019-03-06,[],13a3b8c5272033ef8d5f2b5c83016995256b2f34,"[{'name': 'Qin  Zou', 'ids': ['144117000']}, {'name': 'Hanwen  Jiang', 'ids': ['29640859']}, {'name': 'Qiyu  Dai', 'ids': ['48382102']}, {'name': 'Yuanhao  Yue', 'ids': ['79850318']}, {'name': 'Long  Chen', 'ids': ['1737625']}, {'name': 'Qian  Wang', 'ids': ['47599321']}]","['0b2a95b27d360cd104bd58e1eadf601cf00ae63a', 'd9c4f420e62077b6606394915145485d428ec362', '3ef29ddefc6c70b211a660597f809648fc0caa91', 'cbad19f5448484b62bd6c9cbb1f68ef74805f773', '77ff6dd8e559b873f7014d9e0ed70b96b32b5c37', '00b0f67a932ca1d71206063cce86c6acf82b31d8', '82479111ace12bedfa0a3d95cdefed0a96a5913e', '06c9166ec006bbfb198cd5e83e905494ba2d0edd', '6645054b2cb68fd7290584144a1493f270dc26de', 'd728744fd9cf8a555652465a21a39f511d062d93', '0f2afb2554b8a02db1b7cdc4e43dfbd994008b56', '6cbb393976d870f62636141a46edde42a58a0abe', '19d0d5aae45d6fd6ab2b0c89d15ca7784df05879', '878c60709fe2a10522f179a1e5ca7668b728a3be', 'a43cb252a96e0257b2e288f86943f10bac8c73fb', '5a0ae6907d766fc260475899289cf6aea3da7745', 'd59a11e3cccb87e858c6aaff198435c6e715c868', '208fc501f15d46246461356ca9a081c0450d33a7', 'e2711c2a32b6ed2ab4b070e25aa7e40ea81e69ae', '6132a5251596e158bcd324165d851d25f87745e6', '726574a544b9814188a05a4f62ae172f6f7a3a42', 'd1450a6f033ab6cac79ef8d74b823d5e93db5e63', '7935ed78260d54031102e80fa97409b8b6b3efa7', 'a1d002f1fc221c8b696261fe79745872f6fe0ab6', '15e8801290c1a0208fe7e2df465974bd6eed464e', 'c0e6cd43a81353993544924cd09ca87a75c4e348', '67b3c369784ba31b0864a1f42a6971388298a939', 'a832f5aabb3e24c13458c6fccf87bce15dcd8d13', 'f1289d5a37813cddc4641bf092b754e8755b451b', '408d5cf82ac34c04ac45159f5b1d35c12044e034', '87f29ceb16d6b7a400497ec4424e7ae5f4b0608c', '5252664b5d701c9bea7c300ac84b2e7333000724', '2617fc0fac04d46b5194b86a2d6ec3039ff5b30d', '62b253f1bf110b0ce2505d6ff3fc1e6a9a0fc063', 'd5526f5887a57255611fbb500a0c1f9520778713', '055b13b2bc5a92db922b29a4e94dc8e6589de644', 'e4b42cd730856b563eb676a586cd939e6bc95393', 'cd67bc3dcd818653566ac0982ab0cdd39fa6a54e', 'b40fad6930072b2ed1338de1c438aee43b48325d', '18ecfcc57a082dee90b8ccabdfafcc0e1fb41cb7', '845afef200c039ef0ace775531b019f228033249', '0643e3c2925602ac292e6171b8920551786ae3c5', '5f8c3c22b30fb7a53972ad57594a01ad678d96e3', '1387eeb54f486d1869c7a794d1b32677955672f7', '033617e5e2d52bc0fe772bf2f5bd1df5a118cde1', '3e0422c5a512266a3c8a02b9fcad682ea17cdf87', 'c763eac73514993f091749cfd8b6aad78ca471f7', '367d679a8df4f793c89503e9dea8a3cfceea918d', '85497f7aba2975fb3388af3ead0337a519985417', '37ef8af3914a6814dd37ea84eaeb1ee81dc6cfb4']","['3bc6e1ce39744bd1602fad5c895d63b064c1b643', 'f8e79ac0ea341056ef20f2616628b3e964764cfd', '1fdcf14b2908891f9f1ef0751854fcd7ce9bb7ed', 'd0816d347ac07affe0f39d5d3760e4f47aca3cd5', '2937ffeef1918988485040cad447d4fc9d0462a4', '6364fdaa0a0eccd823a779fcdd489173f938e91a', 'eb42cf88027de515750f230b23b1a057dc782108', '23fe2ab6aa5f0a19e5dd6fabe1a589c8bcb67808', '12806c298e01083a79db77927530367d85939907', 'b9a882c50d99dd7495ceedf113bd7a280a1ac50a', '060d17310b475281a402e9f3e9f99406fb52332b', '44a3e466743add34a34567b4566ebe6c98bd2abf', 'b2807613304c0ee8dcedf0a78d448b95f33816be', 'd2c733e34d48784a37d717fe43d9e93277a8c53e', 'ce7b30d68015d09359c8106ca3eda165f2a9b893', '9915efb95d14d3fb45bd9834982b15dda5d4abc8', '627850c947c662141da305006f6c2bff603de8b7', '1dfe35869c4cdb41cc1bd2c622d38d57ef8e310f', 'db949f62f542b467bda4f5fdec902dccc6c351a4', 'a0b6408f7ec79d7f58a8b7458e208d32b9751a35', 'f054fdd7a36b569eae7627cf12a4d81322dea022', 'd91af9cc0d29b9604cf66c2c178991f259f1a7b7', 'f9c990b1b5724e50e5632b94fdb7484ece8a6ce7', '68c08ea30baa67e8fdb01f2723eb246db16fa6f2', '2be2a01b2cf4afcceb0e209172a9866b47526b9a', '1933198c3a7039da45b4bc29c3f4d10479dd3ac7', '25147f7e93c9d19ea667fd4a447c60bc197dd5b0', '7a89447be0a176368926f1ef108512f4df5e27be', 'b0c065cd43aa7280e766b5dcbcc7e26abce59330', '48a7d3321c839dcfb9cd0d751ef6c6638a5a8295', 'f5b7cba608e39b8090235f697b0ca7949fec8ac7', 'abd2a82e0b2a41e5db311e66b53f8e8f947c9710', '54e325aee6b2d476bbbb88615ac15e251c6e8214', '38090539e13e9e31e2a7dead9c2fc1ef6d4b63dc', '0fbab78360f2206ffba51a067bb5834d00d12992', '14d037b6c84df142a3f60e21a4afab834d947885', '7ffdbc358b63378f07311e883dddacc9faeeaf4b', '421f7e6d1f09e01ae87528af00e19f05cccb5173', '97db4d2bac016467ccbf67f48b3b1b3602c02c22', '2f093eccdae54b5d38ce69b02385a3098027cafb', 'fd67eed4db579700431b5cca62a5e0d8893d428a', '35db8377fab6501e9cf8b7da53854aee8a48e59f', '87a1273dea59e3748372c5ea69488d70e9125046', '26ceceb2f571ff8e0230755d7fd18f804da2a4bd', '57f82fe861c4cb6a2ce543d4c8cd928b25f785bb', '4152d2c8585f7e3f85d3b3d84036171de104cbd7', '7e89f443ea5ef880f293acd00908f880b7e04541', 'c0843219abdee503c65e49b8f544cf3d2c0c535c', 'fdfb56addd85c4c1a6f4bdd95079ace947fc86fa', '188bc9818c72e4ae09b74524cf95547ef32d1bfb', '3cd7b551ce6e96298429060506732478a58a2478', '22d046e926d3ed12d00063b8668cb11fed26e1f1', 'b8375ff50b8a6f1a10dd809129a18df96888ac8b', '7b554bf1e70dab620fd69e504d84701992ad7945', '8b2ea3ecac8abd2357fbf8ca59ec31bad3191388', 'fb89968fe37fb574339cf5f229491b8cf78ed187', 'e6c40fd5adf616b31766f7578fd8c18e18158b78', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', '843654a15c988f9930bf7c6f9d6f0c57fb2188e9', 'dc0c7e131e31ab98e383e708ecb64f0aba1de30f', 'fd2407a8fbe9867b74a61ccc9b34834d54f2c4ac', '5383ee00a5c0c3372081fccfeceb812b7854c9ea', '9c373b1a7d3a987ceca9d33919725ec4bb290683', '6917799fc9bffed1edfc50adddaa270ae5a1daee', '3cccd1f635f89e1e30e598dec96680ac9d08daac', '34d7c6428bd6d6b109f88ac4a6c71553a4a0f489', 'ced5c8fc3e1dd7d96989718e8787c4cdf1953ccf', '3dfe695a0698ab3a123f5caa7a55960bacd17966', 'ea99a5535388196d0d44be5b4d7dd02029a43bb2']",2020,https://semanticscholar.org/paper/13a3b8c5272033ef8d5f2b5c83016995256b2f34,['DBLP'],"['https://arxiv.org/pdf/1903.02193v2.pdf', 'http://export.arxiv.org/pdf/1903.02193', 'http://arxiv.org/abs/1903.02193', 'https://doi.org/10.1109/TVT.2019.2949603']",IEEE Transactions on Vehicular Technology,['Computer Science'],2981992989,131669815,https://api.github.com/repos/qinnzou/Robust-Lane-Detection,21040503,Robust-Lane-Detection,\N,\N,2019-04-07 10:20:15,\N,0,2021-03-03 9:14:39,\N,https://github.com/qinnzou/Robust-Lane-Detection,96,54,Python,2019-04-07 10:20:15,2020-12-17 9:57:43,20,TRUE,TRUE,no homepageUrl,10,"['continuous-driving-scenes', 'lane-detection', 'dataset', 'deep-neural-networks', 'deep-learning', 'lstm', 'cnn']","# Robust Lane Detection From Continuous Driving Scenes Using Deep Neural Networks
This is the source code of Robust Lane Detection from Continuous Driving Scenes Using Deep Neural Networks. We provide the dataset and the pretrained model.

Zou Q, Jiang H, Dai Q, Yue Y, Chen L and Wang Q, Robust Lane Detection from Continuous Driving Scenes Using Deep Neural Networks, IEEE Transactions on Vehicular Technology, 2019.

# Network Architecture
![image](https://github.com/qinnzou/Robust-Lane-Detection/blob/master/LaneDetectionCode/save/result/network.png)
# Some Results
![image](https://github.com/qinnzou/Robust-Lane-Detection/blob/master/LaneDetectionCode/save/result/1_data.jpg)
![image](https://github.com/qinnzou/Robust-Lane-Detection/blob/master/LaneDetectionCode/save/result/2_data.jpg)
![image](https://github.com/qinnzou/Robust-Lane-Detection/blob/master/LaneDetectionCode/save/result/3_data.jpg)
![image](https://github.com/qinnzou/Robust-Lane-Detection/blob/master/LaneDetectionCode/save/result/1_pred.jpg)
![image](https://github.com/qinnzou/Robust-Lane-Detection/blob/master/LaneDetectionCode/save/result/2_pred.jpg)
![image](https://github.com/qinnzou/Robust-Lane-Detection/blob/master/LaneDetectionCode/save/result/3_pred.jpg)

# tvtLANE Dataset
## Description:
This dataset contains 19383 image sequences for lane detection, and 39460 frames of them are labeled. These images were divided into two parts, a training dataset contains 9548 labeled images and augmented by four times, and a test dataset has 1268 labeled images. The size of images in this dataset is 128*256.
+ Training set:
   - Data augmentation:
The training set is augmented. By flipping and rotating the images in three degree, the data volume is quadruple. These augmented data are separated from the original training set, which is name as “origin”. “f” and “3d” after “-” are represent for flipping and rotation. Namely, the “origin- 3df” folder is the rotated and flipped training set.
   - Data construction:
The original training set contains continuous driving scenes images, and they are divided into images sequences by every twenty. All images are contained in “clips_all”, and there are 19096 sequences for training. Each 13th and 20th frame in a sequence are labeled, and the 38192 image and their labels are in “clips_13(_truth)” and “clips_20(_truth)”.
The original training dataset has two parts. Sequences in “0313”, “0531” and “0601” subfolders are constructed on TuSimple lane detection dataset, containing scenes in American highway. The four “weadd” folders are added images in rural road in China.
+ Test set:
   - Testset #1:
The normal testset, named Testset #1, is used for testing the overall performance of algorithms. Sequences in “0530”, “0531” and “0601” subfolders are constructed on TuSimple lane dataset. 270 sequences are contained, and each 13th and 20th image is labeled.
   - Testset #2:
The Testset #2 is used for testing the robustness of algorithms. 12 kinds of hard scenes for human eyes are contained. All frames are labeled.
## Using:
Index are contained. For detecting lanes in continuous scenes, the input size is 5 in our paper. Thus, the former images are additional information to predict lanes in the last frame, and the last frame is the labeled one.
We use different sampling strides to get 5 continuous images, as shown below. Each row in the index represents for a sequence and its label for training.![image](https://github.com/qinnzou/Robust-Lane-Detection/blob/master/LaneDetectionCode/save/result/lane3.png)

## Download:
You can download this **dataset** from the link in the '**Dataset-Description-v1.2.pdf**' file.  
BaiduYun：  
https://pan.baidu.com/s/1lE2CjuFa9OQwLIbi-OomTQ 
passcodes：tf9x Or  
Google Drive:  
https://drive.google.com/drive/folders/1MI5gMDspzuV44lfwzpK6PX0vKuOHUbb_?usp=sharing   

You can also download the **pretrained model** from the following link,  
https://pan.baidu.com/s/1DXy6klAHNmsIjq_g83CGiQ or   
https://drive.google.com/drive/folders/1BHbYU3AoU5i2mduMtfUkFHVuYObA_fWt?usp=sharing

All pretrained models can be download at https://pan.baidu.com/s/1u9ABAPouZL2nIMT4E7uAQw   
passcodes：w30b

# Set up
## Requirements
PyTorch 0.4.0  
Python 3.6  
CUDA 8.0  
We run on the Intel Core Xeon E5-2630@2.3GHz, 64GB RAM and two GeForce GTX TITAN-X GPUs.

## Preparation
### Data Preparation
Our dataset contains 19383 continuous driving scenes image sequences, and 39460 frames of them are labeled. The size of images is 128*256. 
The training set contains 19096 image sequences. Each 13th and 20th frame in a sequence are labeled, and the image and their labels are in “clips_13(_truth)” and “clips_20(_truth)”. All images are contained in “clips_all”.  
Sequences in “0313”, “0531” and “0601” subfolders are constructed on TuSimple lane detection dataset, containing scenes in American highway. The four “weadd” folders are added images in rural road in China.  
The testset has two parts: Testset #1 (270 sequences, each 13th and 20th image is labeled) for testing the overall performance of algorithms. The Testset #2 (12 kinds of hard scenes, all frames are labeled) for testing the robustness of algorithms.   
To input the data, we provide three index files(train_index, val_index, and test_index). Each row in the index represents for a sequence and its label, including the former 5 input images and the last ground truth (corresponding to the last frame of 5 inputs).
Our dataset can be downloaded and put into ""./LaneDetectionCode/data/"". If you want to use your own data, please refer to the format of our dataset and indexs.

### Pretrained Models
Pretrained models on PyTorch are available using links in the Download part, including the propoesd models(SegNet-ConvLSTM, UNet-ConvLSTM) as well as the comparable two(SegNet, UNet)  
You can download them and put them into ""./LaneDetectionCode/pretrained/"".

## Training
Before training, change the paths including ""train_path""(for train_index.txt), ""val_path""(for val_index.txt), ""pretrained_path"" in config.py to adapt to your environment.  
Choose the models(SegNet-ConvLSTM, UNet-ConvLSTM or SegNet, UNet) and adjust the arguments such as class weights, batch size, learning rate in config.py.  
Then simply run:  
```
python train.py
```

## Test
To evlauate the performance of a pre-trained model, please put the pretrained model listed above or your own models into ""./LaneDetectionCode/pretrained/"" and change ""pretrained_path"" in config.py at first, then change ""test_path"" for test_index.txt, and ""save_path"" for the saved results.   
Choose the right model that would be evlauated, and then simply run:  
```
python test.py
```
The quantitative evaluations of Accuracy, Precision, Recall and  F1 measure would be printed, and the result pictures will be save in ""./LaneDetectionCode/save/result/"".  
We have put five images sequences in the ""./LaneDetectionCode/data/testset"" with test_index_demo.txt on UNet-ConvLSTM for demo. You can run test.py directly to check the performance.

# Citation:
Please cite our paper if you use this code or data in your own work:
```
@article{zou2019tvt,
  title={Robust lane detection from continuous driving scenes using deep neural networks},
  author={Q. Zou and H. Jiang and Q. Dai and Y. Yue and L. Chen and Q. Wang},
  journal={IEEE Transactions on Vehicular Technology},
  volume={69},
  number={1},
  pages={41--54},
  year={2020},
}
```
# Copy Right:
This dataset was collected for academic research only.  
# Contact: 
For any problem about this dataset, please contact Dr. Qin Zou (qzou@whu.edu.cn).
",72,165,76,27,315,2016-05-22 1:20:08,https://github.com/qinnzou/Robust-Lane-Detection,54,2455,FALSE,no mirror url,1,0,0,User,no company,no email or private email,no twitter_username or private twitter_username,11,2022-06-09 11:18:20,no License,0,3.163841204,3.252633524,3,3,Robust-Lane-Detection,pytorch,"<h1>robust lane detection from continuous driving scenes using deep neural networks</h1>
<p>this is the source code of robust lane detection from continuous driving scenes using deep neural networks. we provide the dataset and the pretrained model.</p>
<p>zou q, jiang h, dai q, yue y, chen l and wang q, robust lane detection from continuous driving scenes using deep neural networks, ieee transactions on vehicular technology, 2019.</p>
<h1>network architecture</h1>
<p><img src=""https://github.com/qinnzou/robust-lane-detection/blob/master/lanedetectioncode/save/result/network.png"" alt=""image""></p>
<h1>some results</h1>
<p><img src=""https://github.com/qinnzou/robust-lane-detection/blob/master/lanedetectioncode/save/result/1_data.jpg"" alt=""image"">
<img src=""https://github.com/qinnzou/robust-lane-detection/blob/master/lanedetectioncode/save/result/2_data.jpg"" alt=""image"">
<img src=""https://github.com/qinnzou/robust-lane-detection/blob/master/lanedetectioncode/save/result/3_data.jpg"" alt=""image"">
<img src=""https://github.com/qinnzou/robust-lane-detection/blob/master/lanedetectioncode/save/result/1_pred.jpg"" alt=""image"">
<img src=""https://github.com/qinnzou/robust-lane-detection/blob/master/lanedetectioncode/save/result/2_pred.jpg"" alt=""image"">
<img src=""https://github.com/qinnzou/robust-lane-detection/blob/master/lanedetectioncode/save/result/3_pred.jpg"" alt=""image""></p>
<h1>tvtlane dataset</h1>
<h2>description:</h2>
<p>this dataset contains 19383 image sequences for lane detection, and 39460 frames of them are labeled. these images were divided into two parts, a training dataset contains 9548 labeled images and augmented by four times, and a test dataset has 1268 labeled images. the size of images in this dataset is 128*256.</p>
<ul>
<li>training set:<ul>
<li>data augmentation:
the training set is augmented. by flipping and rotating the images in three degree, the data volume is quadruple. these augmented data are separated from the original training set, which is name as “origin”. “f” and “3d” after “-” are represent for flipping and rotation. namely, the “origin- 3df” folder is the rotated and flipped training set.</li>
<li>data construction:
the original training set contains continuous driving scenes images, and they are divided into images sequences by every twenty. all images are contained in “clips_all”, and there are 19096 sequences for training. each 13th and 20th frame in a sequence are labeled, and the 38192 image and their labels are in “clips_13(_truth)” and “clips_20(_truth)”.
the original training dataset has two parts. sequences in “0313”, “0531” and “0601” subfolders are constructed on tusimple lane detection dataset, containing scenes in american highway. the four “weadd” folders are added images in rural road in china.</li>
</ul>
</li>
<li>test set:<ul>
<li>testset #1:
the normal testset, named testset #1, is used for testing the overall performance of algorithms. sequences in “0530”, “0531” and “0601” subfolders are constructed on tusimple lane dataset. 270 sequences are contained, and each 13th and 20th image is labeled.</li>
<li>testset #2:
the testset #2 is used for testing the robustness of algorithms. 12 kinds of hard scenes for human eyes are contained. all frames are labeled.
## using:
index are contained. for detecting lanes in continuous scenes, the input size is 5 in our paper. thus, the former images are additional information to predict lanes in the last frame, and the last frame is the labeled one.
we use different sampling strides to get 5 continuous images, as shown below. each row in the index represents for a sequence and its label for training.<img src=""https://github.com/qinnzou/robust-lane-detection/blob/master/lanedetectioncode/save/result/lane3.png"" alt=""image""></li>
</ul>
</li>
</ul>
<h2>download:</h2>
<p>you can download this <strong>dataset</strong> from the link in the '<strong>dataset-description-v1.2.pdf</strong>' file.<br>
baiduyun：<br>
<a href=""https://pan.baidu.com/s/1le2cjufa9oqwlibi-oomtq"">https://pan.baidu.com/s/1le2cjufa9oqwlibi-oomtq</a> 
passcodes：tf9x or<br>
google drive:<br>
<a href=""https://drive.google.com/drive/folders/1mi5gmdspzuv44lfwzpk6px0vkuohubb_?usp=sharing"">https://drive.google.com/drive/folders/1mi5gmdspzuv44lfwzpk6px0vkuohubb_?usp=sharing</a></p>
<p>you can also download the <strong>pretrained model</strong> from the following link,<br>
<a href=""https://pan.baidu.com/s/1dxy6klahnmsijq_g83cgiq"">https://pan.baidu.com/s/1dxy6klahnmsijq_g83cgiq</a> or<br>
<a href=""https://drive.google.com/drive/folders/1bhbyu3aou5i2mdumtfukfhvuyoba_fwt?usp=sharing"">https://drive.google.com/drive/folders/1bhbyu3aou5i2mdumtfukfhvuyoba_fwt?usp=sharing</a></p>
<p>all pretrained models can be download at <a href=""https://pan.baidu.com/s/1u9abapouzl2nimt4e7uaqw"">https://pan.baidu.com/s/1u9abapouzl2nimt4e7uaqw</a><br>
passcodes：w30b</p>
<h1>set up</h1>
<h2>requirements</h2>
<p>pytorch 0.4.0<br>
python 3.6<br>
cuda 8.0<br>
we run on the intel core xeon e5-2630@2.3ghz, 64gb ram and two geforce gtx titan-x gpus.</p>
<h2>preparation</h2>
<h3>data preparation</h3>
<p>our dataset contains 19383 continuous driving scenes image sequences, and 39460 frames of them are labeled. the size of images is 128*256. 
the training set contains 19096 image sequences. each 13th and 20th frame in a sequence are labeled, and the image and their labels are in “clips_13(_truth)” and “clips_20(_truth)”. all images are contained in “clips_all”.<br>
sequences in “0313”, “0531” and “0601” subfolders are constructed on tusimple lane detection dataset, containing scenes in american highway. the four “weadd” folders are added images in rural road in china.<br>
the testset has two parts: testset #1 (270 sequences, each 13th and 20th image is labeled) for testing the overall performance of algorithms. the testset #2 (12 kinds of hard scenes, all frames are labeled) for testing the robustness of algorithms.<br>
to input the data, we provide three index files(train_index, val_index, and test_index). each row in the index represents for a sequence and its label, including the former 5 input images and the last ground truth (corresponding to the last frame of 5 inputs).
our dataset can be downloaded and put into ""./lanedetectioncode/data/"". if you want to use your own data, please refer to the format of our dataset and indexs.</p>
<h3>pretrained models</h3>
<p>pretrained models on pytorch are available using links in the download part, including the propoesd models(segnet-convlstm, unet-convlstm) as well as the comparable two(segnet, unet)<br>
you can download them and put them into ""./lanedetectioncode/pretrained/"".</p>
<h2>training</h2>
<p>before training, change the paths including ""train_path""(for train_index.txt), ""val_path""(for val_index.txt), ""pretrained_path"" in config.py to adapt to your environment.<br>
choose the models(segnet-convlstm, unet-convlstm or segnet, unet) and adjust the arguments such as class weights, batch size, learning rate in config.py.<br>
then simply run:</p>
<pre><code>python train.py
</code></pre>
<h2>test</h2>
<p>to evlauate the performance of a pre-trained model, please put the pretrained model listed above or your own models into ""./lanedetectioncode/pretrained/"" and change ""pretrained_path"" in config.py at first, then change ""test_path"" for test_index.txt, and ""save_path"" for the saved results.<br>
choose the right model that would be evlauated, and then simply run:</p>
<pre><code>python test.py
</code></pre>
<p>the quantitative evaluations of accuracy, precision, recall and  f1 measure would be printed, and the result pictures will be save in ""./lanedetectioncode/save/result/"".<br>
we have put five images sequences in the ""./lanedetectioncode/data/testset"" with test_index_demo.txt on unet-convlstm for demo. you can run test.py directly to check the performance.</p>
<h1>citation:</h1>
<p>please cite our paper if you use this code or data in your own work:</p>
<pre><code>@article{zou2019tvt,
  title={robust lane detection from continuous driving scenes using deep neural networks},
  author={q. zou and h. jiang and q. dai and y. yue and l. chen and q. wang},
  journal={ieee transactions on vehicular technology},
  volume={69},
  number={1},
  pages={41--54},
  year={2020},
}
</code></pre>
<h1>copy right:</h1>
<p>this dataset was collected for academic research only.</p>
<h1>contact:</h1>
<p>for any problem about this dataset, please contact dr. qin zou (qzou@whu.edu.cn).</p>
",3,3,0,0,0,FALSE,0,0,FALSE,FALSE,TRUE,1,4,TRUE,FALSE,FALSE,7,909,4,981,1,FALSE,FALSE,FALSE,robust lane detection from continuous driving scenes using deep neural networks,1,robust lane detection from continuous driving scenes using deep neural networks,6,35,57,46,46,92,11,2946,620,2326,1473,1473,853,30,9,21,15,15,6,57,2326,21,['no affiliation'],FALSE,57,2326,21,['no affiliation'],FALSE,TRUE,FALSE,160,69,50,4,0,1,1,4,1,1,unknow,11,192
https://paperswithcode.com/paper/targeted-adversarial-perturbations-for,2006.08602,https://arxiv.org/abs/2006.08602v2,https://arxiv.org/pdf/2006.08602v2.pdf,https://github.com/alexklwong/targeted-adversarial-perturbations-monocular-depth,FALSE,TRUE,"We study the effect of adversarial perturbations on the task of monocular depth prediction. Specifically, we explore the ability of small, imperceptible additive perturbations to selectively alter the perceived geometry of the scene. We show that such perturbations can not only globally re-scale the predicted distances from the camera, but also alter the prediction to match a different target scene. We also show that, when given semantic or instance information, perturbations can fool the network to alter the depth of specific categories or instances in the scene, and even remove them while preserving the rest of the scene. To understand the effect of targeted perturbations, we conduct experiments on state-of-the-art monocular depth prediction methods. Our experiments reveal vulnerabilities in monocular depth prediction networks, and shed light on the biases and context learned by them.",NeurIPS 2020 12,"['Alex Wong', 'Safa Cicek', 'Stefano Soatto']","['Adversarial Attack', 'Adversarial Defense', 'Depth Estimation', 'Monocular Depth Estimation']",2020-06-12,[],ce826add054b2d38031c6fa9f8ae945844593358,"[{'name': 'Alex  Wong', 'ids': ['144821962']}, {'name': 'Safa  Cicek', 'ids': ['46232059']}, {'name': 'Stefano  Soatto', 'ids': ['1715959']}]","['c4f96d199f66fa8e598d8b53b08644531193848e', '887ab73ec9e44c64aeaa5612a0ff48ad22d55a92', 'cb39e4ff49b623e825f3f4dd01974f64eed0c078', 'a009ad2fe978ae24ff26d6f8dbac092ea4e80f13']","['3f25b3ddef8626ace7aa0865a1a9e3dad1f23fb6', '44a462b3240a1987756cf6071427b29653446af1', '4463dc4a32b948f0230f3b782cbfecaf1c9e5b1d', '71acad655c616df320faa16234c7221a41869679', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', 'e7867244de690a1ae11a7a6d5a021e868fa75a3c', '9a683a827be40cbdc8134d8467b8514758a57d18', '614fa41266f5fba084cf6f6bb12554fcbfd37631', 'c1994ba5946456fc70948c549daf62363f13fa2d', 'f0c5991dbb130fa6b5de011cf7a04f6ed815ef68', '282578039c767f3d393529565cae6be56fda6242', '4e1b1b98d9b0c73eaca4e71f553e72503fbd4aba', '4d3beb32fa0efdd10280bad003ef37e5f62f6cbd', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', '60f997a400188233f2448d00a77c25784c4c8dc3', '06feba1ffd596b41884cea6e8ef0da89b6dd2233', 'f0d229fda980640b7efd50fbdadc10a93b177829', '52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35', 'f6e0856b4a9199fa968ac00da612a9407b5cb85c', '2887d97aaff6513106b99a917f2305eb182294c7', 'daf8cd0f2c159d022477914bfacee9ff6da70c8b', '3f7bc67330b3eff749459568e7995f0017dfe645', '92c92f42ee7c38c4e6010354b21408e427220074', '67711d42b77a13a04822ae00620660cef3abf8c4', '1f4294d8e0b0c8559479fac569fc0ea91b4dc0bd', 'bc1138738f24c4a23d865d7786fc4c8229e4662a', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'c279e565e950c4335060865b6ed5247b9cb5b4f2', '9c21b3ffdac5f2450b82dd6660ac69f72bb9018b', '16aa01ca0834a924c25faad5d8bfef3fd1acfcfe', '612668d62b1ad8a031278a73fde250790fb5b866', 'c33b61e4c4cae519fc65319ec0305e1b10d17219', 'c8c494ee5488fe20e0aa01bddf3fc4632086d654', 'b17e61972e674f8f734bd428cb882a9bb797abe2', '589cfcb2f995c94b0a98c902cc1f5e0f27cbd927', '1b47265245e8db53a553049dcb27ed3e495fd625', 'd6dcbf0bb628658df27abd297c03147115e476e4', '86a891b3209984562c72fd49c7fd26737cb58c5d', '3abf64d10a5d9a426d864bcfd68daed370d6904c', 'fb38fc75f58cf8e171d59b868b1afbddbb9a28eb', 'de5b0fd02ea4f4d67fe3ae0d74603b9822df4e42', '1d6bc45c31c17f5091eec3def813cc2cd26d811e', 'b7a2d31bf89ae5d9204dd2ce75db9dfa23e8b1ac', 'ead93f252a650c79475264b7a1e128d52d81eb7a', '2f7ae3361b2aafa24fc289252a9cad8f1135edae', 'd8c04365ed0627a5043996cdd26c1a56b5a630b8', '887ab73ec9e44c64aeaa5612a0ff48ad22d55a92', '63b059cdad77906ff381515b3cfac21757e5e64c', 'f25e1160b1b28c9cb14e0000ea35d397f98a46ce', '2100274f9fe5aec8b4c581dcd112d78e0b60f200', 'ba5c19a02696029566b9e01f5f55e908d25763bd']",2020,https://semanticscholar.org/paper/ce826add054b2d38031c6fa9f8ae945844593358,['DBLP'],"['https://proceedings.neurips.cc/paper/2020/hash/609e9d4bcc8157c00808993f612f1acd-Abstract.html', 'https://arxiv.org/abs/2006.08602', 'https://arxiv.org/pdf/2006.08602v2.pdf']",NeurIPS,['Computer Science'],3098467408,187638491,https://api.github.com/repos/alexklwong/targeted-adversarial-perturbations-monocular-depth,34593045,targeted-adversarial-perturbations-monocular-depth,PyTorch implementation of Targeted Adversarial Perturbations for Monocular Depth Predictions (in NeurIPS 2020),\N,2020-10-22 6:32:19,\N,0,2021-02-06 15:52:42,\N,https://github.com/alexklwong/targeted-adversarial-perturbations-monocular-depth,4,3,HTML,2020-10-22 6:32:19,2021-02-06 10:30:55,0,TRUE,TRUE,no homepageUrl,2,"['adversarial-attacks', 'adversarial-machine-learning', 'adversarial-examples', 'adversarial-perturbations', 'targeted-adversarial-attacks', 'computer-vision', 'machine-learning', 'adversarial-robustness', 'pytorch', 'neurips-2020', 'monocular-depth-estimation', 'monocular-depth', '3d-reconstruction', '3d-vision', 'neurips', 'monocular-depth-predictions']","# Targeted Adversarial Perturbations for Monocular Depth Prediction

PyTorch implementation of *Targeted Adversarial Perturbations for Monocular Depth Prediction*

\[[arxiv](https://arxiv.org/pdf/2006.08602.pdf)\] \[[poster](http://web.cs.ucla.edu/~alexw/nips2020_targeted_adversarial_monodepth_poster.pdf)\]

Published in the Proceedings of Neural Information Processing Systems (NeurIPS) 2020

Authors: [Alex Wong](http://web.cs.ucla.edu/~alexw/), [Safa Cicek](https://bsafacicek.github.io/)

If this work is useful to you, please consider citing our paper:
```
@inproceedings{wong2020targeted,
  title={Targeted Adversarial Perturbations for Monocular Depth Prediction},
  author={Wong, Alex and Cicek, Safa and Soatto, Stefano},
  booktitle={Advances in neural information processing systems},
  year={2020}
}
```

**Table of Contents**
1. [About targeted adversarial perturbations](#about-targeted-adversarial-perturbations)
2. [Setting up your virtual environment](#setting-up-virtual-environment)
3. [Setting up your data directories](#setting-up-data-directories)
4. [Setting up pretrained models](#setting-up-pretrained-models)
5. [Optimizing targeted adversarial perturbations](#optimizing-targeted-adversarial-perturbations)
6. [Broader impact](#broader-impact)
7. [License and disclaimer](#license-disclaimer)

## About targeted adversarial perturbations <a name=""about-targeted-adversarial-perturbations""></a>

<p align=""center"">
    <img src=""figures/teaser.png"" width=""800"">
</p>

A number of studies have shown that classification, detection and segmentation outputed by a deep network, can be dramatically altered by small additive signals. These *adversarial perturbations* are designed on purpose to “fool” the network. We explore the possibility of fooling a network into inferring the wrong depth of a scene. More specifically, we show that *not only* is it possible to fool a network to provide the wrong depth map, but it is possible to fool it to return *any* depth map. Targeted adversarial perturbations are designed not just to return the wrong answer, but to return the answer that the *adversary wants*.

For example, in the figure above, visually imperceptible noise can (i) scale the scene 10% closer to the camera, (ii) scale the vehicles 10% closer, and (iii) remove an instance of a vehicle from the scene.

<p align=""center"">
    <img src=""figures/remove_instance.png"" width=""800"">
</p>

Our work sheds light on the role of context in the representation of geometry with deep networks. When attacking a specific category or instance, while most of the perturbations are localized, some are distributed throughout the scene, far from the object of interest. Even when the target effect is localized (e.g., make a car disappear), the perturbations are non-local, indicating that the network exploits non-local context, which represents a vulnerability. This begs the question: could one perturb regions in the image, for instance displaying billboards, thus making cars seemingly disappear?

We hope that our repository can provide you with the tools to study and further explore these phenonmenons -- ultimately to develop more robust representations of geometry.


## Setting up your virtual environment <a name=""setting-up-virtual-environment""></a>
```
virtualenv -p /usr/bin/python3 targeted-attacks-py3env
source targeted-attacks-py3env/bin/activate
pip install opencv-python scipy scikit-learn scikit-image matplotlib future yacs pandas gdown
pip install numpy==1.16.4 gast==0.2.2
pip install Pillow==6.1.0 torch==1.2.0 torchvision==0.4.0 tensorboard==1.14.0
```

## Setting up your data directories <a name=""setting-up-data-directories""></a>
Assuming you have the KITTI semantics dataset:

```
mkdir data
ln -s /path/to/kitti_data_semantics/ data/kitti_data_semantics

python setup/setup_dataset_kitti_semantics.py
python setup/setup_dataset_kitti_instance.py
```

## Setting up pretrained models <a name=""setting-up-pretrained-models""></a>
We have provided the implementations of Monodepth, Monodepth2, and PackNet in `external_src`. The implementation of Monodepth was written by us and follows closely to the official Tensorflow implementation. Each model can be accessed through a wrapper class (e.g. `src/monodepth2_model.py`, `src/packnet_model.py`).

We have the copy of each pretrained model used into our Google drive to download them, you may use gdown (already installed in virtual environment):

```
python setup/setup_model_monodepth.py
python setup/setup_model_monodepth2.py
python setup/setup_model_packnet.py
```

If you are unable to download them via our provided setup scripts, you can directly access them using the following Google drive links:

```
Monodepth :
https://drive.google.com/file/d/1yJ8wx9khv-pg-D_UQIrvAAWW3zsM3OGA/view?usp=sharing

Monodepth2 :
https://drive.google.com/file/d/1ArW1Tr9-Clukepy0_olWw8AHMbigOXTH/view?usp=sharing

PackNet :
https://drive.google.com/file/d/1PLeCOMZjki6XSJmGPOF2Tc2iRvKMIBN4/view?usp=sharing
```

Each of the links will give you a `.zip` file (`resnet50.zip`, `stereo_640x192.zip`, `packnet_velsup_csk.zip`), please create the appropriate directories when unzipping them:
```
mkdir pretrained_models

mkdir pretrained_models/monodepth
unzip -o path/to/monodepth/resnet50.zip -d pretrained_models/monodepth/

mkdir pretrained_models/monodepth2
unzip -o path/to/monodepth2/stereo_640x192.zip -d pretrained_models/monodepth2/

mkdir pretrained_models/packnet
unzip -o path/to/packnet/packnet_velsup_csk.zip -d pretrained_models/packnet/
```

## Optimizing targeted adversarial perturbations <a name=""optimizing-targeted-adversarial-perturbations""></a>
You can run targeted attacks against each model using `src/run_perturb.py`.

For example, to fool Monodepth2 into scaling the entire scene 10% farther away:
```
python src/run_perturb.py \
--image_path testing/kitti_semantic_test_vehicle_image.txt \
--n_height 192 \
--n_width 640 \
--n_channel 3 \
--output_norm 0.02 \
--n_step 500 \
--learning_rates 5.0 1.0 \
--learning_schedule 400 \
--depth_method monodepth2 \
--depth_transform_func multiply \
--depth_transform_value 1.10 \
--mask_constraint none \
--checkpoint_path perturbations/monodepth2/all_mult110_norm002_lr5e0_1e0 \
--depth_model_restore_path0 pretrained_models/monodepth2/stereo_640x192/encoder.pth \
--depth_model_restore_path1 pretrained_models/monodepth2/stereo_640x192/depth.pth \
--device gpu
```

Note: `testing/kitti_semantic_test_vehicle_image.txt` contains the complete 200 images from KITTI semantics dataset.

Additional examples of different attacks (e.g. symmetric flipping, instance removal) can be found in the `bash` directory. To run them:
```
bash bash/run_perturb_monodepth.sh
bash bash/run_perturb_monodepth2.sh
bash bash/run_perturb_packnet.sh
```

## Broader impact <a name=""broader-impact""></a>
Adversarial perturbations highlight limitations and failure modes of deep networks. They have captured the collective imagination by conjuring scenarios where AI goes awry at the tune of imperceptible changes. Some popular media and press has gone insofar as suggesting them as proof that AI cannot be trusted.

While monocular depth prediction networks are indeed vulnerable to these attacks, we want to assure the reader that these perturbations cannot cause harm outside of the academic setting. As mentioned in the paper, optimizing for these perturbations is computationally expensive and hence it is infeasible to craft these perturbations in real time. Additionally, they also do not transfer; so, we see little negative implications for real-world applications. However, the fact that they exist implies that there is room for improvement in the way that we learn representations for depth prediction.

Hence, we see the existence of adversaries as an opportunity. Studying their effects on deep networks is also the first step to render a system robust to such a vulnerability. The broader impact of our work is to understand the corner cases and failure modes in order to develop more robust representations. This, in turn, will improve interpretability (or, rather, reduce nonsensical behavior). In the fullness of time, we expect this research to pay a small contribution to benefit transportation safety.

## License and disclaimer <a name=""license-disclaimer""></a>
This software is property of the UC Regents, and is provided free of charge for research purposes only. It comes with no warranties, expressed or implied, according to these [terms and conditions](license). For commercial use, please contact [UCLA TDG](https://tdg.ucla.edu).
",15,84,69,52,273,2017-03-08 2:37:24,https://github.com/alexklwong/targeted-adversarial-perturbations-monocular-depth,3,30433,FALSE,no mirror url,2,0,0,User,no company,alexw@cs.ucla.edu,no twitter_username or private twitter_username,11,2022-06-09 4:27:15,"Academic Software License

Targeted Adversarial Perturbations for Monocular Depth Prediction

No Commercial Use

This License governs use of the accompanying Software, and your use of the Software constitutes acceptance of this license.

You may use this Software for any non-commercial purpose, subject to the restrictions in this license. Uses which are non-commercial include teaching, academic research, and personal experimentation.

You may not use or distribute this Software or any derivative works in any form for any commercial purpose. Examples of commercial purposes would be running business operations, licensing, leasing, or selling the Software, or distributing the Software for use with commercial products.

You may modify this Software and distribute the modified Software for non-commercial purposes; however, you may not grant rights to the Software or derivative works that are broader than those provided by this License. For example, you may not distribute modifications of the Software under terms that would permit commercial use, or under terms that purport to require the Software or derivative works to be sublicensed to others.

You agree:

Not remove any copyright or other notices from the Software.

That if you distribute the Software in source or object form, you will include a verbatim copy of this license.

That if you distribute derivative works of the Software in source code form you do so only under a license that includes all of the provisions of this License, and if you distribute derivative works of the Software solely in object form you do so only under a license that complies with this License.

That if you have modified the Software or created derivative works, and distribute such modifications or derivative works, you will cause the modified files to carry prominent notices so that recipients know that they are not receiving the original Software. Such notices must state: (i) that you have changed the Software; and (ii) the date of any changes.

THAT THIS PRODUCT IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS PRODUCT, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. YOU MUST PASS THIS LIMITATION OF LIABILITY ON WHENEVER YOU DISTRIBUTE THE SOFTWARE OR DERIVATIVE WORKS.

That if you sue anyone over patents that you think may apply to the Software or anyone's use of the Software, your license to the Software ends automatically.

That your rights under the License end automatically if you breach it in any way.

UCLA reserves all rights not expressly granted to you in this license.
",0,1.620095027,1.982244673,1,1,targeted-adversarial-perturbations-monocular-depth,pytorch,"<h1>targeted adversarial perturbations for monocular depth prediction</h1>
<p>pytorch implementation of <em>targeted adversarial perturbations for monocular depth prediction</em></p>
<p>[<a href=""https://arxiv.org/pdf/2006.08602.pdf"">arxiv</a>] [<a href=""http://web.cs.ucla.edu/~alexw/nips2020_targeted_adversarial_monodepth_poster.pdf"">poster</a>]</p>
<p>published in the proceedings of neural information processing systems (neurips) 2020</p>
<p>authors: <a href=""http://web.cs.ucla.edu/~alexw/"">alex wong</a>, <a href=""https://bsafacicek.github.io/"">safa cicek</a></p>
<p>if this work is useful to you, please consider citing our paper:</p>
<pre><code>@inproceedings{wong2020targeted,
  title={targeted adversarial perturbations for monocular depth prediction},
  author={wong, alex and cicek, safa and soatto, stefano},
  booktitle={advances in neural information processing systems},
  year={2020}
}
</code></pre>
<p><strong>table of contents</strong></p>
<ol>
<li><a href=""#about-targeted-adversarial-perturbations"">about targeted adversarial perturbations</a></li>
<li><a href=""#setting-up-virtual-environment"">setting up your virtual environment</a></li>
<li><a href=""#setting-up-data-directories"">setting up your data directories</a></li>
<li><a href=""#setting-up-pretrained-models"">setting up pretrained models</a></li>
<li><a href=""#optimizing-targeted-adversarial-perturbations"">optimizing targeted adversarial perturbations</a></li>
<li><a href=""#broader-impact"">broader impact</a></li>
<li><a href=""#license-disclaimer"">license and disclaimer</a></li>
</ol>
<h2>about targeted adversarial perturbations <a name=""about-targeted-adversarial-perturbations""></a></h2>
<p align=""center"">
    <img src=""figures/teaser.png"" width=""800"">
</p><p>a number of studies have shown that classification, detection and segmentation outputed by a deep network, can be dramatically altered by small additive signals. these <em>adversarial perturbations</em> are designed on purpose to “fool” the network. we explore the possibility of fooling a network into inferring the wrong depth of a scene. more specifically, we show that <em>not only</em> is it possible to fool a network to provide the wrong depth map, but it is possible to fool it to return <em>any</em> depth map. targeted adversarial perturbations are designed not just to return the wrong answer, but to return the answer that the <em>adversary wants</em>.</p>
<p>for example, in the figure above, visually imperceptible noise can (i) scale the scene 10% closer to the camera, (ii) scale the vehicles 10% closer, and (iii) remove an instance of a vehicle from the scene.</p>
<p align=""center"">
    <img src=""figures/remove_instance.png"" width=""800"">
</p><p>our work sheds light on the role of context in the representation of geometry with deep networks. when attacking a specific category or instance, while most of the perturbations are localized, some are distributed throughout the scene, far from the object of interest. even when the target effect is localized (e.g., make a car disappear), the perturbations are non-local, indicating that the network exploits non-local context, which represents a vulnerability. this begs the question: could one perturb regions in the image, for instance displaying billboards, thus making cars seemingly disappear?</p>
<p>we hope that our repository can provide you with the tools to study and further explore these phenonmenons -- ultimately to develop more robust representations of geometry.</p>
<h2>setting up your virtual environment <a name=""setting-up-virtual-environment""></a></h2>
<pre><code>virtualenv -p /usr/bin/python3 targeted-attacks-py3env
source targeted-attacks-py3env/bin/activate
pip install opencv-python scipy scikit-learn scikit-image matplotlib future yacs pandas gdown
pip install numpy==1.16.4 gast==0.2.2
pip install pillow==6.1.0 torch==1.2.0 torchvision==0.4.0 tensorboard==1.14.0
</code></pre>
<h2>setting up your data directories <a name=""setting-up-data-directories""></a></h2>
<p>assuming you have the kitti semantics dataset:</p>
<pre><code>mkdir data
ln -s /path/to/kitti_data_semantics/ data/kitti_data_semantics

python setup/setup_dataset_kitti_semantics.py
python setup/setup_dataset_kitti_instance.py
</code></pre>
<h2>setting up pretrained models <a name=""setting-up-pretrained-models""></a></h2>
<p>we have provided the implementations of monodepth, monodepth2, and packnet in <code>external_src</code>. the implementation of monodepth was written by us and follows closely to the official tensorflow implementation. each model can be accessed through a wrapper class (e.g. <code>src/monodepth2_model.py</code>, <code>src/packnet_model.py</code>).</p>
<p>we have the copy of each pretrained model used into our google drive to download them, you may use gdown (already installed in virtual environment):</p>
<pre><code>python setup/setup_model_monodepth.py
python setup/setup_model_monodepth2.py
python setup/setup_model_packnet.py
</code></pre>
<p>if you are unable to download them via our provided setup scripts, you can directly access them using the following google drive links:</p>
<pre><code>monodepth :
https://drive.google.com/file/d/1yj8wx9khv-pg-d_uqirvaaww3zsm3oga/view?usp=sharing

monodepth2 :
https://drive.google.com/file/d/1arw1tr9-clukepy0_olww8ahmbigoxth/view?usp=sharing

packnet :
https://drive.google.com/file/d/1plecomzjki6xsjmgpof2tc2irvkmibn4/view?usp=sharing
</code></pre>
<p>each of the links will give you a <code>.zip</code> file (<code>resnet50.zip</code>, <code>stereo_640x192.zip</code>, <code>packnet_velsup_csk.zip</code>), please create the appropriate directories when unzipping them:</p>
<pre><code>mkdir pretrained_models

mkdir pretrained_models/monodepth
unzip -o path/to/monodepth/resnet50.zip -d pretrained_models/monodepth/

mkdir pretrained_models/monodepth2
unzip -o path/to/monodepth2/stereo_640x192.zip -d pretrained_models/monodepth2/

mkdir pretrained_models/packnet
unzip -o path/to/packnet/packnet_velsup_csk.zip -d pretrained_models/packnet/
</code></pre>
<h2>optimizing targeted adversarial perturbations <a name=""optimizing-targeted-adversarial-perturbations""></a></h2>
<p>you can run targeted attacks against each model using <code>src/run_perturb.py</code>.</p>
<p>for example, to fool monodepth2 into scaling the entire scene 10% farther away:</p>
<pre><code>python src/run_perturb.py \
--image_path testing/kitti_semantic_test_vehicle_image.txt \
--n_height 192 \
--n_width 640 \
--n_channel 3 \
--output_norm 0.02 \
--n_step 500 \
--learning_rates 5.0 1.0 \
--learning_schedule 400 \
--depth_method monodepth2 \
--depth_transform_func multiply \
--depth_transform_value 1.10 \
--mask_constraint none \
--checkpoint_path perturbations/monodepth2/all_mult110_norm002_lr5e0_1e0 \
--depth_model_restore_path0 pretrained_models/monodepth2/stereo_640x192/encoder.pth \
--depth_model_restore_path1 pretrained_models/monodepth2/stereo_640x192/depth.pth \
--device gpu
</code></pre>
<p>note: <code>testing/kitti_semantic_test_vehicle_image.txt</code> contains the complete 200 images from kitti semantics dataset.</p>
<p>additional examples of different attacks (e.g. symmetric flipping, instance removal) can be found in the <code>bash</code> directory. to run them:</p>
<pre><code>bash bash/run_perturb_monodepth.sh
bash bash/run_perturb_monodepth2.sh
bash bash/run_perturb_packnet.sh
</code></pre>
<h2>broader impact <a name=""broader-impact""></a></h2>
<p>adversarial perturbations highlight limitations and failure modes of deep networks. they have captured the collective imagination by conjuring scenarios where ai goes awry at the tune of imperceptible changes. some popular media and press has gone insofar as suggesting them as proof that ai cannot be trusted.</p>
<p>while monocular depth prediction networks are indeed vulnerable to these attacks, we want to assure the reader that these perturbations cannot cause harm outside of the academic setting. as mentioned in the paper, optimizing for these perturbations is computationally expensive and hence it is infeasible to craft these perturbations in real time. additionally, they also do not transfer; so, we see little negative implications for real-world applications. however, the fact that they exist implies that there is room for improvement in the way that we learn representations for depth prediction.</p>
<p>hence, we see the existence of adversaries as an opportunity. studying their effects on deep networks is also the first step to render a system robust to such a vulnerability. the broader impact of our work is to understand the corner cases and failure modes in order to develop more robust representations. this, in turn, will improve interpretability (or, rather, reduce nonsensical behavior). in the fullness of time, we expect this research to pay a small contribution to benefit transportation safety.</p>
<h2>license and disclaimer <a name=""license-disclaimer""></a></h2>
<p>this software is property of the uc regents, and is provided free of charge for research purposes only. it comes with no warranties, expressed or implied, according to these <a href=""license"">terms and conditions</a>. for commercial use, please contact <a href=""https://tdg.ucla.edu"">ucla tdg</a>.</p>
",1,8,10,0,0,FALSE,0,0,FALSE,TRUE,TRUE,5,6,FALSE,FALSE,FALSE,113,12470,11,1001,13,FALSE,TRUE,FALSE,targeted adversarial perturbations for monocular depth prediction,0,targeted adversarial perturbations for monocular depth prediction,3,67,67,67,67,67,0,5108,5108,5108,5108,5108,0,27,27,27,27,27,0,67,5108,27,['no affiliation'],FALSE,67,5108,27,['no affiliation'],FALSE,TRUE,FALSE,7,51,4,4,0,1,1,3,1,1,A*,7,134
https://paperswithcode.com/paper/on-leveraging-pretrained-gans-for-limited,2002.1181,https://arxiv.org/abs/2002.11810v3,https://arxiv.org/pdf/2002.11810v3.pdf,https://github.com/MiaoyunZhao/GANTransferLimitedData,TRUE,TRUE,"Recent work has shown generative adversarial networks (GANs) can generate highly realistic images, that are often indistinguishable (by humans) from real images. Most images so generated are not contained in the training dataset, suggesting potential for augmenting training sets with GAN-generated data. While this scenario is of particular relevance when there are limited data available, there is still the issue of training the GAN itself based on that limited data. To facilitate this, we leverage existing GAN models pretrained on large-scale datasets (like ImageNet) to introduce additional knowledge (which may not exist within the limited data), following the concept of transfer learning. Demonstrated by natural-image generation, we reveal that low-level filters (those close to observations) of both the generator and discriminator of pretrained GANs can be transferred to facilitate generation in a perceptually-distinct target domain with limited training data. To further adapt the transferred filters to the target domain, we propose adaptive filter modulation (AdaFM). An extensive set of experiments is presented to demonstrate the effectiveness of the proposed techniques on generation with limited data.",ICML 2020 1,"['Miaoyun Zhao', 'Yulai Cong', 'Lawrence Carin']","['Image Generation', 'Transfer Learning']",2020-02-26,"[{'name': 'Convolution', 'full_name': 'Convolution', 'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\r\n\r\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\r\n\r\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)', 'introduced_year': 1980, 'source_url': None, 'source_title': None, 'code_snippet_url': None, 'main_collection': {'name': 'Convolutions', 'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.', 'parent': 'Image Feature Extractors', 'area': 'Computer Vision'}}, {'name': 'GAN', 'full_name': 'Generative Adversarial Network', 'description': 'A **GAN**, or **Generative Adversarial Network**, is a generative model that simultaneously trains\r\ntwo models: a generative model $G$ that captures the data distribution, and a discriminative model $D$ that estimates the\r\nprobability that a sample came from the training data rather than $G$.\r\n\r\nThe training procedure for $G$ is to maximize the probability of $D$ making\r\na mistake. This framework corresponds to a minimax two-player game. In the\r\nspace of arbitrary functions $G$ and $D$, a unique solution exists, with $G$\r\nrecovering the training data distribution and $D$ equal to $\\frac{1}{2}$\r\neverywhere. In the case where $G$ and $D$ are defined by multilayer perceptrons,\r\nthe entire system can be trained with backpropagation. \r\n\r\n(Image Source: [here](http://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html))', 'introduced_year': 2000, 'source_url': 'https://arxiv.org/abs/1406.2661v1', 'source_title': 'Generative Adversarial Networks', 'code_snippet_url': 'https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py', 'main_collection': {'name': 'Generative Models', 'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.', 'parent': None, 'area': 'Computer Vision'}}]",a47d3653a47ae1ad1a922cd25e9d6df757ad07ea,"[{'name': 'Miaoyun  Zhao', 'ids': ['7202209']}, {'name': 'Yulai  Cong', 'ids': ['2147914']}, {'name': 'Lawrence  Carin', 'ids': ['145006560']}]","['92da590980d2d47ba2c8bb678818b8b434bceffc', '6ca741d6ccb9ce838fb0863136bdc5f42d32ede6', '9003b24d61aeb282cd3467fbc46fb1c37a1a0cf2', '37fd5706e6fffebbfae54421cd159fbbb3001126', 'e6adc9ce44f267c22dfc12bf8a0051dc92c43695']","['f3e3d1f86a534a3654d0ee263142e44f4e2c61e9', '2fe2cfd98e232f1396f01881853ed6b3d5e37d65', '80bf8b9598312d7897082057783da583b46f8ffb', '0b10d741f6c2cf946604105bd85707af65c7ccf6', 'b4458e7abc14269d45ffec33ef070e050caae05b', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', 'bbb6ef63bec6db62e1bdfb5137ba7f7a42a507d4', 'ae1745dbe1fedad56235f22bbbb8926f6f9a8fb0', '1e10a58cd3d3974d083ec5aa323e5d10c43fc061', '3f8781992c33a4d45c1e4cd0b830fab4ae9d083b', 'df0c54fe61f0ffb9f0e36a17c2038d9a1964cba3', '743d39f81ae653c0d5ebf6e8ef3d0a4d17dfe75e', '9667f8264745b626c6173b1310e2ff0298b09cfc', 'df7ad8eeb595da5f7774e91dae06075be952acff', '192ff78cc0d478c82730898f9e0085674b51d72d', '84de7d27e2f6160f634a483e8548c499a2cda7fa', 'fac36fa1b809b71756c259f2c5db20add0cb0da0', '7340f090f8a0df5b109682e9f6d57e4b8ca1a2f7', '347bac45298f37cd83c3e79d99b826dc65a70c46', '37e52ff4714c7a08900b518127e438a195b84611', 'be0ef77fb0345c5851bb5d297f3ed84ae3c581ee', 'a83cec6a91701bd8500f8c43ad731d4353c71d55', '73c5c3c8da53d3167e84e88702bbb76c33154f94', '037de0cad5987478f3abbe91a7cd6e21bd9dc2de', 'a29b7a000e6e1a2907ce6ae24a03aba5d9ef873f', 'a88f86093e6f2d14761d4b8cbdcadfeff496c948', '3e58d9800aa31e5db89d99dcd33e5786b7837bfd', 'c88e8d85fd5160b0793598bda037f977366acf7a', '6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4', '6edbd6fcb4db16057ec4a069f383afa93bf66e84', 'ca2f3a290bc24bc1c602389207ee86f9e01779e7', '907a90967f68da4311802247408e0515e363f930', '744fe47157477235032f7bb3777800f9f2f45e52', '210da45e57f86a50c04bdd7b37d498c8ecc288da', 'acbeebdfd9dd3456628604eefcd53f50f974b132', '081651b38ff7533550a3adfc1c00da333a8fe86c', '17fa1c2a24ba8f731c8b21f1244463bc4b465681', '1109b663453e78a59e4f66446d71720ac58cec25', '04d8303864808addb7773505df4a172bb1fef225', '2f4df08d9072fc2ac181b7fced6a245315ce05c8', 'c08f5fa876181fc040d76c75fe2433eee3c9b001', '003ec88cbec156131058e53409115cd056057644', '02b28f3b71138a06e40dbd614abf8568420ae183', '1f6c3f1def78919f06efe050e9403e85d5fa3ac9', 'abd1c342495432171beb7ca8fd9551ef13cbd0ff', '7f4afc1bf3272ae6ec00b46e27efc4a4f6b0826d', '22aab110058ebbd198edb1f1e7b4f69fb13c0613', '15d0330ef349310db06e7f9babd8f09970905b51', '2f6e03d60bcd38c76811463eb653e0d5012b9480', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', 'b8de958fead0d8a9619b55c7299df3257c624a96', 'a82dd07100355718ff2d4c6ec9aa467c2aed5423', '6b4ca249b3b28d3fee65f69714440c08d42cee64', '1a2a770d23b4a171fa81de62a78a3deb0588f238', 'c5b55f410365bb889c25a9f0354f2b86ec61c4f0', 'a105aacf158b39db9ac87dc97a0769d51e8c681a', '725c650ae8db8d9a57e4a7b15a555dbe69b67054', '8578fef93bdc5ebf43e12d9c5857a1e322e8a278', 'a8f3dc53e321fbb2565f5925def4365b9f68d1af', '54e325aee6b2d476bbbb88615ac15e251c6e8214', 'ceb2ebef0b41e31c1a21b28c2734123900c005e2', '231af7dc01a166cac3b5b01ca05778238f796e41', '55caf5154cd558c355d7191daa565ebebb8336e1', '744464cd6fa8341633cd3b5d378faab18a3b543a', '0597aba86088282423e7c2d2deb6fca4075e7a91', '5dfd68a4f10f5983a97f977c85bd089b8083c2c1', '4b54963534dbdb7dae56e2407fde3ed04a82edf9', 'e3a10973ee4e3be5fc53bee96e4d8e56469e432a']",2020,https://semanticscholar.org/paper/a47d3653a47ae1ad1a922cd25e9d6df757ad07ea,['DBLP'],"['http://proceedings.mlr.press/v119/zhao20a/zhao20a-supp.pdf', 'https://proceedings.icml.cc/static/paper_files/icml/2020/1164-Paper.pdf', 'http://proceedings.mlr.press/v119/zhao20a/zhao20a.pdf', 'https://proceedings.icml.cc/static/paper_files/icml/2020/1164-Supplemental.pdf', 'http://people.ee.duke.edu/~lcarin/Miaoyun_ICML_2020.pdf', 'http://proceedings.mlr.press/v119/zhao20a.html', 'https://arxiv.org/pdf/2002.11810v3.pdf']",ICML,"['Computer Science', 'Engineering']",3037512191,167436898,https://api.github.com/repos/MiaoyunZhao/GANTransferLimitedData,59258549,GANTransferLimitedData,"This is a pytorch implementation of the paper \On Leveraging Pretrained GANs for Limited-Data Generation\"".""",Python,2020-06-09 21:34:57,\N,0,2021-02-21 5:06:41,\N,https://github.com/MiaoyunZhao/GANTransferLimitedData,23,6,Python,2020-06-09 21:34:57,2020-08-10 3:48:41,2,TRUE,TRUE,no homepageUrl,3,[],"# GANTransferLimitedData
This is a pytorch implementation of the paper [On Leveraging Pretrained GANs for Limited-Data Generation](https://arxiv.org/pdf/2002.11810.pdf).

Please consider citing our paper if you refer to this code in your research.
```
@inproceedings{zhao2020leveraging,
  title={On Leveraging Pretrained GANs for Limited-Data Generation},
  author={Zhao, Miaoyun and Cong, Yulai and Carin, Lawrence},
  booktitle={ICML},
  year={2020},
}
```

# Requirement
```
python=3.7.3
pytorch=1.2.0
```

# Notes
`CELEBA_[f]GmDn.py` is the implementation of the model in Figure1(f).

`Flower_[h]our.py` is the implementation of the model in Figure1(h). This code is for ""Section 4.1 Comparisons with Existing Methods"".

`Flower25_our.py`is the code for the experiments on Flowers-25.

# Usage

First, download the pretrained GP-GAN model by running `download_pretrainedGAN.py`. Note please change the path therein.

Second, download the training data to the folder `./data/`. For example, download the Flowers dataset from: https://www.robots.ox.ac.uk/~vgg/data/flowers/102/ to the folder `./data/102flowers/`.
For Flowers-25, we choose the first 25 images from the passion category, following [Image Generation from Small Datasets via Batch Statistics Adaptation](https://arxiv.org/abs/1904.01774).

## Dataset preparation
```angular2
data
├──102flowers
           ├──all8189images
                      ├──image_folder
           ├──passion25 
                      ├──image_folder
├── CelebA
...
```

Finally, run `Flower_[h]our.py` or  `Flower25_our.py`.

## Acknowledgement
Our code is based on GAN_stability: https://github.com/LMescheder/GAN_stability from the paper [Which Training Methods for GANs do actually Converge?](https://avg.is.tuebingen.mpg.de/publications/meschedericml2018).

",58,103,94,8,168,2019-03-15 12:08:33,https://github.com/MiaoyunZhao/GANTransferLimitedData,6,8909,FALSE,no mirror url,2,0,0,User,no company,miaoyun9zhao@gmail.com,no twitter_username or private twitter_username,3,2022-01-20 11:55:57,"MIT License

Copyright (c) 2020 Miaoyun Zhao

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""Software""), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
",0,1.987996274,2.275200723,1,2,GANTransferLimitedData,pytorch,"<h1>gantransferlimiteddata</h1>
<p>this is a pytorch implementation of the paper <a href=""https://arxiv.org/pdf/2002.11810.pdf"">on leveraging pretrained gans for limited-data generation</a>.</p>
<p>please consider citing our paper if you refer to this code in your research.</p>
<pre><code>@inproceedings{zhao2020leveraging,
  title={on leveraging pretrained gans for limited-data generation},
  author={zhao, miaoyun and cong, yulai and carin, lawrence},
  booktitle={icml},
  year={2020},
}
</code></pre>
<h1>requirement</h1>
<pre><code>python=3.7.3
pytorch=1.2.0
</code></pre>
<h1>notes</h1>
<p><code>celeba_[f]gmdn.py</code> is the implementation of the model in figure1(f).</p>
<p><code>flower_[h]our.py</code> is the implementation of the model in figure1(h). this code is for ""section 4.1 comparisons with existing methods"".</p>
<p><code>flower25_our.py</code>is the code for the experiments on flowers-25.</p>
<h1>usage</h1>
<p>first, download the pretrained gp-gan model by running <code>download_pretrainedgan.py</code>. note please change the path therein.</p>
<p>second, download the training data to the folder <code>./data/</code>. for example, download the flowers dataset from: <a href=""https://www.robots.ox.ac.uk/~vgg/data/flowers/102/"">https://www.robots.ox.ac.uk/~vgg/data/flowers/102/</a> to the folder <code>./data/102flowers/</code>.
for flowers-25, we choose the first 25 images from the passion category, following <a href=""https://arxiv.org/abs/1904.01774"">image generation from small datasets via batch statistics adaptation</a>.</p>
<h2>dataset preparation</h2>
<pre><code class=""lang-angular2"">data
├──102flowers
           ├──all8189images
                      ├──image_folder
           ├──passion25 
                      ├──image_folder
├── celeba
...
</code></pre>
<p>finally, run <code>flower_[h]our.py</code> or  <code>flower25_our.py</code>.</p>
<h2>acknowledgement</h2>
<p>our code is based on gan_stability: <a href=""https://github.com/lmescheder/gan_stability"">https://github.com/lmescheder/gan_stability</a> from the paper <a href=""https://avg.is.tuebingen.mpg.de/publications/meschedericml2018"">which training methods for gans do actually converge?</a>.</p>
",0,2,8,0,0,FALSE,0,1,FALSE,FALSE,TRUE,2,8,TRUE,FALSE,FALSE,22,3006,25,195,15,FALSE,TRUE,FALSE,on leveraging pretrained gans for generation with limited data,0,on leveraging pretrained gans for generation with limited data,3,13,20,16.5,16.5,33,3.5,517,156,361,258.5,258.5,102.5,19,7,12,9.5,9.5,2.5,20,361,12,['no affiliation'],FALSE,13,156,7,['no affiliation'],FALSE,TRUE,FALSE,47,68,5,2,2,2,1,7,1,1,A*,9,175
https://paperswithcode.com/paper/cgnet-a-light-weight-context-guided-network,1811.08201,http://arxiv.org/abs/1811.08201v2,http://arxiv.org/pdf/1811.08201v2.pdf,https://github.com/wutianyiRosun/CGNet,TRUE,TRUE,"The demand of applying semantic segmentation model on mobile devices has been
increasing rapidly. Current state-of-the-art networks have enormous amount of
parameters hence unsuitable for mobile devices, while other small memory
footprint models follow the spirit of classification network and ignore the
inherent characteristic of semantic segmentation. To tackle this problem, we
propose a novel Context Guided Network (CGNet), which is a light-weight and
efficient network for semantic segmentation. We first propose the Context
Guided (CG) block, which learns the joint feature of both local feature and
surrounding context, and further improves the joint feature with the global
context. Based on the CG block, we develop CGNet which captures contextual
information in all stages of the network and is specially tailored for
increasing segmentation accuracy. CGNet is also elaborately designed to reduce
the number of parameters and save memory footprint. Under an equivalent number
of parameters, the proposed CGNet significantly outperforms existing
segmentation networks. Extensive experiments on Cityscapes and CamVid datasets
verify the effectiveness of the proposed approach. Specifically, without any
post-processing and multi-scale testing, the proposed CGNet achieves 64.8% mean
IoU on Cityscapes with less than 0.5 M parameters. The source code for the
complete system can be found at https://github.com/wutianyiRosun/CGNet.",no proceeding,"['Tianyi Wu', 'Sheng Tang', 'Rui Zhang', 'Yongdong Zhang']",['Semantic Segmentation'],2018-11-20,[],bbd8411033f125b06ad0b59912cfb49395b3cc82,"[{'name': 'Tianyi  Wu', 'ids': ['144637279']}, {'name': 'Sheng  Tang', 'ids': ['144044848']}, {'name': 'Rui  Zhang', 'ids': ['144142354']}, {'name': 'Yongdong  Zhang', 'ids': ['1699819']}]","['c05b79938a6fe0bdbf4de11278bf7ef7cb7d4f47', '8a927a3dbc54a8045c9fc47b2f1c262e771c3fc8', '2560bf28cb258e1186839b31050f81740aeae862', '5912a3516a9ed04cb0342578fd1049f146964c7f', 'acd8f62d6acd387bb5e845ed69035e913843db90', '05bfd38b73812a63cfbc0d4760d008fbb3649ddb', '37077342156091095f2486116856b9031d558478', 'a74e6d5a0ce3ebf0ccc0186c0e8b8c2c2250a53c', '85906bb5131923a2e80280ad853e13d671fb061f', 'dad2adf2774e575e4d33b5cb0d37f38ecbc159ae', 'db48fa731066c97714a5e48f48d0e9a526f547ac', '1747f7f65afe4ee4a3032c5ea5f0c8fcc3a188c3', 'd4c5f3b6135c9662bad5318e0f7e9962201e790f', '980ebfb3367c558fc345d9d4902cedab03e96ed2', '36ed77718f875f99245986dbad04df530e7ed1d8', 'dcf7d209a1a8efa89b16b40f7a829dcb92b2aca4', '723813d3c673c28793f5066f5548ef829b387dab', '1d327de48c02f886303c1b7627724d4ed795ad91', 'bea92a69b48287caa3a25ff3dfe727bed8888348', '789e196e7f021170ad8d8ae4543492d1032e7888', '533d092e08dd070b6a47748a210a728e5cc013ee', 'cbace01223871cd94d3e7d618610973ef7ac9520', 'cacb6cacf09293587e9c018d55a62e13f2704d93', 'bf56ef26c641af5c5018cf8b9b96ed0ac00dc717', '0b261e572a3df0b95df3debb455fa7c8f72f8c14', '0963c44a0846e668a307c2c8a919317e82d8b723', 'a8ef98bdab475cba77e1f7f32f6d5dd20b0a4ec7', 'c454cea35d8d583ae281d98f76c7ddff6d5fe5ff', '1486e26d5d4add35ce2e7b40a420f342d2964cbb', '937eda87ac27d189e0feee58d5a03dd350e9039e', '47fdce2b3b27819d61525247bc3de34f2d157983', 'a1854d797154d06657f7ee9621937af4b8e73a91', 'd03366338a20b4b18583579f58f11f4baa990699', 'abf3a11021eeec0c925417a00194481bad4c5152', 'b867b8ac03beb32593ac7a1a0307c273ae2fa075', '228a00d04092b9ae073f51408936a485ce86e12a', '1b1276c9d311baef4ee3e0defabc66ad69a4ce5c', '276304e967090edb71e05178a230bd6b82ef0707', '9d02cd89691cb6c7c2b5a6fd8c2310cd33f2ba80', '489936641d9d6033ef4b5ee6524d56e29d8ff24f', '59ef4f25877fda9920243cb6e15c0f1c02cc18a9', 'ac3baad8dc9a6d570e367706706a92f05c241b60', '17cb4a0a3c404b829741ce410c2af31e44501837', '473996e16fa7a4dc57d291e5b5d1ad88120aebd4', '0efa93d265530eb1068bae987ae316686287ef26', '33a6de4a5a10c5662e93da7b017f93bd582c96d4', 'ddda5c3254b24abf1be9190b179c4fc6941eee19', '4ebb43fba4a308fb41d9078836be9069cb18503f', 'ce190a9acdbebdd54168e686f150e8d94352c133']","['f50a70c29b629f90b6faa9d37f2c281bcf132a8d', 'c2aa0581254ce342c77b2c7a835b3587bd453486', '1bb5081766d84549b3411cb56e60483c33cca52d', '3f285a81d39adc2bb7b33e77c88738c4fe423ee5', 'd1c091bf9402f1caf13892a3fae39326507401be', '10d85561e4aafc516d10064f30dff05b41f70afe', '1031a69923b80ad01cf3fbb703d10757a80e699b', 'e746c8eec81384bd37dede9700be9c8a3700f936', 'd1aab16162fc85d8476d6d4ca8b016c9d3388ab8', 'b5ebfe3e1926e0ac100ed7568448ba3ebaab8b88', '37077342156091095f2486116856b9031d558478', '8899094797e82c5c185a0893896320ef77f60e64', 'fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5', 'ad655c25e052fa4eeed53421344aca6f239c4c9d', 'ee4a012a4b12d11d7ab8c0e79c61e807927a163c', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '794ca7a3a856683221797c6e03cdc6ef798d1f5e', '64c1158a4061882d610f1cffd5ddb1e8fc9a74b4', '9217e28b2273eb3b26e4e9b7b498b4661e6e09f5', 'daf74c34f7da0695b154f645c8b78a7397a98f16', '723813d3c673c28793f5066f5548ef829b387dab', '4eee87d960754f755fdec073a160af3e2e31672f', 'b0c065cd43aa7280e766b5dcbcc7e26abce59330', 'fb37561499573109fc2cebb6a7b08f44917267dd', '5f94e354faeba1d330088b926d1f7886067bc93f', '1d327de48c02f886303c1b7627724d4ed795ad91', '35fe3dd3350c32467030884337dde10d5e20ff99', 'c81c20109c809cfc47565a9477c04ee005d424bf', 'c17519cec26cef6ec3edf6b30c5168c7ebb87792', 'e15cf50aa89fee8535703b9f9512fca5bfc43327', '480b5fa4371043d4386f76f0755e2a6e7dc386a6', '1747f7f65afe4ee4a3032c5ea5f0c8fcc3a188c3', 'a5bb1a662883d502ca3d8ddfa9321e8830860fd2', 'e01058388d139e027482a7d89a2997606f7ef4fd', '8757a02ec150876ec422dbc06262076a74367d52', 'ca5c766b2d31a1f5ce8896a0a42b40a2bff9323a', '7f5fc84819c0cf94b771fe15141f65b123f7b8ec', 'a5fa91dbc72f200970e70debe88375b71ddef40b', '08f624f7ee5c3b05b1b604357fb1532241e208db', 'cab372bc3824780cce20d9dd1c22d4df39ed081a', '2796c448023b78fd77f3a4b57966f257c5e654c2', '56f52f4564b1d90ee66d894b649d1a6ae8472211', '7a39a3ca168dfebb2e2d55b3fca0f750b32896da', '21b58c8aba44c173493e418a797a1f36c6dae8a9', 'bac3a5266d4fef1f207c7b32a90ab9a79888abc0', '37fb64f0e52d763b1f75f706e9b5c16ad20cc2d7', 'dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4', '99c69fe118efbc47efc91ceaa3b2e711405eef20', '90006064cafcb0a9ad8a30cffeb56efe7e14129b', '343a128beb49db94169f24ea6c8cb22d6d59ecc4', '3647d6d0f151dc05626449ee09cc7bce55be497e', '4fdc075d1cc14d64c11c3c860e86a082b761f4a0', 'c1baa0d7c0680e528285aebdac73649ec5dc823c', '5912a3516a9ed04cb0342578fd1049f146964c7f', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', 'ea743597a5f48babef1982259566d76a9bf66bf2', '2a31b4bf2a294b6e67956a6cd5ed6d875af548e0', '1847249502bcc85d56870a72f9a4b1722e477046', '0e0f4f90c4744eb52acc910d96d76f5dbca386d4', '9f48616039cb21903132528c0be5348b3019db50', '796a6e78d4c4b63926ee956f202d874a8c4542b0', 'c8c494ee5488fe20e0aa01bddf3fc4632086d654']",2021,https://semanticscholar.org/paper/bbd8411033f125b06ad0b59912cfb49395b3cc82,"['Medline', 'DBLP']","['https://arxiv.org/pdf/1811.08201v1.pdf', 'https://export.arxiv.org/pdf/1811.08201', 'http://export.arxiv.org/pdf/1811.08201', 'http://arxiv.org/abs/1811.08201', 'https://doi.org/10.1109/TIP.2020.3042065']",IEEE Transactions on Image Processing,"['Computer Science', 'Medicine']",2901189993,110461373,https://api.github.com/repos/wutianyiRosun/CGNet,40449927,CGNet,\N,\N,2018-09-19 10:50:03,\N,0,2021-03-06 8:41:35,\N,https://github.com/wutianyiRosun/CGNet,167,50,Python,2018-09-19 11:29:55,2022-02-16 22:10:28,7,TRUE,TRUE,no homepageUrl,10,"['semantic-segmentation', 'cityscapes', 'camvid', 'pytorch']","# [CGNet: A Light-weight Context Guided Network for Semantic Segmentation](https://arxiv.org/pdf/1811.08201.pdf)
## Introduction
The demand of applying semantic segmentation model on mobile devices has been increasing rapidly. Current
state-of-the-art networks have enormous amount of parameters, hence unsuitable for mobile devices, while other small
memory footprint models follow the spirit of classification network and ignore the inherent characteristic of semantic
segmentation. To tackle this problem, we propose a novel Context Guided Network (CGNet), which is a light-weight
and efficient network for semantic segmentation. We first propose the Context Guided (CG) block, which learns the
joint feature of both local feature and surrounding context, and further improves the joint feature with the global context.
Based on the CG block, we develop CGNet which captures contextual information in all stages of the network and
is specially tailored for increasing segmentation accuracy. CGNet is also elaborately designed to reduce the number
of parameters and save memory footprint. Under an equivalent number of parameters, the proposed CGNet significantly outperforms existing segmentation networks. Extensive experiments on Cityscapes and CamVid datasets verify the effectiveness of the proposed approach. Specifically, without any post-processing and multi-scale testing, the proposed CGNet achieves 64.8% mean IoU on Cityscapes with less than 0.5 M parameters.


## Installation
1. Install PyTorch
  - Env: PyTorch\_0.4; cuda\_9.2; cudnn\_7.5; python\_3.6
2. Clone the repository
   ```shell
   git clone https://github.com/wutianyiRosun/CGNet.git 
   cd CGNet
   ```
3. Dataset

  - Download the [Cityscapes](https://www.cityscapes-dataset.com/) dataset and convert the dataset to [19 categories](https://github.com/mcordts/cityscapesScripts/blob/master/cityscapesscripts/helpers/labels.py). It should have this basic structure.
  ```
  ├── cityscapes_test_list.txt
  ├── cityscapes_train_list.txt
  ├── cityscapes_trainval_list.txt
  ├── cityscapes_val_list.txt
  ├── cityscapes_val.txt
  ├── gtCoarse
  │   ├── train
  │   ├── train_extra
  │   └── val
  ├── gtFine
  │   ├── test
  │   ├── train
  │   └── val
  ├── leftImg8bit
  │   ├── test
  │   ├── train
  │   └── val
  ├── license.txt
```
  - Download the [Camvid](https://github.com/alexgkendall/SegNet-Tutorial/tree/master/CamVid) dataset. It should have this basic structure.
  ```
  ├── camvid_test_list.txt
  ├── camvid_train_list.txt
  ├── camvid_trainval_list.txt
  ├── camvid_val_list.txt
  ├── test
  ├── testannot
  ├── train
  ├── trainannot
  ├── val
  └── valannot

  ```
## Train your own model
  
###  For Cityscapes
  1. training on train set
  ```
  python cityscapes_train.py --gpus 0,1 --dataset cityscapes --train_type ontrain --train_data_list ./dataset/list/Cityscapes/cityscapes_train_list.txt --max_epochs 300
  ```
  
  2. training on train+val set
  ```
  python cityscapes_train.py --gpus 0,1 --dataset cityscapes --train_type ontrainval --train_data_list ./dataset/list/Cityscapes/cityscapes_trainval_list.txt --max_epochs 350
  ```
  3. Evaluation (on validation set)
 
  ```
  python cityscapes_eval.py --gpus 0 --val_data_list ./dataset/list/Cityscapes/cityscapes_val_list.txt --resume ./checkpoint/cityscapes/CGNet_M3N21bs16gpu2_ontrain/model_cityscapes_train_on_trainset.pth
  ```
  
  - model file download: [model_cityscapes_train_on_trainset.pth](https://pan.baidu.com/s/1rilPxLqBH57_sLg0Lc1--Q)
  
  4. Testing (on test set)
  ```
  python cityscapes_test.py --gpus 0 --test_data_list ./dataset/list/Cityscapes/cityscapes_test_list.txt --resume ./checkpoint/cityscapes/CGNet_M3N21bs16gpu2_ontrainval/model_cityscapes_train_on_trainvalset.pth
  ```
  - model file download: [model_cityscapes_train_on_trainvalset.pth](https://pan.baidu.com/s/1x7LEunjweoDvb_-xNQmFAg)
  5. Running time on Tesla V100 (single card single batch)
  ```
  56.8 ms with command ""torch.cuda.synchronize()""
  20.0 ms w/o command ""torch.cuda.synchronize()""
  ```
  
###  For Camvid
  1. training on train+val set
   ```
  python camvid_train.py
  ```
  2. testing (on test set)
  ```
  python camvid_test.py
  ```

  - model file download: [model_camvid_train_on_trainvalset.pth](https://pan.baidu.com/s/1gH6pI3jFmtlBgjgLUCjVvA)
  
  ## Citation
If CGNet is useful for your research, please consider citing:
```
  @article{wu2020cgnet,
  title={Cgnet: A light-weight context guided network for semantic segmentation},
  author={Wu, Tianyi and Tang, Sheng and Zhang, Rui and Cao, Juan and Zhang, Yongdong},
  journal={IEEE Transactions on Image Processing},
  volume={30},
  pages={1169--1179},
  year={2020},
  publisher={IEEE}
}
```
  ## License

This code is released under the MIT License. See [LICENSE](LICENSE) for additional details.

## Thanks to the Third Party Libs
https://github.com/speedinghzl/Pytorch-Deeplab.
",62,193,15,89,230,2018-01-06 7:49:19,https://github.com/wutianyiRosun/CGNet,50,2033,FALSE,no mirror url,1,0,0,User,BDR,wutianyirosun@gmail.com,no twitter_username or private twitter_username,118,2022-04-19 12:30:14,"MIT License

Copyright (c) 2018 Tianyi Wu

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""Software""), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
",1,3.711290146,3.542851667,3,3,CGNet,pytorch,"<h1><a href=""https://arxiv.org/pdf/1811.08201.pdf"">cgnet: a light-weight context guided network for semantic segmentation</a></h1>
<h2>introduction</h2>
<p>the demand of applying semantic segmentation model on mobile devices has been increasing rapidly. current
state-of-the-art networks have enormous amount of parameters, hence unsuitable for mobile devices, while other small
memory footprint models follow the spirit of classification network and ignore the inherent characteristic of semantic
segmentation. to tackle this problem, we propose a novel context guided network (cgnet), which is a light-weight
and efficient network for semantic segmentation. we first propose the context guided (cg) block, which learns the
joint feature of both local feature and surrounding context, and further improves the joint feature with the global context.
based on the cg block, we develop cgnet which captures contextual information in all stages of the network and
is specially tailored for increasing segmentation accuracy. cgnet is also elaborately designed to reduce the number
of parameters and save memory footprint. under an equivalent number of parameters, the proposed cgnet significantly outperforms existing segmentation networks. extensive experiments on cityscapes and camvid datasets verify the effectiveness of the proposed approach. specifically, without any post-processing and multi-scale testing, the proposed cgnet achieves 64.8% mean iou on cityscapes with less than 0.5 m parameters.</p>
<h2>installation</h2>
<ol>
<li>install pytorch<ul>
<li>env: pytorch_0.4; cuda_9.2; cudnn_7.5; python_3.6</li>
</ul>
</li>
<li>clone the repository<pre><code class=""lang-shell"">git clone https://github.com/wutianyirosun/cgnet.git 
cd cgnet
</code></pre>
</li>
<li><p>dataset</p>
<ul>
<li>download the <a href=""https://www.cityscapes-dataset.com/"">cityscapes</a> dataset and convert the dataset to <a href=""https://github.com/mcordts/cityscapesscripts/blob/master/cityscapesscripts/helpers/labels.py"">19 categories</a>. it should have this basic structure.<pre><code>├── cityscapes_test_list.txt
├── cityscapes_train_list.txt
├── cityscapes_trainval_list.txt
├── cityscapes_val_list.txt
├── cityscapes_val.txt
├── gtcoarse
│   ├── train
│   ├── train_extra
│   └── val
├── gtfine
│   ├── test
│   ├── train
│   └── val
├── leftimg8bit
│   ├── test
│   ├── train
│   └── val
├── license.txt
</code></pre>
</li>
<li>download the <a href=""https://github.com/alexgkendall/segnet-tutorial/tree/master/camvid"">camvid</a> dataset. it should have this basic structure.
```
├── camvid_test_list.txt
├── camvid_train_list.txt
├── camvid_trainval_list.txt
├── camvid_val_list.txt
├── test
├── testannot
├── train
├── trainannot
├── val
└── valannot</li>
</ul>
<p>```
## train your own model</p>
</li>
</ol>
<h3>for cityscapes</h3>
<ol>
<li><p>training on train set</p>
<pre><code>python cityscapes_train.py --gpus 0,1 --dataset cityscapes --train_type ontrain --train_data_list ./dataset/list/cityscapes/cityscapes_train_list.txt --max_epochs 300
</code></pre>
</li>
<li><p>training on train+val set</p>
<pre><code>python cityscapes_train.py --gpus 0,1 --dataset cityscapes --train_type ontrainval --train_data_list ./dataset/list/cityscapes/cityscapes_trainval_list.txt --max_epochs 350
</code></pre>
</li>
<li><p>evaluation (on validation set)</p>
<pre><code>python cityscapes_eval.py --gpus 0 --val_data_list ./dataset/list/cityscapes/cityscapes_val_list.txt --resume ./checkpoint/cityscapes/cgnet_m3n21bs16gpu2_ontrain/model_cityscapes_train_on_trainset.pth
</code></pre>
</li>
</ol>
<ul>
<li>model file download: <a href=""https://pan.baidu.com/s/1rilpxlqbh57_slg0lc1--q"">model_cityscapes_train_on_trainset.pth</a></li>
</ul>
<ol>
<li>testing (on test set)<pre><code>python cityscapes_test.py --gpus 0 --test_data_list ./dataset/list/cityscapes/cityscapes_test_list.txt --resume ./checkpoint/cityscapes/cgnet_m3n21bs16gpu2_ontrainval/model_cityscapes_train_on_trainvalset.pth
</code></pre>
</li>
</ol>
<ul>
<li>model file download: <a href=""https://pan.baidu.com/s/1x7leunjweodvb_-xnqmfag"">model_cityscapes_train_on_trainvalset.pth</a></li>
</ul>
<ol>
<li>running time on tesla v100 (single card single batch)<pre><code>56.8 ms with command ""torch.cuda.synchronize()""
20.0 ms w/o command ""torch.cuda.synchronize()""
</code></pre>
</li>
</ol>
<h3>for camvid</h3>
<ol>
<li>training on train+val set<pre><code>python camvid_train.py
</code></pre>
</li>
<li>testing (on test set)<pre><code>python camvid_test.py
</code></pre>
</li>
</ol>
<ul>
<li><p>model file download: <a href=""https://pan.baidu.com/s/1gh6pi3jfmtlbgjglucjvva"">model_camvid_train_on_trainvalset.pth</a></p>
<p>## citation
if cgnet is useful for your research, please consider citing:</p>
<pre><code>@article{wu2020cgnet,
title={cgnet: a light-weight context guided network for semantic segmentation},
author={wu, tianyi and tang, sheng and zhang, rui and cao, juan and zhang, yongdong},
journal={ieee transactions on image processing},
volume={30},
pages={1169--1179},
year={2020},
publisher={ieee}
}
</code></pre>
<p>## license</p>
</li>
</ul>
<p>this code is released under the mit license. see <a href=""license"">license</a> for additional details.</p>
<h2>thanks to the third party libs</h2>
<p><a href=""https://github.com/speedinghzl/pytorch-deeplab"">https://github.com/speedinghzl/pytorch-deeplab</a>.</p>
",10,9,0,0,0,FALSE,0,3,FALSE,TRUE,FALSE,4,11,TRUE,FALSE,FALSE,32,4639,25,529,1,TRUE,TRUE,FALSE,cgnet: a light-weight context guided network for semantic segmentation,1,cgnet: a light-weight context guided network for semantic segmentation,4,65,65,65,65,65,0,2825,2825,2825,2825,2825,0,18,18,18,18,18,0,65,2825,18,['no affiliation'],FALSE,65,2825,18,['no affiliation'],FALSE,FALSE,FALSE,217,62,49,1,0,2,2,5,1,1,unknow,9,202
https://paperswithcode.com/paper/multi-modal-dense-video-captioning,2003.07758,https://arxiv.org/abs/2003.07758v2,https://arxiv.org/pdf/2003.07758v2.pdf,https://github.com/v-iashin/MDVC,TRUE,TRUE,"Dense video captioning is a task of localizing interesting events from an untrimmed video and producing textual description (captions) for each localized event. Most of the previous works in dense video captioning are solely based on visual information and completely ignore the audio track. However, audio, and speech, in particular, are vital cues for a human observer in understanding an environment. In this paper, we present a new dense video captioning approach that is able to utilize any number of modalities for event description. Specifically, we show how audio and speech modalities may improve a dense video captioning model. We apply automatic speech recognition (ASR) system to obtain a temporally aligned textual description of the speech (similar to subtitles) and treat it as a separate input alongside video frames and the corresponding audio track. We formulate the captioning task as a machine translation problem and utilize recently proposed Transformer architecture to convert multi-modal input data into textual descriptions. We demonstrate the performance of our model on ActivityNet Captions dataset. The ablation studies indicate a considerable contribution from audio and speech components suggesting that these modalities contain substantial complementary information to video frames. Furthermore, we provide an in-depth analysis of the ActivityNet Caption results by leveraging the category tags obtained from original YouTube videos. Code is publicly available: github.com/v-iashin/MDVC",no proceeding,"['Vladimir Iashin', 'Esa Rahtu']","['Automatic Speech Recognition', 'Dense Video Captioning', 'Video Captioning']",2020-03-17,"[{'name': 'Absolute Position Encodings', 'full_name': 'Absolute Position Encodings', 'description': '**Absolute Position Encodings** are a type of position embeddings for [[Transformer](https://paperswithcode.com/method/transformer)-based models] where positional encodings are added to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension $d\\_{model}$ as the embeddings, so that the two can be summed. In the original implementation, sine and cosine functions of different frequencies are used:\r\n\r\n$$ \\text{PE}\\left(pos, 2i\\right) = \\sin\\left(pos/10000^{2i/d\\_{model}}\\right) $$\r\n\r\n$$ \\text{PE}\\left(pos, 2i+1\\right) = \\cos\\left(pos/10000^{2i/d\\_{model}}\\right) $$\r\n\r\nwhere $pos$ is the position and $i$ is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from $2\\pi$ to $10000 \\dot 2\\pi$. This function was chosen because the authors hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset $k$,  $\\text{PE}\\_{pos+k}$ can be represented as a linear function of $\\text{PE}\\_{pos}$.\r\n\r\nImage Source: [D2L.ai](https://d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html)', 'introduced_year': 2000, 'source_url': 'http://arxiv.org/abs/1706.03762v5', 'source_title': 'Attention Is All You Need', 'code_snippet_url': '', 'main_collection': {'name': 'Position Embeddings', 'description': '', 'parent': None, 'area': 'General'}}, {'name': 'Position-Wise Feed-Forward Layer', 'full_name': 'Position-Wise Feed-Forward Layer', 'description': '**Position-Wise Feed-Forward Layer** is a type of [feedforward layer](https://www.paperswithcode.com/method/category/feedforwad-networks) consisting of two [dense layers](https://www.paperswithcode.com/method/dense-connections) that applies to the last dimension, which means the same dense layers are used for each position item in the sequence, so called position-wise.', 'introduced_year': 2000, 'source_url': 'http://arxiv.org/abs/1706.03762v5', 'source_title': 'Attention Is All You Need', 'code_snippet_url': None, 'main_collection': {'name': 'Feedforward Networks', 'description': '**Feedforward Networks** are a type of neural network architecture which rely primarily on dense-like connections. Below you can find a continuously updating list of feedforward network components.', 'parent': None, 'area': 'General'}}, {'name': 'Residual Connection', 'full_name': 'Residual Connection', 'description': '**Residual Connections** are a type of skip-connection that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. \r\n\r\nFormally, denoting the desired underlying mapping as $\\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\\mathcal{F}({x}):=\\mathcal{H}({x})-{x}$. The original mapping is recast into $\\mathcal{F}({x})+{x}$.\r\n\r\nThe intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.', 'introduced_year': 2000, 'source_url': 'http://arxiv.org/abs/1512.03385v1', 'source_title': 'Deep Residual Learning for Image Recognition', 'code_snippet_url': 'https://github.com/pytorch/vision/blob/7c077f6a986f05383bcb86b535aedb5a63dd5c4b/torchvision/models/resnet.py#L118', 'main_collection': {'name': 'Skip Connections', 'description': '**Skip Connections** allow layers to skip layers and connect to layers further up the network, allowing for information to flow more easily up the network. Below you can find a continuously updating list of skip connection methods.', 'parent': None, 'area': 'General'}}, {'name': 'Dense Connections', 'full_name': 'Dense Connections', 'description': '**Dense Connections**, or **Fully Connected Connections**, are a type of layer in a deep neural network that use a linear operation where every input is connected to every output by a weight. This means there are $n\\_{\\text{inputs}}*n\\_{\\text{outputs}}$ parameters, which can lead to a lot of parameters for a sizeable network.\r\n\r\n$$h\\_{l} = g\\left(\\textbf{W}^{T}h\\_{l-1}\\right)$$\r\n\r\nwhere $g$ is an activation function.\r\n\r\nImage Source: Deep Learning by Goodfellow, Bengio and Courville', 'introduced_year': 2000, 'source_url': None, 'source_title': None, 'code_snippet_url': None, 'main_collection': {'name': 'Feedforward Networks', 'description': '**Feedforward Networks** are a type of neural network architecture which rely primarily on dense-like connections. Below you can find a continuously updating list of feedforward network components.', 'parent': None, 'area': 'General'}}, {'name': 'Label Smoothing', 'full_name': 'Label Smoothing', 'description': '**Label Smoothing** is a regularization technique that introduces noise for the labels. This accounts for the fact that datasets may have mistakes in them, so maximizing the likelihood of $\\log{p}\\left(y\\mid{x}\\right)$ directly can be harmful. Assume for a small constant $\\epsilon$, the training set label $y$ is correct with probability $1-\\epsilon$ and incorrect otherwise. Label Smoothing regularizes a model based on a [softmax](https://paperswithcode.com/method/softmax) with $k$ output values by replacing the hard $0$ and $1$ classification targets with targets of $\\frac{\\epsilon}{k-1}$ and $1-\\epsilon$ respectively.\r\n\r\nSource: Deep Learning, Goodfellow et al\r\n\r\nImage Source: [When Does Label Smoothing Help?](https://arxiv.org/abs/1906.02629)', 'introduced_year': 1985, 'source_url': None, 'source_title': None, 'code_snippet_url': None, 'main_collection': {'name': 'Regularization', 'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.', 'parent': None, 'area': 'General'}}, {'name': 'ReLU', 'full_name': 'Rectified Linear Units', 'description': '**Rectified Linear Units**, or **ReLUs**, are a type of activation function that are linear in the positive dimension, but zero in the negative dimension. The kink in the function is the source of the non-linearity. Linearity in the positive dimension has the attractive property that it prevents non-saturation of gradients (contrast with [sigmoid activations](https://paperswithcode.com/method/sigmoid-activation)), although for half of the real line its gradient is zero.\r\n\r\n$$ f\\left(x\\right) = \\max\\left(0, x\\right) $$', 'introduced_year': 2000, 'source_url': None, 'source_title': None, 'code_snippet_url': 'https://github.com/DimTrigkakis/Python-Net/blob/efb81b2f828da5a81b77a141245efdb0d5bcfbf8/incredibleMathFunctions.py#L12-L13', 'main_collection': {'name': 'Activation Functions', 'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.', 'parent': None, 'area': 'General'}}, {'name': 'Adam', 'full_name': 'Adam', 'description': '**Adam** is an adaptive learning rate optimization algorithm that utilises both momentum and scaling, combining the benefits of [RMSProp](https://paperswithcode.com/method/rmsprop) and [SGD w/th Momentum](https://paperswithcode.com/method/sgd-with-momentum). The optimizer is designed to be appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. \r\n\r\nThe weight updates are performed as:\r\n\r\n$$ w_{t} = w_{t-1} - \\eta\\frac{\\hat{m}\\_{t}}{\\sqrt{\\hat{v}\\_{t}} + \\epsilon}  $$\r\n\r\nwith\r\n\r\n$$ \\hat{m}\\_{t} = \\frac{m_{t}}{1-\\beta^{t}_{1}} $$\r\n\r\n$$ \\hat{v}\\_{t} = \\frac{v_{t}}{1-\\beta^{t}_{2}} $$\r\n\r\n$$ m_{t} = \\beta_{1}m_{t-1} + (1-\\beta_{1})g_{t} $$\r\n\r\n$$ v_{t} = \\beta_{2}v_{t-1} + (1-\\beta_{2})g_{t}^{2}  $$\r\n\r\n\r\n$ \\eta $ is the step size/learning rate, around 1e-3 in the original paper. $ \\epsilon $ is a small number, typically 1e-8 or 1e-10, to prevent dividing by zero. $ \\beta_{1} $ and $ \\beta_{2} $ are forgetting parameters, with typical values 0.9 and 0.999, respectively.', 'introduced_year': 2000, 'source_url': 'http://arxiv.org/abs/1412.6980v9', 'source_title': 'Adam: A Method for Stochastic Optimization', 'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/optim/adam.py#L6', 'main_collection': {'name': 'Stochastic Optimization', 'description': ""**Stochastic Optimization** methods are used to optimize neural networks. We typically take a mini-batch of data, hence 'stochastic', and perform a type of gradient descent with this minibatch. Below you can find a continuously updating list of stochastic optimization algorithms."", 'parent': 'Optimization', 'area': 'General'}}, {'name': 'Softmax', 'full_name': 'Softmax', 'description': ""The **Softmax** output function transforms a previous layer's output into a vector of probabilities. It is commonly used for multiclass classification.  Given an input vector $x$ and a weighting vector $w$ we have:\r\n\r\n$$ P(y=j \\mid{x}) = \\frac{e^{x^{T}w_{j}}}{\\sum^{K}_{k=1}e^{x^{T}wk}} $$"", 'introduced_year': 2000, 'source_url': None, 'source_title': None, 'code_snippet_url': None, 'main_collection': {'name': 'Output Functions', 'description': '**Output functions** are layers used towards the end of a network to transform to the desired form for a loss function. For example, the softmax relies on logits to construct a conditional probability. Below you can find a continuously updating list of output functions.', 'parent': None, 'area': 'General'}}, {'name': 'Dropout', 'full_name': 'Dropout', 'description': '**Dropout** is a regularization technique for neural networks that drops a unit (along with connections) at training time with a specified probability $p$ (a common value is $p=0.5$). At test time, all units are present, but with weights scaled by $p$ (i.e. $w$ becomes $pw$).\r\n\r\nThe idea is to prevent co-adaptation, where the neural network becomes too reliant on particular connections, as this could be symptomatic of overfitting. Intuitively, dropout can be thought of as creating an implicit ensemble of neural networks.', 'introduced_year': 2000, 'source_url': 'http://jmlr.org/papers/v15/srivastava14a.html', 'source_title': 'Dropout: A Simple Way to Prevent Neural Networks from Overfitting', 'code_snippet_url': 'https://github.com/google/jax/blob/7f3078b70d0ed9bea6228efa420879c56f72ef69/jax/experimental/stax.py#L271-L275', 'main_collection': {'name': 'Regularization', 'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.', 'parent': None, 'area': 'General'}}, {'name': 'Multi-Head Attention', 'full_name': 'Multi-Head Attention', 'description': '**Multi-head Attention** is a module for attention mechanisms which runs through an attention mechanism several times in parallel. The independent attention outputs are then concatenated and linearly transformed into the expected dimension. Intuitively, multiple attention heads allows for attending to parts of the sequence differently (e.g. longer-term dependencies versus shorter-term dependencies). \r\n\r\n$$ \\text{MultiHead}\\left(\\textbf{Q}, \\textbf{K}, \\textbf{V}\\right) = \\left[\\text{head}\\_{1},\\dots,\\text{head}\\_{h}\\right]\\textbf{W}_{0}$$\r\n\r\n$$\\text{where} \\text{ head}\\_{i} = \\text{Attention} \\left(\\textbf{Q}\\textbf{W}\\_{i}^{Q}, \\textbf{K}\\textbf{W}\\_{i}^{K}, \\textbf{V}\\textbf{W}\\_{i}^{V} \\right) $$\r\n\r\nAbove $\\textbf{W}$ are all learnable parameter matrices.\r\n\r\nNote that [scaled dot-product attention](https://paperswithcode.com/method/scaled) is most commonly used in this module, although in principle it can be swapped out for other types of attention mechanism.\r\n\r\nSource: [Lilian Weng](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms)', 'introduced_year': 2000, 'source_url': 'http://arxiv.org/abs/1706.03762v5', 'source_title': 'Attention Is All You Need', 'code_snippet_url': 'https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/fec78a687210851f055f792d45300d27cc60ae41/transformer/SubLayers.py#L9', 'main_collection': {'name': 'Attention Modules', 'description': '**Attention Modules** refer to modules that incorporate attention mechanisms. For example, multi-head attention is a module that incorporates multiple attention heads. Below you can find a continuously updating list of attention modules.', 'parent': 'Attention', 'area': 'General'}}, {'name': 'Layer Normalization', 'full_name': 'Layer Normalization', 'description': 'Unlike [batch normalization](https://paperswithcode.com/method/batch-normalization), **Layer Normalization** directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases. It works well for [RNNs](https://paperswithcode.com/methods/category/recurrent-neural-networks) and improves both the training time and the generalization performance of several existing RNN models. More recently, it has been used with [Transformer](https://paperswithcode.com/methods/category/transformers) models.\r\n\r\nWe compute the layer normalization statistics over all the hidden units in the same layer as follows:\r\n\r\n$$ \\mu^{l} = \\frac{1}{H}\\sum^{H}\\_{i=1}a\\_{i}^{l} $$\r\n\r\n$$ \\sigma^{l} = \\sqrt{\\frac{1}{H}\\sum^{H}\\_{i=1}\\left(a\\_{i}^{l}-\\mu^{l}\\right)^{2}}  $$\r\n\r\nwhere $H$ denotes the number of hidden units in a layer. Under layer normalization, all the hidden units in a layer share the same normalization terms $\\mu$ and $\\sigma$, but different training cases have different normalization terms. Unlike batch normalization, layer normalization does not impose any constraint on the size of the mini-batch and it can be used in the pure online regime with batch size 1.', 'introduced_year': 2000, 'source_url': 'http://arxiv.org/abs/1607.06450v1', 'source_title': 'Layer Normalization', 'code_snippet_url': 'https://github.com/CyberZHG/torch-layer-normalization/blob/89f405b60f53f85da6f03fe685c190ef394ce50c/torch_layer_normalization/layer_normalization.py#L8', 'main_collection': {'name': 'Normalization', 'description': '**Normalization** layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.', 'parent': None, 'area': 'General'}}, {'name': 'Scaled Dot-Product Attention', 'full_name': 'Scaled Dot-Product Attention', 'description': '**Scaled dot-product attention** is an attention mechanism where the dot products are scaled down by $\\sqrt{d_k}$. Formally we have a query $Q$, a key $K$ and a value $V$ and calculate the attention as:\r\n\r\n$$ {\\text{Attention}}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^{T}}{\\sqrt{d_k}}\\right)V $$\r\n\r\nIf we assume that $q$ and $k$ are $d_k$-dimensional vectors whose components are independent random variables with mean $0$ and variance $1$, then their dot product, $q \\cdot k = \\sum_{i=1}^{d_k} u_iv_i$, has mean $0$ and variance $d_k$.  Since we would prefer these values to have variance $1$, we divide by $\\sqrt{d_k}$.', 'introduced_year': 2000, 'source_url': 'http://arxiv.org/abs/1706.03762v5', 'source_title': 'Attention Is All You Need', 'code_snippet_url': 'https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/5c0264915ab43485adc576f88971fc3d42b10445/transformer/Modules.py#L7', 'main_collection': {'name': 'Attention Mechanisms', 'description': '**Attention Mechanisms** are a component used in neural networks to model long-range interaction, for example across a text in NLP. The key idea is to build shortcuts between a context vector and the input, to allow a model to attend to different parts. Below you can find a continuously updating list of attention mechanisms.', 'parent': 'Attention', 'area': 'General'}}, {'name': 'Transformer', 'full_name': 'Transformer', 'description': 'A **Transformer** is a model architecture that eschews recurrence and instead relies entirely on an [attention mechanism](https://paperswithcode.com/methods/category/attention-mechanisms-1) to draw global dependencies between input and output. Before Transformers, the dominant sequence transduction models were based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The Transformer also employs an encoder and decoder, but removing recurrence in favor of [attention mechanisms](https://paperswithcode.com/methods/category/attention-mechanisms-1) allows for significantly more parallelization than methods like [RNNs](https://paperswithcode.com/methods/category/recurrent-neural-networks) and [CNNs](https://paperswithcode.com/methods/category/convolutional-neural-networks).', 'introduced_year': 2000, 'source_url': 'http://arxiv.org/abs/1706.03762v5', 'source_title': 'Attention Is All You Need', 'code_snippet_url': 'https://github.com/tunz/transformer-pytorch/blob/e7266679f0b32fd99135ea617213f986ceede056/model/transformer.py#L201', 'main_collection': {'name': 'Transformers', 'description': '**Transformers** are a type of neural network architecture that have several properties that make them effective for modeling data with long-range dependencies. They generally feature a combination of multi-headed attention mechanisms, residual connections, layer normalization, feedforward connections, and positional embeddings.', 'parent': 'Language Models', 'area': 'Natural Language Processing'}}]",23e36087637e9d74815eba07990c38c02fecc966,"[{'name': 'Vladimir  Iashin', 'ids': ['47698311']}, {'name': 'Esa  Rahtu', 'ids': ['2827962']}]","['25efc17ba82ba4af29f2e03868de74e1ea66d025', '5388388db25f0d40ef6612333a0279373f8dddcf', '8c3fa8525e9997badb86d2c7b1fd5c676a9e6bd2', '674dd1fa7c67df6b5c68f3b5910790b980ebf413', 'f8acf4efc8ac817329486ed96af9a81a5602320f', 'd87489d2facf197caafd24d0796523d55d47fb62', 'bcffc406b4cc5b179ed973cd7f974c656e129c4f', 'fb2ec7696a6559de3fd43986b18655b0468120a9']","['616a23ebf79e35033c84797993943013c5dde5a0', '8136e4ed207be52d840edb78bfbfabeafc32ca28', '5f425b7abf2ed3172ed060df85bb1885860a297e', 'd92ef81e39d65d89f2c35d63088d1950ed862e7d', 'ea57ffa3e13400cad53dc061887a6fbbd45e7f12', '96dd1fc39a368d23291816d57763bc6eb4f7b8d6', 'bb4e2d6a6e3e1067f21a4cad087fc91c671e495d', 'dde65325dc7600d02983a76bd54693f0050946a4', '4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0', '35ed258aede3df17ee20a6635364cb5fd2461049', 'd25c65d261ea0e6a458be4c50c40ffe5bc508f77', 'b12124f7bbdd3a99d6b392024806d0f3124380ac', '74b284a66e75b65f5970d05bac000fe91243ee49', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', 'b74a094b6e35fab07e1a4694afd12cad9696f1c1', '778ce81457383bd5e3fdb11b145ded202ebb4970', 'd53a97a3dd7760b193c0d9a5293b60feff239059', '23ffaa0fe06eae05817f527a47ac3291077f9e58', 'e8cd37fbd8bd5e690eef5861cf92af8e002d4533', '726b1ade8b3d0023f0b4a9f86b7c2c3004885e37', 'c5a757427132fda0c66e18a0d059eca8e2472d13', '34f25a8704614163c4095b3ee2fc969b60de4698', '6979be4e3acbb6a5455946dc332565ccb10cf8de', 'a84ccaecfd0199f5c7910fd34aa832ee7db4c406', '19d7f83c3d7147f0eed1e1471438066eb4fe51fb', '5ba2218b708ca64ab556e39d5997202e012717d5', '1fcd73e0c09f35bfeb7d0db7426d50d3610bf46d', 'f01fc808592ea7c473a69a6e7484040a435f36d9', '9311779489e597315488749ee6c386bfa3f3512e', 'b910a6f687a4e56062dc326786cee297bd60e8c1', '44d2abe2175df8153f465f6c39b68b76a0d40ab9', '0d3b5ffff118326fea73341a86a7c29423eb95f0', 'd7da009f457917aa381619facfa5ffae9329a6e9', '8e561b60c6aea937f9d98ee336dde01abd1ff651', 'ff172624dd0a3bd31ca925b73cd7295d596173e2', 'fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5', 'e58a110fa1e4ddf247d5c614d117d64bfbe135c4', '8735ac2324b5aeaa3a8418af97eb82e9aa1910cb', '204e3073870fae3d05bcbc2f6a8e263d9b72e776', '2714a3932b9d096b7bb285f6ec415cb047eafe09', '99cdb10443a0543be3466c9231ff922bcc996843', '26adb749fc5d80502a6d889966e50b31391560d3', 'f678a0041f2c6f931168010e7418c500c3f14cdb', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', '08903ceeee6420992d30ff3f3b8b4830118af4d9', 'cea967b59209c6be22829699f05b8b1ac4dc092d', 'f66a2c5225551837b8894f94ae9feca0e406c9c1', '66aebb3af16aaa78579344784212ae10f60ec27e', '8cef41606f1e1324b683441e694f0e1c96387abf', 'd7ce5665a72c0b607f484c1b448875f02ddfac3b', '59d8c68de09da69a608ceb149f40114f5538c5b1', '889e723cd6d581e120ee6776b231fdf69707ab50', 'b61a3f8b80bbd44f24544dc915f52fd30bbdf485', '7a2de516a4e628a30036193d71faac7240d553ef', 'a23ab0fb7d9e9961e92d704ed71e3dbc15c0d908', '659e2f1d54b88252bcf08c4f3d54c0832a181c3e']",2020,https://semanticscholar.org/paper/23e36087637e9d74815eba07990c38c02fecc966,['DBLP'],"['https://arxiv.org/pdf/2003.07758v2.pdf', 'https://openaccess.thecvf.com/content_CVPRW_2020/papers/w56/Iashin_Multi-Modal_Dense_Video_Captioning_CVPRW_2020_paper.pdf', 'https://export.arxiv.org/pdf/2003.07758', 'https://doi.org/10.1109/CVPRW50498.2020.00487', 'https://arxiv.org/abs/2003.07758']",2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),"['Computer Science', 'Engineering']",3012386959,160100363,https://api.github.com/repos/v-iashin/MDVC,48108867,MDVC,PyTorch implementation of Multi-modal Dense Video Captioning.,Python,2020-04-14 13:37:39,\N,0,2021-03-02 8:49:21,\N,https://github.com/v-iashin/MDVC,53,12,Python,2020-04-14 13:37:39,2021-05-31 12:11:02,4,TRUE,TRUE,https://iashin.ai/mdvc,7,"['pytorch', 'transformer', 'multi-modal', 'dense-video-captioning', 'cvpr-workshop', 'audio', 'visual', 'speech', 'i3d', 'activitynet-captions']","<img src=""https://github.com/v-iashin/v-iashin.github.io/raw/master/images/mdvc/MDVC.svg"" alt=""MDVC"" width=""900"">

# Multi-modal Dense Video Captioning

[Project Page](https://v-iashin.github.io/mdvc) | [Proceedings](http://openaccess.thecvf.com/content_CVPRW_2020/html/w56/Iashin_Multi-Modal_Dense_Video_Captioning_CVPRW_2020_paper.html) | [ArXiv](https://arxiv.org/abs/2003.07758) | [Presentation](https://www.youtube.com/watch?v=0Vmx_gzP1bM) ([Mirror](https://a3s.fi/swift/v1/AUTH_a235c0f452d648828f745589cde1219a/mdvc/14-oral.mp4))

This is a PyTorch implementation of our paper Multi-modal Dense Video Captioning (CVPR Workshops 2020).

The publication will appear in the conference proceedings of CVPR Workshops. Please, use this bibtex citation
```
@InProceedings{MDVC_Iashin_2020,
  author = {Iashin, Vladimir and Rahtu, Esa},
  title = {Multi-Modal Dense Video Captioning},
  booktitle = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  pages={958--959},
  year = {2020}
}
```

If you found this work interesting, [check out](https://v-iashin.github.io/bmt) our latest paper, where we propose a novel architecture for the dense video captioning task called **Bi-modal Transformer with Proposal Generator**.

## Usage
_The code is tested on `Ubuntu 16.04/18.04` with one `NVIDIA GPU 1080Ti/2080Ti`. If you are planning to use it with other software/hardware, you might need to adapt `conda` environment files or even the code._

Clone this repository. Mind the `--recursive` flag to make sure `submodules` are also cloned (evaluation scripts for Python 3).
```bash
git clone --recursive https://github.com/v-iashin/MDVC.git
```

Download features [I3D (17GB)](https://a3s.fi/swift/v1/AUTH_a235c0f452d648828f745589cde1219a/mdvc/sub_activitynet_v1-3.i3d_25fps_stack24step24_2stream.hdf5), [VGGish (1GB)](https://a3s.fi/swift/v1/AUTH_a235c0f452d648828f745589cde1219a/mdvc/sub_activitynet_v1-3.vggish.hdf5) and put in `./data/` folder (speech segments are already there). You may use `curl -O <link>` to download the features.

```
# MD5 Hash
a661cfe3535c0d832ec35dd35a4fdc42  sub_activitynet_v1-3.i3d_25fps_stack24step24_2stream.hdf5
54398be59d45b27397a60f186ec25624  sub_activitynet_v1-3.vggish.hdf5
```

Setup `conda` environment. Requirements are in file `conda_env.yml`
```bash
# it will create new conda environment called 'mdvc' on your machine
conda env create -f conda_env.yml
conda activate mdvc
# install spacy language model. Make sure you activated the conda environment
python -m spacy download en
```

## Train and Predict
Run the training and prediction script. It will, first, train the captioning model and, then, evaluate the predictions of the best model in the learned proposal setting. It will take ~24 hours (50 epochs) to run on a 2080Ti GPU. Please, note that the performance is expected to reach its peak after ~30 epochs.
```bash
# make sure to activate environment: conda activate mdvc
# the cuda:1 device will be used for the run
python main.py --device_ids 1
```
The script keeps the log files, including `tensorboard` log, under `./log` directory by default. You may specify other path using `--log_dir` argument. Also, if you stored the downloaded data (`.hdf5`) files in another directory other than `./data`, make sure to specify it using `–-video_features_path` and `--audio_features_path` arguments.

You may also download the pre-trained model [here (~2 GB)](https://a3s.fi/swift/v1/AUTH_a235c0f452d648828f745589cde1219a/mdvc/best_model.pt).
```
# MD5 Hash
55cda5bac1cf2b7a803da24fca60898b  best_model.pt
```

## Evaluation Scrips and Results
If you want to skip the training procedure, you may replicate the main results of the paper using the prediction files in `./results` and the [official evaluation script](https://github.com/ranjaykrishna/densevid_eval/tree/9d4045aced3d827834a5d2da3c9f0692e3f33c1c).

1. To evaluate the performance in the learned proposal set up, run the official evaluation script on `./results/results_val_learned_proposals_e30.json`. Our final result is 6.8009
2. To evaluate the performance on ground truth segments, run the script on each validation part (`./results/results_val_*_e30.json`) against the corresponding ground truth files (use `-r` argument in the script to specify each of them). When both values are obtained, average them to verify the final result. We got 9.9407 and 10.2478 on `val_1` and `val_2` parts, respectively, thus, the average is 10.094.

As we mentioned in the paper, we didn't have access to the full dataset as [ActivityNet Captions](https://cs.stanford.edu/people/ranjaykrishna/densevid/) is distributed as the list of links to YouTube video. Consequently, many videos (~8.8 %) were no longer available at the time when we were downloading the dataset. In addition, some videos didn't have any speech. We filtered out such videos from the validation files and reported the results as `no missings` in the paper. We provide these filtered ground truth files in `./data`.

## Raw Data & Details on Feature Extraction
If you are feeling brave, you may want extract features on your own. Check out our script for extraction of the I3D and VGGish features from a set of videos: [video_features on GitHub](https://github.com/v-iashin/video_features/tree/4fa02bd5c5b8c34081dcfb609e2bcd5a973eaab2) (make sure to check out to `4fa02bd5c5b8c34081dcfb609e2bcd5a973eaab2` commit). Also see [#7](https://github.com/v-iashin/MDVC/issues/7) for more details on configuration. We also provide the script used to process the timestamps `./utils/parse_subs.py`.

## Misc.
We additionally provide
- the file with subtitles with original timestamps in `./data/asr_en.csv`
- the file with video categories in `./data/vid2cat.json`

## Acknowledgments
Funding for this research was provided by the Academy of Finland projects 327910 & 324346. The authors acknowledge CSC — IT Center for Science, Finland, for computational resources for our experimentation.

## Media Coverage
- (in Russian) [Рубрика «Читаем статьи за вас». Июнь 2020 года (habr.com)](https://habr.com/ru/company/ods/blog/515688/)
",28,111,52,114,348,2015-10-03 11:16:31,https://github.com/v-iashin/MDVC,12,32348,FALSE,no mirror url,1,0,0,User,Tampere University,vladimir.iashin@tuni.fi,_iashin,23,2022-06-10 18:07:32,no License,0,2.142226569,2.220442583,2,2,MDVC,pytorch,"<p><img src=""https://github.com/v-iashin/v-iashin.github.io/raw/master/images/mdvc/mdvc.svg"" alt=""mdvc"" width=""900""></p>
<h1>multi-modal dense video captioning</h1>
<p><a href=""https://v-iashin.github.io/mdvc"">project page</a> | <a href=""http://openaccess.thecvf.com/content_cvprw_2020/html/w56/iashin_multi-modal_dense_video_captioning_cvprw_2020_paper.html"">proceedings</a> | <a href=""https://arxiv.org/abs/2003.07758"">arxiv</a> | <a href=""https://www.youtube.com/watch?v=0vmx_gzp1bm"">presentation</a> (<a href=""https://a3s.fi/swift/v1/auth_a235c0f452d648828f745589cde1219a/mdvc/14-oral.mp4"">mirror</a>)</p>
<p>this is a pytorch implementation of our paper multi-modal dense video captioning (cvpr workshops 2020).</p>
<p>the publication will appear in the conference proceedings of cvpr workshops. please, use this bibtex citation</p>
<pre><code>@inproceedings{mdvc_iashin_2020,
  author = {iashin, vladimir and rahtu, esa},
  title = {multi-modal dense video captioning},
  booktitle = {the ieee/cvf conference on computer vision and pattern recognition (cvpr) workshops},
  pages={958--959},
  year = {2020}
}
</code></pre>
<p>if you found this work interesting, <a href=""https://v-iashin.github.io/bmt"">check out</a> our latest paper, where we propose a novel architecture for the dense video captioning task called <strong>bi-modal transformer with proposal generator</strong>.</p>
<h2>usage</h2>
<p><em>the code is tested on <code>ubuntu 16.04/18.04</code> with one <code>nvidia gpu 1080ti/2080ti</code>. if you are planning to use it with other software/hardware, you might need to adapt <code>conda</code> environment files or even the code.</em></p>
<p>clone this repository. mind the <code>--recursive</code> flag to make sure <code>submodules</code> are also cloned (evaluation scripts for python 3).</p>
<pre><code class=""lang-bash"">git clone --recursive https://github.com/v-iashin/mdvc.git
</code></pre>
<p>download features <a href=""https://a3s.fi/swift/v1/auth_a235c0f452d648828f745589cde1219a/mdvc/sub_activitynet_v1-3.i3d_25fps_stack24step24_2stream.hdf5"">i3d (17gb)</a>, <a href=""https://a3s.fi/swift/v1/auth_a235c0f452d648828f745589cde1219a/mdvc/sub_activitynet_v1-3.vggish.hdf5"">vggish (1gb)</a> and put in <code>./data/</code> folder (speech segments are already there). you may use <code>curl -o <link></code> to download the features.</p>
<pre><code># md5 hash
a661cfe3535c0d832ec35dd35a4fdc42  sub_activitynet_v1-3.i3d_25fps_stack24step24_2stream.hdf5
54398be59d45b27397a60f186ec25624  sub_activitynet_v1-3.vggish.hdf5
</code></pre>
<p>setup <code>conda</code> environment. requirements are in file <code>conda_env.yml</code></p>
<pre><code class=""lang-bash""># it will create new conda environment called &#39;mdvc&#39; on your machine
conda env create -f conda_env.yml
conda activate mdvc
# install spacy language model. make sure you activated the conda environment
python -m spacy download en
</code></pre>
<h2>train and predict</h2>
<p>run the training and prediction script. it will, first, train the captioning model and, then, evaluate the predictions of the best model in the learned proposal setting. it will take ~24 hours (50 epochs) to run on a 2080ti gpu. please, note that the performance is expected to reach its peak after ~30 epochs.</p>
<pre><code class=""lang-bash""># make sure to activate environment: conda activate mdvc
# the cuda:1 device will be used for the run
python main.py --device_ids 1
</code></pre>
<p>the script keeps the log files, including <code>tensorboard</code> log, under <code>./log</code> directory by default. you may specify other path using <code>--log_dir</code> argument. also, if you stored the downloaded data (<code>.hdf5</code>) files in another directory other than <code>./data</code>, make sure to specify it using <code>–-video_features_path</code> and <code>--audio_features_path</code> arguments.</p>
<p>you may also download the pre-trained model <a href=""https://a3s.fi/swift/v1/auth_a235c0f452d648828f745589cde1219a/mdvc/best_model.pt"">here (~2 gb)</a>.</p>
<pre><code># md5 hash
55cda5bac1cf2b7a803da24fca60898b  best_model.pt
</code></pre>
<h2>evaluation scrips and results</h2>
<p>if you want to skip the training procedure, you may replicate the main results of the paper using the prediction files in <code>./results</code> and the <a href=""https://github.com/ranjaykrishna/densevid_eval/tree/9d4045aced3d827834a5d2da3c9f0692e3f33c1c"">official evaluation script</a>.</p>
<ol>
<li>to evaluate the performance in the learned proposal set up, run the official evaluation script on <code>./results/results_val_learned_proposals_e30.json</code>. our final result is 6.8009</li>
<li>to evaluate the performance on ground truth segments, run the script on each validation part (<code>./results/results_val_*_e30.json</code>) against the corresponding ground truth files (use <code>-r</code> argument in the script to specify each of them). when both values are obtained, average them to verify the final result. we got 9.9407 and 10.2478 on <code>val_1</code> and <code>val_2</code> parts, respectively, thus, the average is 10.094.</li>
</ol>
<p>as we mentioned in the paper, we didn't have access to the full dataset as <a href=""https://cs.stanford.edu/people/ranjaykrishna/densevid/"">activitynet captions</a> is distributed as the list of links to youtube video. consequently, many videos (~8.8 %) were no longer available at the time when we were downloading the dataset. in addition, some videos didn't have any speech. we filtered out such videos from the validation files and reported the results as <code>no missings</code> in the paper. we provide these filtered ground truth files in <code>./data</code>.</p>
<h2>raw data &amp; details on feature extraction</h2>
<p>if you are feeling brave, you may want extract features on your own. check out our script for extraction of the i3d and vggish features from a set of videos: <a href=""https://github.com/v-iashin/video_features/tree/4fa02bd5c5b8c34081dcfb609e2bcd5a973eaab2"">video_features on github</a> (make sure to check out to <code>4fa02bd5c5b8c34081dcfb609e2bcd5a973eaab2</code> commit). also see <a href=""https://github.com/v-iashin/mdvc/issues/7"">#7</a> for more details on configuration. we also provide the script used to process the timestamps <code>./utils/parse_subs.py</code>.</p>
<h2>misc.</h2>
<p>we additionally provide</p>
<ul>
<li>the file with subtitles with original timestamps in <code>./data/asr_en.csv</code></li>
<li>the file with video categories in <code>./data/vid2cat.json</code></li>
</ul>
<h2>acknowledgments</h2>
<p>funding for this research was provided by the academy of finland projects 327910 &amp; 324346. the authors acknowledge csc — it center for science, finland, for computational resources for our experimentation.</p>
<h2>media coverage</h2>
<ul>
<li>(in russian) <a href=""https://habr.com/ru/company/ods/blog/515688/"">рубрика «читаем статьи за вас». июнь 2020 года (habr.com)</a></li>
</ul>
",3,3,28,0,0,TRUE,0,1,TRUE,FALSE,TRUE,10,12,TRUE,FALSE,FALSE,9,2103,2,740,7,TRUE,TRUE,TRUE,multi-modal dense video captioning,1,multi-modal dense video captioning,2,5,5,5,5,5,0,86,86,86,86,86,0,3,3,3,3,3,0,5,86,3,['no affiliation'],FALSE,5,86,3,['no affiliation'],FALSE,TRUE,FALSE,90,56,8,3,13,2,1,5,1,1,unknow,4,218
https://paperswithcode.com/paper/sequential-neural-likelihood-fast-likelihood,1805.07226,http://arxiv.org/abs/1805.07226v2,http://arxiv.org/pdf/1805.07226v2.pdf,https://github.com/gpapamak/snl,TRUE,TRUE,"We present Sequential Neural Likelihood (SNL), a new method for Bayesian
inference in simulator models, where the likelihood is intractable but
simulating data from the model is possible. SNL trains an autoregressive flow
on simulated data in order to learn a model of the likelihood in the region of
high posterior density. A sequential training procedure guides simulations and
reduces simulation cost by orders of magnitude. We show that SNL is more
robust, more accurate and requires less tuning than related neural-based
methods, and we discuss diagnostics for assessing calibration, convergence and
goodness-of-fit.",no proceeding,"['George Papamakarios', 'David C. Sterratt', 'Iain Murray']",['Bayesian Inference'],2018-05-18,[],af73d10a65f092f6530ad70e34a13ca0ccef03fa,"[{'name': 'George  Papamakarios', 'ids': ['3065681']}, {'name': 'David C. Sterratt', 'ids': ['3001384']}, {'name': 'Iain  Murray', 'ids': ['145797336']}]","['266148e854644f0ca3b7807b4bcad018ab4a3816', 'defd382df9bb61890ba0b05507f899047217e586', '4771dd1cdce3364d7437572d371c52ed25918b22', 'bec942264ff302b5d418436b1f333e6f1439cf87', 'a3f97fe81216ab600e14de409ce8166f71155792', 'bf0a72b386f894741d6b9f624ec8400ddeb04c9f', '6988447a106d310795dc2c5fc1656a4ef4240ea3', '4c6532024b60308c8cecd4534d29f49fcb5c853a', 'a0e9caaf76cead03a3cbd1215d76aa699fb6be2b', '0222acfef00d63fd94acfa1072bbe5b885717164', '1125abd1cbe1d2d9ec1cb52b99792fd48e6a89c9', '37fe7be95906bf040e2d72b53453ce0c8d864a56', '501c02c7caa7fc2c7077405299b4cbe7d294b170', 'c6263ecc77be18970c19162387a5a4dd38e334f3', '136f591a60591e9e9b0f175454e94ae157415090', 'fc9ee6149fe7d83ee0b6fd1ffe546fe64c019d1c', 'd3bc79619c6303ed7908f875fe785eb26da4a4f5', '67650fc84c045b62a7fe98b069e529e7f975afef', 'b3600338d67d63f1d637fd5c7b6c14ba35943d4e', 'aa875efd61b8736fd2a1da7d2bb8efb973eb381c', '2ce9c72aed2394f6fd0300679065a4ca8abf2c4c', '8cfe542c301f79bc1ba5a5220da12fcfaa3c0be6', 'a408682b1d15b9044b33ee84340971c51c41c839', 'e007ee59133a19f80d944aeeda8f0412a13534bb', 'e3209ae18e6f624af80f171ad0dc01deb7116337', '82bbd1303f06dc5efdea1633f178be6049c25892', '15ee82d7277bec0b3cf0b62130ce60f64b8aa93d', '3d7951e0c51828a558a52b6646c66be7a901a7d2', 'e44f9fc331b9711b2ec03f32af14dac12c3e5cb3', '43a19bd0a0144a9d6027a8659e2e0ebb465bfebd', '1d9dfd2bb1136960ff033923328e476e81a6a6b5', '31ef914eeef99f59ac00f7b68dea04fe0bd658f9', 'a584366533b7fe0ba75d842218ba8ef16ef08d5d', 'ec90205e3eeebaa83f86719efb1368cd4ae28d63', 'd4eb4655374c8711fb5e9bcd35b28576a7e43fc4', 'b525e1354507298f07a51563e6f09eb0ce868a87', '34497a9ea0165be25942eb4daf363c0a68f54212', '61845c21c3273d06ac48364c073ef624924cb2b9', '8416d997482c2459c3e5510a7bfd7c80838c78e7', 'f9916ed582b274f16b0631f7c3a876f232543105', '7c7e6c8ed53b7e50ec60eb3ebdabcd353ef55d89', 'c7f136c34df511c69ab695581fcb2d769317c71b', '0b1daab0a8d7f6824ff06ec79a09936f1be57961', '2a1680946455fa97dcb7f488afc911debb41ad0f', '9b2586d6e1e145e4411921126c9138b33fd49813', '36133eaf00b70bc8da46a5a0155b609da9303bf3', '44019572c54957ec44a904d4d931601cb896c40c', '47ba7f457772b287fd387f2d3a59dac2b2519fb0', 'eaba63f95fb332f08c14d808ca4a94356930687c', '38ccaf953d6f5c739b46b872a4316bb75d13c189', 'ee48a67fe7a9626a2bbdd1e6400621764f2cd8ee', '9dfe5d4fe11d5a708e9988e9a1464fd8afe3b797', 'bd7917f6f56bdeed5b262333b9a39bd9d9855bcb', 'f93938e0e5404a51b3b8a23fabd12db070cc4cfe', '425c1afc7373a260cc0e9a0673e083cd9fa2898a', '43b731f9d50ce82288b3ce505d634dbc906a9859', '67c4a4fe4ad4acbdc6be5caaaf41ae61c19470d8', '3999277a0e186c08bd65c729326bfa464c81106d', '4800ff7a90fb69596399d849974da5b0b751ae48', '4da094ab6823c70fb45f585e949638dcab2cccd5', 'c7a013e15be6fcef46f6a211b62f0410f57d4a5c', '42f72e45b016273142d3bb7992076290f96915ba', '5e583bf0ccab9b25849c924bff514b0a60a7a13a', 'a1d1aaf7005c1e0513e6efdfd67ffb06b316a667', 'd5c30edb8e53fe4dbd7bc778697c84803b093ae6', 'a0f05af40a2b8111069ce70679665fad5a220414', '03c346060b2cbf04947305f0a5a36eef6ca852ec', '384acf878982209f8e390831154c79c26c2bd990', 'ba28ede4d56e38c374cb836b8b9c50241ae2bdab', '67e4d0e24883c1c2bcd6db8d587e1d69acbdec5c', '4db3338f4bb382fabf92cec4a3d632276c7430e8', 'aacd6b645de854bd4696b4405b6989213857cd69', '308a9883b8e5cc8dc606767ad7d1019717f29dd5', '1f8ebeafff428718da7295eb48fe5722bdc15866', '9702b20a18475637005288c5f0216e31a0d7bcc7', '6c5fee4ba7315bf3ada38806f4d84a6d10e18715', '9d5233ea2135e7bc9023127af5b11a6af92e19cd', '96599a68d35860272c850d33c8d2a36bf66413fe', '24f43ba810b36f896996083928678849cdbc180c', 'e518c9f6c730dfc6deecfb10a5835ebde871dfab', 'f774a7b3a843736cfa477df718243b90a5d83792', '1c8c04636cbb22d8cf98fec56ab5ec46fb3e0303', '7b43dd7a1279dce87dba1603408360b5cc9c0f8c', 'ab5d773c45981bd3c4e18471af75f54d99cb269c', 'a9cbbef8f4426329d0687025b34287c35bdd8b38', '36b45f49c270e1ef2d3d997bf1fbbe59fe91fc4a', '0b95a631264af394acdf12b6499ab68df797c00a', '1f6b2b4aa9a0a299289dbffaf974a544a279df10', 'e474d8f1cdfcc04373bdc976e3fd07575f7fe797', '53468428adfe166d95be0fa782cf4fb10262a512', 'b98697c26c5aaafb7f2e16ee4c0e9a483a08cf29', '2229959efed8a6580552560681609fc38fee8260', '0c662377ebfe8f1b37fc4718aff8e70caca28947']","['4d376d6978dad0374edfa6709c9556b42d3594d3', '8b529af22b29a45d3a1b553e5e932d1efaffbc21', '87ccd3e61d2a70794a65fac03f158b90d867d3d2', 'dc54c99981bc3624ac36269a087cc1a1ff1dff9f', 'f6cbf83e1ce3b099d656d2346b261d5ef7f2b62e', '76934c996f10a29ee03f97b4b262a435822af46a', '4cb8f307288d9151a69a07d3dff30d7a2f2059b1', 'fbd616a659e8412ba37f1bd54cfe8ed543a35eb6', '375aa8e5ed444c6f83e2abeff8475f76833a0a2e', '7d52e2eb00fd3fa8bbd7235017fa003ba3ccf3ad', '69f048cf133e7eb25cb45e5777722794919a6292', 'b64185358bbe743635129b3462da481606bdbae8', '09a1fe15402611e9ccad9332c9eddad1e288eb71', '9797bba598d49fc626fbcec9ede895e08923c4ec', '0ee392ad467b967c0a32d8ecb19fc20f7c1d62fe', '41f1d50c85d3180476c4c7b3eea121278b0d8474', 'c734d113159b728668193179eadd20339a6ce925', '0f73e63043ae8d2af837b04abe46b56b19c83b0b', 'aa77874c8b9d53d96022864a7687e3e1dd9add5c', '74974454d1e1805de5e890bc38b36baed2b4ec19', '1125abd1cbe1d2d9ec1cb52b99792fd48e6a89c9', '5f3574eeab18c6702a0903375c0cce15cfc7415f', 'd7633503b0f45f491ccbca8a7826d918dae974c3', 'b66c0d47798432ce6b263e878841cf15d9882338', 'ba337e17d3b7388188c9fd93368809fa500498df', 'ac2edcd49138fcb3ca4a332f27f6350bde2e764a', '0ce2fa7500a81b8946bbd42720a921534f0b3f61', '8968c3a765423c773e1236da9ca25a41138e7fbf', '356a59100c06df4c6f1b9ee9e57fd0475d19dbda', '128628997c4fdcd4b0e98863ff6f4c9dd1a27c63', 'fea0b2dc5b05946eae371ddd4586b91671410145', '585bf7bea8fa5267738bc465611d6f197e0f87dd', 'df0402517a7338ae28bc54acaac400de6b456a46', '4cf4429f11acb8a51a362cbcf3713c06bba5aec7', '9869df823c3bb68a0164a67d60475981dba17cfd', '5bb6dbe2f855a772bec8861d41f934432476ec91', 'c07e98b0d5b65d8bfb9e41ad382bc815a60c880e', '0e3b4249ca1a365d00eaae4545588bc34288b60d', 'd4eb4655374c8711fb5e9bcd35b28576a7e43fc4', '9d7fee3a35a954a1b19f57ff15eff9ca90ad327b', '6b130b8f2074ec2ffd59a8183a7b20746ff9ad37', '3ac96506e1ccef100166ac412c7e91de05f346fd', '77fccf96a6f81cefea458616ab92359149ae852b', '100123870656318bc7fa1ecb439e90d43eaf7c40', '502e8e0d8502ff9d8cbfc565d9360afdfb2aea45', '90d5a98943d4c9f22a79843898568e3a2e0b29e7', '0c1055fcd5df14147596c98c0d6f85e607922451', '4f756a37ecc75a50a834db7239912a32a9a85d85', 'e11817ce34636abb2aedf31442c040aee12208a4', '8a53376675290dfad1ef574aad2f8fecf8d77585', 'd98f6c826d569baf6e2dff80a8808bcb348cc4aa', '2fe6acc045395d7e04cc37a736a0925a826848aa', 'e56dea3fc1cd5ffb8e711098a7660afd69ab42e2', '5696fda6738e16401c3dc3187c6c0f1fa5449bf3', '8badd2a020ccb215a02cda4dcbc206d17c548092', '0b3f8f0e0811180f2dc3e246ee5fa561807b9ada', 'd98d0d1900b13b87aa4ffd6b69c046beb63f0434', '0d499ac0de809d38210140ab6e21c2e399838987', '8c33cc55a811665e5dc299f4d350b87f08f8727d', '63cac9352d4a5c77eb9a16466633adf2841a6f05', 'e7ad8adbb447300ecafb4d00fb84efc3cf4996cf', 'f75e9becfa372e7a0fe6af20bf51b07dc3af6a08', 'f0ddb2bc6e5464d992ddbcdfdc7e894150fc81f2', '74a6c2b6408fb1a14664b08c018fefa6aee22dc7', '9f649b234f5ebf20757e611f2e9708ddb6954d5e', '4123ac46635b015227f06370e367c828d6ccffd3', 'f79f56aabd0db8b2f6024b159590362071532175', 'df8e36699a4a95ebfff7e26984c02ab6731be3dc', 'eb47f3e90dc9c3fa8de09f3dccb4f16be4d720d4', '69750b05335072b63affb2314f5c4a4e0fac97f7', '90f72fbbe5f0a29e627db28999e01a30a9655bc6', '5e0d88d700c281a1f48bac768686c7556596d766', '4d4748a6e8ed9b641f6b6a15f61b35fc5618aef7', '09879f7956dddc2a9328f5c1472feeb8402bcbcf', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', 'a1cd00d5bfb7abc5a874508518d44083968edd91', 'ab515c33ced707b35b480330155ab51b1729dbbf', '4cf3569e045993dfe090749f26a55a768684ab86', '65cd14495383a10e4b91ee916477dd7f8030c789', '5c306ce578a8da634a4a64fce282a48d0eacfda1', '0936352b78a52bc5d2b5e3f04233efc56664af51', '225f78ae8a44723c136646044fd5c5d7f1d3d15a', '5a953e9fed819acc65d37f1f32f9c079eb98ff46', '4792e7dd12bfebeb00cbb29d29dd84f07c9a978d', 'c80c6d0c8f0f8061c19fe6c3d090bc2782576b76', '098a47f3bb2176b70ca59d5565dde66f71c3281e', 'cf363c1a2010d6b34138cf0e44438a9daa6527ac', 'd7498672d81707c01b539d20eb67b27664f16d41']",2019,https://semanticscholar.org/paper/af73d10a65f092f6530ad70e34a13ca0ccef03fa,['DBLP'],"['http://export.arxiv.org/pdf/1805.07226', 'https://www.research.ed.ac.uk/portal/files/80437246/Sequential_Neural_Likelihood.pdf', 'http://arxiv.org/abs/1805.07226', 'https://arxiv.org/pdf/1805.07226v2.pdf', 'https://www.research.ed.ac.uk/portal/files/80437243/SNL_appendix.pdf', 'http://proceedings.mlr.press/v89/papamakarios19a/papamakarios19a.pdf', 'http://proceedings.mlr.press/v89/papamakarios19a.html']",AISTATS,"['Computer Science', 'Mathematics']",2804425690,98340431,https://api.github.com/repos/gpapamak/snl,30956680,snl,Sequential Neural Likelihood,\N,2018-05-17 22:23:23,\N,0,2020-12-04 5:07:12,\N,https://github.com/gpapamak/snl,23,11,Python,2018-05-17 22:25:53,2019-09-12 22:35:35,0,TRUE,TRUE,no homepageUrl,4,[],"# Sequential Neural Likelihood

Code for reproducing the experiments in the paper:

> G. Papamakarios, D. C. Sterratt, I. Murray, _Sequential Neural Likelihood: Fast Likelihood-free Inference with Autoregressive Flows_, AISTATS 2019.
> [[arXiv]](https://arxiv.org/abs/1805.07226) [[bibtex]](https://gpapamak.github.io/bibtex/snl.bib)

## How to run the experiments

Each file in folder `exps` contains the description of one or more experiments. Using Lotka-Volterra as an example, the files are organized like this:

File           | Experiments
---------------|------------
`lv_nl.txt`    | Neural Likelihood
`lv_seq.txt`   | Sequential methods: SNL, SNPE-A and SNPE-B
`lv_smc.txt`   | SMC-ABC
`lv_sl.txt`    | Synthetic Likelihood
`lv_calib.txt` | Calibration experiment for SNL

Replace `lv` with `gauss` for the toy Gaussian model, `mg1` for the M/G/1 queue, or `hh` for Hodgkin-Huxley.  Note that to run the Hodgkin-Huxley simulation, it is necessary to install NEURON; see [How to install NEURON](#how-to-install-neuron).

You can run all experiments in file `lv_nl.txt` by:
```
python main.py run exps/lv_nl.txt
```
Depending on the experiment, this can take from a couple of hours to a couple of weeks.
After the experiments are finished, the results will be saved in folder `data/experiments`.
After running the experiments, you can view the results by:
```
python main.py view exps/lv_nl.txt
```
This will produce several plots, depending on the experiment.
You can also print the experiment log by:
```
python main.py log exps/lv_nl.txt
```
This will print the experiment log on the screen.

For the calibration test, you need to run a particular experiment multiple times, each time with different parameters randomly drawn from the prior. You can do this by:
```
python main.py trials 1 200 exps/lv_calib.txt
```
This runs the experiment(s) described in `lv_calib.txt` for 200 trials, and labels the trials 1..200. Each trial is independent, so you can save time by issuing multiple commands in paralell, for example:
```
python main.py trials 1 10 exps/lv_calib.txt
python main.py trials 11 20 exps/lv_calib.txt
...
python main.py trials 191 200 exps/lv_calib.txt
```
This will produce the same result as above.

## How to reproduce the figures

After having run all the experiments in folder `exps`, you can reproduce the figures as follows:

Command                                | Figure it reproduces
---------------------------------------|----------------------------
`python plot_results_mmd.py <sim>`     | MMD vs simulation cost (only works for the Gaussian model)
`python plot_results_lprob.py <sim>`   | Minus log probability vs simulation cost
`python plot_results_dist.py <sim>`    | Distance vs number of rounds
`python plot_results_gof.py <sim>`     | Likelihood goodness-of-fit vs simulation cost
`python plot_results_calib.py <sim>`   | Histograms for the calibration test
`python plot_results_post.py <sim>`    | Posterior histograms and pairwise scatter plots for SNL

Here, `<sim>` can be any of `gauss`, `mg1`, `lv`, or `hh`. The first time you run one of the above it may take a long time, because the code will be calculating lots of intermediate results, such as MCMC samples, MMDs, etc. All intermediate results will be saved in folder `data/results` for future use. The second time you run it, the figures should appear immediately.

## How to install NEURON

The SNL code has been tested with NEURON on Scientific Linux 7.3. The following steps should allow the code to run on multiple versions of Linux:

1. Install NEURON 7.5 from using the 64 bit .deb (Debian or Ubuntu) or .rpm (Fedora derivatives) precompiled installer from https://neuron.yale.edu/neuron/download/precompiled-installers

2. Set `PYTHONPATH=/usr/local/nrn/lib/python:$PYTHONPATH`

3. In this directory run `nrnivmodl`, which will compile the `.mod` files into an executable file in a new directory, `x86_64`.
",15,211,142,110,418,2014-05-30 18:04:59,https://github.com/gpapamak/snl,12,115,FALSE,no mirror url,2,0,0,User,no company,no email or private email,gpapamak,9,2022-05-13 13:40:56,"MIT License

Copyright (c) 2018 George Papamakarios

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""Software""), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
",1,4.052281317,4.05210237,4,4,snl,none,"<h1>sequential neural likelihood</h1>
<p>code for reproducing the experiments in the paper:</p>
<blockquote><p>g. papamakarios, d. c. sterratt, i. murray, <em>sequential neural likelihood: fast likelihood-free inference with autoregressive flows</em>, aistats 2019.
<a href=""https://arxiv.org/abs/1805.07226"">[arxiv]</a> <a href=""https://gpapamak.github.io/bibtex/snl.bib"">[bibtex]</a></p>
</blockquote>
<h2>how to run the experiments</h2>
<p>each file in folder <code>exps</code> contains the description of one or more experiments. using lotka-volterra as an example, the files are organized like this:</p>
<table>
<thead><tr>
<th>file</th>
<th>experiments</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>lv_nl.txt</code></td>
<td>neural likelihood</td>
</tr>
<tr>
<td><code>lv_seq.txt</code></td>
<td>sequential methods: snl, snpe-a and snpe-b</td>
</tr>
<tr>
<td><code>lv_smc.txt</code></td>
<td>smc-abc</td>
</tr>
<tr>
<td><code>lv_sl.txt</code></td>
<td>synthetic likelihood</td>
</tr>
<tr>
<td><code>lv_calib.txt</code></td>
<td>calibration experiment for snl</td>
</tr>
</tbody>
</table>
<p>replace <code>lv</code> with <code>gauss</code> for the toy gaussian model, <code>mg1</code> for the m/g/1 queue, or <code>hh</code> for hodgkin-huxley.  note that to run the hodgkin-huxley simulation, it is necessary to install neuron; see <a href=""#how-to-install-neuron"">how to install neuron</a>.</p>
<p>you can run all experiments in file <code>lv_nl.txt</code> by:</p>
<pre><code>python main.py run exps/lv_nl.txt
</code></pre>
<p>depending on the experiment, this can take from a couple of hours to a couple of weeks.
after the experiments are finished, the results will be saved in folder <code>data/experiments</code>.
after running the experiments, you can view the results by:</p>
<pre><code>python main.py view exps/lv_nl.txt
</code></pre>
<p>this will produce several plots, depending on the experiment.
you can also print the experiment log by:</p>
<pre><code>python main.py log exps/lv_nl.txt
</code></pre>
<p>this will print the experiment log on the screen.</p>
<p>for the calibration test, you need to run a particular experiment multiple times, each time with different parameters randomly drawn from the prior. you can do this by:</p>
<pre><code>python main.py trials 1 200 exps/lv_calib.txt
</code></pre>
<p>this runs the experiment(s) described in <code>lv_calib.txt</code> for 200 trials, and labels the trials 1..200. each trial is independent, so you can save time by issuing multiple commands in paralell, for example:</p>
<pre><code>python main.py trials 1 10 exps/lv_calib.txt
python main.py trials 11 20 exps/lv_calib.txt
...
python main.py trials 191 200 exps/lv_calib.txt
</code></pre>
<p>this will produce the same result as above.</p>
<h2>how to reproduce the figures</h2>
<p>after having run all the experiments in folder <code>exps</code>, you can reproduce the figures as follows:</p>
<table>
<thead><tr>
<th>command</th>
<th>figure it reproduces</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>python plot_results_mmd.py <sim></code></td>
<td>mmd vs simulation cost (only works for the gaussian model)</td>
</tr>
<tr>
<td><code>python plot_results_lprob.py <sim></code></td>
<td>minus log probability vs simulation cost</td>
</tr>
<tr>
<td><code>python plot_results_dist.py <sim></code></td>
<td>distance vs number of rounds</td>
</tr>
<tr>
<td><code>python plot_results_gof.py <sim></code></td>
<td>likelihood goodness-of-fit vs simulation cost</td>
</tr>
<tr>
<td><code>python plot_results_calib.py <sim></code></td>
<td>histograms for the calibration test</td>
</tr>
<tr>
<td><code>python plot_results_post.py <sim></code></td>
<td>posterior histograms and pairwise scatter plots for snl</td>
</tr>
</tbody>
</table>
<p>here, <code><sim></code> can be any of <code>gauss</code>, <code>mg1</code>, <code>lv</code>, or <code>hh</code>. the first time you run one of the above it may take a long time, because the code will be calculating lots of intermediate results, such as mcmc samples, mmds, etc. all intermediate results will be saved in folder <code>data/results</code> for future use. the second time you run it, the figures should appear immediately.</p>
<h2>how to install neuron</h2>
<p>the snl code has been tested with neuron on scientific linux 7.3. the following steps should allow the code to run on multiple versions of linux:</p>
<ol>
<li><p>install neuron 7.5 from using the 64 bit .deb (debian or ubuntu) or .rpm (fedora derivatives) precompiled installer from <a href=""https://neuron.yale.edu/neuron/download/precompiled-installers"">https://neuron.yale.edu/neuron/download/precompiled-installers</a></p>
</li>
<li><p>set <code>pythonpath=/usr/local/nrn/lib/python:$pythonpath</code></p>
</li>
<li><p>in this directory run <code>nrnivmodl</code>, which will compile the <code>.mod</code> files into an executable file in a new directory, <code>x86_64</code>.</p>
</li>
</ol>
",1,5,30,0,0,FALSE,2,0,FALSE,FALSE,FALSE,7,26,TRUE,FALSE,FALSE,45,8722,32,551,3,FALSE,FALSE,FALSE,sequential neural likelihood: fast likelihood-free inference with autoregressive flows,0,sequential neural likelihood: fast likelihood-free inference with autoregressive flows,3,23,23,23,23,23,0,2292,2292,2292,2292,2292,0,15,15,15,15,15,0,23,2292,15,['no affiliation'],FALSE,23,2292,15,['no affiliation'],FALSE,FALSE,FALSE,30,88,93,1,0,2,1,7,1,1,unknow,9,93
https://paperswithcode.com/paper/reinforcement-learning-through-asynchronous,1611.06256,http://arxiv.org/abs/1611.06256v3,http://arxiv.org/pdf/1611.06256v3.pdf,https://github.com/NVlabs/GA3C,TRUE,FALSE,"We introduce a hybrid CPU/GPU version of the Asynchronous Advantage
Actor-Critic (A3C) algorithm, currently the state-of-the-art method in
reinforcement learning for various gaming tasks. We analyze its computational
traits and concentrate on aspects critical to leveraging the GPU's
computational power. We introduce a system of queues and a dynamic scheduling
strategy, potentially helpful for other asynchronous algorithms as well. Our
hybrid CPU/GPU version of A3C, based on TensorFlow, achieves a significant
speed up compared to a CPU implementation; we make it publicly available to
other researchers at https://github.com/NVlabs/GA3C .",no proceeding,"['Mohammad Babaeizadeh', 'Iuri Frosio', 'Stephen Tyree', 'Jason Clemons', 'Jan Kautz']",['reinforcement-learning'],2016-11-18,"[{'name': 'Entropy Regularization', 'full_name': 'Entropy Regularization', 'description': '**Entropy Regularization** is a type of regularization used in [reinforcement learning](https://paperswithcode.com/methods/area/reinforcement-learning). For on-policy policy gradient based methods like [A3C](https://paperswithcode.com/method/a3c), the same mutual  reinforcement behaviour leads to a highly-peaked $\\pi\\left(a\\mid{s}\\right)$ towards a few actions or action sequences, since it is easier for the actor and critic to overoptimise to a small portion of the environment. To reduce this problem, entropy regularization adds an entropy term to the loss to promote action diversity:\r\n\r\n$$H(X) = -\\sum\\pi\\left(x\\right)\\log\\left(\\pi\\left(x\\right)\\right) $$\r\n\r\nImage Credit: Wikipedia', 'introduced_year': 2000, 'source_url': 'http://arxiv.org/abs/1602.01783v2', 'source_title': 'Asynchronous Methods for Deep Reinforcement Learning', 'code_snippet_url': 'https://github.com/ikostrikov/pytorch-a3c/blob/48d95844755e2c3e2c7e48bbd1a7141f7212b63f/train.py#L100', 'main_collection': {'name': 'Regularization', 'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.', 'parent': None, 'area': 'General'}}, {'name': 'Convolution', 'full_name': 'Convolution', 'description': 'A **convolution** is a type of matrix operation, consisting of a kernel, a small matrix of weights, that slides over input data performing element-wise multiplication with the part of the input it is on, then summing the results into an output.\r\n\r\nIntuitively, a convolution allows for weight sharing - reducing the number of effective parameters - and image translation (allowing for the same feature to be detected in different parts of the input space).\r\n\r\nImage Source: [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)', 'introduced_year': 1980, 'source_url': None, 'source_title': None, 'code_snippet_url': None, 'main_collection': {'name': 'Convolutions', 'description': '**Convolutions** are a type of operation that can be used to learn representations from images. They involve a learnable kernel sliding over the image and performing element-wise multiplication with the input. The specification allows for parameter sharing and translation invariance. Below you can find a continuously updating list of convolutions.', 'parent': 'Image Feature Extractors', 'area': 'Computer Vision'}}, {'name': 'Dense Connections', 'full_name': 'Dense Connections', 'description': '**Dense Connections**, or **Fully Connected Connections**, are a type of layer in a deep neural network that use a linear operation where every input is connected to every output by a weight. This means there are $n\\_{\\text{inputs}}*n\\_{\\text{outputs}}$ parameters, which can lead to a lot of parameters for a sizeable network.\r\n\r\n$$h\\_{l} = g\\left(\\textbf{W}^{T}h\\_{l-1}\\right)$$\r\n\r\nwhere $g$ is an activation function.\r\n\r\nImage Source: Deep Learning by Goodfellow, Bengio and Courville', 'introduced_year': 2000, 'source_url': None, 'source_title': None, 'code_snippet_url': None, 'main_collection': {'name': 'Feedforward Networks', 'description': '**Feedforward Networks** are a type of neural network architecture which rely primarily on dense-like connections. Below you can find a continuously updating list of feedforward network components.', 'parent': None, 'area': 'General'}}, {'name': 'Softmax', 'full_name': 'Softmax', 'description': ""The **Softmax** output function transforms a previous layer's output into a vector of probabilities. It is commonly used for multiclass classification.  Given an input vector $x$ and a weighting vector $w$ we have:\r\n\r\n$$ P(y=j \\mid{x}) = \\frac{e^{x^{T}w_{j}}}{\\sum^{K}_{k=1}e^{x^{T}wk}} $$"", 'introduced_year': 2000, 'source_url': None, 'source_title': None, 'code_snippet_url': None, 'main_collection': {'name': 'Output Functions', 'description': '**Output functions** are layers used towards the end of a network to transform to the desired form for a loss function. For example, the softmax relies on logits to construct a conditional probability. Below you can find a continuously updating list of output functions.', 'parent': None, 'area': 'General'}}, {'name': 'A3C', 'full_name': 'A3C', 'description': ""**A3C**, **Asynchronous Advantage Actor Critic**, is a policy gradient algorithm in reinforcement learning that maintains a policy $\\pi\\left(a\\_{t}\\mid{s}\\_{t}; \\theta\\right)$ and an estimate of the value\r\nfunction $V\\left(s\\_{t}; \\theta\\_{v}\\right)$. It operates in the forward view and uses a mix of $n$-step returns to update both the policy and the value-function. The policy and the value function are updated after every $t\\_{\\text{max}}$ actions or when a terminal state is reached. The update performed by the algorithm can be seen as $\\nabla\\_{\\theta{'}}\\log\\pi\\left(a\\_{t}\\mid{s\\_{t}}; \\theta{'}\\right)A\\left(s\\_{t}, a\\_{t}; \\theta, \\theta\\_{v}\\right)$ where $A\\left(s\\_{t}, a\\_{t}; \\theta, \\theta\\_{v}\\right)$ is an estimate of the advantage function given by:\r\n\r\n$$\\sum^{k-1}\\_{i=0}\\gamma^{i}r\\_{t+i} + \\gamma^{k}V\\left(s\\_{t+k}; \\theta\\_{v}\\right) - V\\left(s\\_{t}; \\theta\\_{v}\\right)$$\r\n\r\nwhere $k$ can vary from state to state and is upper-bounded by $t\\_{max}$.\r\n\r\nThe critics in A3C learn the value function while multiple actors are trained in parallel and get synced with global parameters every so often. The gradients are accumulated as part of training for stability - this is like parallelized stochastic gradient descent.\r\n\r\nNote that while the parameters $\\theta$ of the policy and $\\theta\\_{v}$ of the value function are shown as being separate for generality, we always share some of the parameters in practice. We typically use a convolutional neural network that has one [softmax](https://paperswithcode.com/method/softmax) output for the policy $\\pi\\left(a\\_{t}\\mid{s}\\_{t}; \\theta\\right)$ and one linear output for the value function $V\\left(s\\_{t}; \\theta\\_{v}\\right)$, with all non-output layers shared."", 'introduced_year': 2000, 'source_url': 'http://arxiv.org/abs/1602.01783v2', 'source_title': 'Asynchronous Methods for Deep Reinforcement Learning', 'code_snippet_url': None, 'main_collection': {'name': 'Policy Gradient Methods', 'description': '**Policy Gradient Methods** try to optimize the policy function directly in reinforcement learning. This contrasts with, for example Q-Learning, where the policy manifests itself as maximizing a value function. Below you can find a continuously updating catalogue of policy gradient methods.', 'parent': None, 'area': 'Reinforcement Learning'}}]",48830e2e4272fa88dc256f1ac9cf81be14112bdb,"[{'name': 'Mohammad  Babaeizadeh', 'ids': ['3365707']}, {'name': 'Iuri  Frosio', 'ids': ['145134808']}, {'name': 'Stephen  Tyree', 'ids': ['2342481']}, {'name': 'Jason  Clemons', 'ids': ['144334042']}, {'name': 'Jan  Kautz', 'ids': ['1690538']}]","['1c32a1001fc10bc5d1d84d77c8b91fe661c9864c', 'ad35de1aeb563d5ce74b7554bde592f20ba6e0a6', 'f79bc5e08d5d00c486baefb29e2306715d991b69', 'df6fb7cbf367dbf2eb814b40684195e0c7b99824', '7b35984b9b0bde78290cf92d8176682b65e59f54', 'd7e6649a576b22499aced0ba2e36b03906eb16d0', '511a22cfbb3ea1a3328f65f6549bbfe60d64c85d', '77f8e71563265f3d09f242367e661d0fd14ead2d', 'fb9b0a6e88ca6e3cef9fc6ba060b27c5303da258', '5ed6ea9e59a4f1945bb9707d871ff1041bfb3487', '9f17313e0d7b173765800e6f471407544e0f2202', '48d42e5cc002560996d0b9d19c65401b77243dc2', '7bf5a9fdc14dbcaef58f257f7be547cf1f48be91', 'b88243d0f8301ca02c82d8e234c67dc5a78dce7a', '47231dec371ac92a5caabf64508ed5332cf7e4f8', '26ef18d87b200c5a00b0592609cba7b527a4d8d6', '48fb041e0a50fabd44581f920e6f5229ca6d691a', 'add7b8b65355d5408a1ffb93a94b0ae688806bc4', '1f08598381af9146d0fd9a61b30d0e51a7331689', '3cf31ae1a2e97a637e3cb1045fb3f6b36a71aca8', '4dfaa337fb4cc3d7f755c258f192edc774f601fc', '3da43bd9e6d691600e97cd04322660d324d623c0', '3f43f08611cbcfba62bb9e0c5339c2a8f0cc3e4b', 'f4eb729c9ec71d1fc4a220c4ae6a97425d2a8618', 'a51294166c33df536676d05637c322aa97d9ecb1', '91a0af215172aa8cdc2a0a4cc4568ae4b4829047', '1069b4be1174f171f0e0fba111bed73804c06cdf', '694325327132f56a9dae634b92b2f4fbf1a2b2ae', '116e0fd34219dca8c7d83668927091d1b75d1260', '0b70398c3e24570dfc802cb0c89c6cf5e3b3a15f', 'b8a1c237ffdc79d2ec5647121d143e051b59368b', '4292c58c46629d74317cdfe944a1cfc7b9d1a3cf', 'db3ca29cbc01bf5a752d8842789092d8385d0e24', 'f2cc53460120139e10b5c541a759a2e4ca2d38e1', '9ab5a496fb714c130ff03b2fec9da75f5df57b79', '441e66e7fd773444ef6534b42c662116199409e6', '1fd4694e7c2d9c872a427d50e81b5475056de6bc', 'ee638036826cef13f3d4c455cc11538cc5642a73', '8e4c2f60e2e81b68b65ab202462583718ecd44b0', '551768e573bb28b3a2fad4993adbc5c028b95489', '344e982196d2323346c5b86a89be3e82f4c5326a', '0e04e5e777d1f82b16dc58abb408b4b18e1bcc08', '0780eb24edccb75b9cbba17c0c1e6cc3da8eed37', 'e84cd2f61ebd97d3f73807f38a337de296799c3a', '390364c3986d05ad71a40a967b9cc12aa30e4305', 'd9b4c4c71c4970ae5503e14dcd8858b819e40d75', '913f8972992116662e0701004b4a073bcf5c235d', '4fe9142a47b35638edcc59013ad5e9e87bd93ea7', '34b5da5845a4307eb8342efcb95e5155f661b5f8', 'f2dde617cdbc01d93537d69df4c86bf4271cde64', 'a8f80b8f5cd4be2da9b8e4b9931135f16cbaa5c8', 'fd7d0c82b7da11a034846f6848d146feb01016e7', '76d366c2949d438c6bde26b5f2f7cf02d7ad5c46', '0c5ed03762e8725834fc49c6e213853715a2fae6', '1c73a5c449c04489ef965d7d0a7ec1a565d88e5c', '814426f20ade4345feb9d5a9c1ee2f1553c21076', 'efb2bfdcb3d9d5407c0d2e4a60ab28f991dd6f19', 'f3161b75de1e37b0591f250068b676ea72d1ba22', '688fb5c92aea1fbcf43f521432ca45f8714aab1c', 'ed3332a795ab9809af746d005095ee2eaeb4566f', 'efadee37d835d0352d7677da1dcd02ba2e6522b7', '2481f0f5aa59f8321e5b9e9289f8274cae86b8a0', 'eca5f0963b77f6bbf02549807b19449c08b7ba62', 'f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751', 'c5815903a5469b57ba4617a377d7be40e7b68242', 'd4f1c23948cdaeaedbdc9d24000ae19dedb4815e', 'bbad4d025aaad9ea8f62d11fef9b600df1f3e3c5', '4aa546b8159a0dddcec490d13620f395bc4c5133', '142a1697067c89c05330f631ed1b1dc5f07b48b2', 'e7ef32db6e6a0e37fa68da6a432f2cd53cb8096f', 'c1d4b568afe2b29a49b09a60c525a21170eb30c5', '930ff196d2adf210acf849579a5ea376ff4cf1ce', 'a41320eda6278de3e9e22c5c992f6b57c46580c2', 'a9d8c7bcbe7d6a46edacf6d8241c3a719e897b21', '39d759d2b2e1e896757243d438aed646f0823e35', '364592630d84661b5b516719f5d46dd3bccbd6b7', '7d62b504a3b874a8aa22775a750708007040db6a', '1323aaac8eb57017698665074e403147f8bedf62', '050a89a91b3e828c841972a81f17807f82c79713', '630f3d486a254ae2507353e308fa33af64fb7283', '9f1e9e56d80146766bc2316efbc54d8b770a23df', 'b463909212f802a338cc204e663e65aafc36412b', 'e764c21eadff89810a283ae80b76b6341bb29771', '3473d81d07b7301adcf75272d780746a70698716', 'e852e178d58dc8aedbd836003202d2324283a914', '6874b196102f1de902bef1679ec26337bde6891a', '5c70798bec0478d673cc196aef4704cc72f8d394', '01baab1073b81025972e35024a9af20ac7efde61', 'a2e43270a9b1421e452c2975e5163e2a216abeac', 'acf8669643bec206fc818aee7a5959125c6ca297', '4c79edba0079d9b64359a085a96661c3d125b159', '5d8a1d422db0a678d29deb96ee98aff711a255df', '117a5760957e7aa9d479e01d747be27757f8e09f', 'd91e0d62626dd1e33521b110e9702710ed1e5088', '88c5a5acfead1d98f2c97db5db7d6fc4dd0a6864', '69d1a6be560e728282ae3d93707a7ebfc55fd100', 'f4cbec048d2261c528a3279e1eb29a1f5ecbde3e', '88ba3aafa45dd64e25dc55e84b4de72e89401d9a', '80bb30e65c36545f7dcaae8fa9f1e66e550d4731', 'd4327142f9a72a8cdaba12f37c0624376d9fd6b9', '4e12ba400f3786a829a531a4b7864f2315449356', '9519538f9d4e12acc5045434f2fde7ef3d32e2a4', '574a8f0e8b88cfad48a5e125522a60af32022ac0', '3c9ca90938ad7c0484325df6c73f234baa0e5653', 'e34816126aca5303f85deb68f648edc7659a2c2e', '72588bc52e14807f66071ae605e909786e81fa57', '364ee1f5878c18e30ca935fce51408aa939e20e3', 'ffd31213481d728718089757a4e4049a7e9dd1ee', '3402db72317bdae33b94b3bf84f80c6f86201fb5', '823080b3eab693dde2a3504b882a1985474fd1e9', 'e480bf2d472a18de33f7fc638365a2dcf2074419', '8a522d3d634351636d72779e85062a4d360cd8da']","['024006d4c2a89f7acacc6e4438d156525b60a98f', 'bcd857d75841aa3e92cd4284a8818aba9f6c0c3f', '9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d', '66cdc28dc084af6507e979767755e99fe0b46b39', '846aedd869a00c09b40f1f1f35673cb22bc87490', 'c221cc946425d85f93c86e3be2c31d4feb00faa1', '4c05d7caa357148f0bbd61720bdd35f0bc05eb81', 'b1362879e77efef96ab552f5cb1198c2a67204d6', '6e90fd78e8a3b98af3954aae5209703aa966603e', 'e0b65d3839e3bf703d156b524d7db7a5e10a2623', 'e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d', 'd5ed07113ddcd038062525a5a54550c012ac9a74', '3b9732bb07dc99bde5e1f9f75251c6ea5039373e', '69e76e16740ed69f4dc55361a3d319ac2f1293dd', 'c6170fa90d3b2efede5a2e1660cb23e1c824f2ca']",2017,https://semanticscholar.org/paper/48830e2e4272fa88dc256f1ac9cf81be14112bdb,['DBLP'],"['https://openreview.net/forum?id=r1VGvBcxl', 'https://arxiv.org/pdf/1611.06256v3.pdf', 'https://openreview.net/pdf?id=r1VGvBcxl']",ICLR,['Computer Science'],2601298905,51119135,https://api.github.com/repos/NVlabs/GA3C,1633928,GA3C,Hybrid CPU/GPU implementation of the A3C algorithm for deep reinforcement learning.,\N,2016-11-19 21:22:59,\N,0,2021-02-27 7:38:59,\N,https://github.com/NVlabs/GA3C,605,200,Python,2016-11-19 21:22:59,2020-02-25 0:15:09,22,TRUE,TRUE,no homepageUrl,71,[],"# GA3C: Reinforcement Learning through Asynchronous Advantage Actor-Critic on a GPU

A hybrid CPU/GPU version of the Asynchronous Advantage Actor-Critic (A3C) algorithm, currently the state-of-the-art method in reinforcement learning for various gaming tasks. This CPU/GPU implementation, based on TensorFlow, achieves a significant speed up compared to a similar CPU implementation.

## How do I get set up? ###

* Install [Python > 3.0](https://www.python.org/)
* Install [TensorFlow 1.0](https://www.tensorflow.org/install/install_linux) 
* Install [OpenAI Gym](https://github.com/openai/gym)
* Clone the repo.
* That's it folks!

## How to Train a model from scratch? ###

Run `sh _clean.sh` first, and then `sh _train.sh`.
The script `_clean.sh` cleans the checkpoints folder, which contains the network models saved during the training process, as well as removing `results.txt`, which is a log of the scores achieved during training.

> Remember to save your trained models and scores in a different folder if needed before cleaning.

`_train.sh` launches the training procedure, following the parameters in `Config.py`.
You can modify the training parameters directly in `Config.py`, or pass them as argument to `_train.sh`.
E.g., launching `sh _train.sh LEARNING_RATE_START=0.001` overwrites the starting value of the learning rate in `Config.py` with the one passed as argument (see below).
You may want to modify `_train.sh` for your particular needs. 

The output should look like below:

...  
[Time:       33] [Episode:       26 Score:   -19.0000] [RScore:   -20.5000 RPPS:   822] [PPS:   823 TPS:   183] [NT:  2 NP:  2 NA: 32]  
[Time:       33] [Episode:       27 Score:   -20.0000] [RScore:   -20.4815 RPPS:   855] [PPS:   856 TPS:   183] [NT:  2 NP:  2 NA: 32]  
[Time:       35] [Episode:       28 Score:   -20.0000] [RScore:   -20.4643 RPPS:   854] [PPS:   855 TPS:   185] [NT:  2 NP:  2 NA: 32]  
[Time:       35] [Episode:       29 Score:   -19.0000] [RScore:   -20.4138 RPPS:   877] [PPS:   878 TPS:   185] [NT:  2 NP:  2 NA: 32]  
[Time:       36] [Episode:       30 Score:   -20.0000] [RScore:   -20.4000 RPPS:   899] [PPS:   900 TPS:   186] [NT:  2 NP:  2 NA: 32]  
...  

**PPS** (predictions per second) demonstrates the speed of processing frames, while **Score** shows the achieved score.  
**RPPS** and **RScore** are the rolling average of the above values.

To stop the training procedure, adjuts `EPISODES` in `Config.py` propoerly, or simply use ctrl + c.

## How to continue training a model? ###

If you want to continue training a model, set `LOAD_CHECKPOINTS=True` in `Config.py`, and set `LOAD_EPISODE` to the episode number you want to load.
Be sure that the corresponding model has been saved in the checkpoints folder (the model name includes the number of the episode).

> Be sure not to use `_clean.sh` if you want to stop and then continue training! 

## How to play a game with a trained agent? ###

Run `_play.sh`
You may want to modify this script for your particular needs.

## How to change the game, configurations, etc.? ###
All the configurations are in `Config.py`  
As mentioned before, one useful way of modifying a config is to pass it as an argument to `_train.sh`. For example, to save the models while training, just run: `train.sh TRAINERS=4`.

## Sample learning curves
Typical learning curves for Pong and Boxing are shown here. These are easily obtained from the results.txt file.
![Convergence Curves](http://mb2.web.engr.illinois.edu/images/pong_boxing.png)

### References ###

If you use this code, please refer to our [ICLR 2017 paper](https://openreview.net/forum?id=r1VGvBcxl):

```
@conference{babaeizadeh2017ga3c,
  title={Reinforcement Learning thorugh Asynchronous Advantage Actor-Critic on a GPU},
  author={Babaeizadeh, Mohammad and Frosio, Iuri and Tyree, Stephen and Clemons, Jason and Kautz, Jan},
  booktitle={ICLR},
  biurl={https://openreview.net/forum?id=r1VGvBcxl},
  year={2017}
}
```
This work was first presented in an oral talk at the [The 1st International Workshop on Efficient Methods for Deep Neural Networks](http://allenai.org/plato/emdnn/papers.html), NIPS Workshop, Barcelona (Spain), Dec. 9, 2016:

```
@article{babaeizadeh2016ga3c,
  title={{GA3C:} {GPU}-based {A3C} for Deep Reinforcement Learning},
  author={Babaeizadeh, Mohammad and Frosio, Iuri and Tyree, Stephen and Clemons, Jason and Kautz, Jan},
  journal={NIPS Workshop},
  biurl={arXiv preprint arXiv:1611.06256},
  year={2016}
}
```
",20,289,118,0,500,2012-11-01 0:32:54,https://github.com/NVlabs/GA3C,200,27,FALSE,no mirror url,3,0,0,Organization,no company,no email or private email,no twitter_username or private twitter_username,180,2021-11-09 20:46:27,"# Copyright (c) 2016, NVIDIA CORPORATION. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#  * Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#  * Neither the name of NVIDIA CORPORATION nor the names of its
#    contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
",10,5.541822322,5.546999596,5,5,GA3C,tf,"<h1>ga3c: reinforcement learning through asynchronous advantage actor-critic on a gpu</h1>
<p>a hybrid cpu/gpu version of the asynchronous advantage actor-critic (a3c) algorithm, currently the state-of-the-art method in reinforcement learning for various gaming tasks. this cpu/gpu implementation, based on tensorflow, achieves a significant speed up compared to a similar cpu implementation.</p>
<h2>how do i get set up?</h2>
<ul>
<li>install <a href=""https://www.python.org/"">python > 3.0</a></li>
<li>install <a href=""https://www.tensorflow.org/install/install_linux"">tensorflow 1.0</a> </li>
<li>install <a href=""https://github.com/openai/gym"">openai gym</a></li>
<li>clone the repo.</li>
<li>that's it folks!</li>
</ul>
<h2>how to train a model from scratch?</h2>
<p>run <code>sh _clean.sh</code> first, and then <code>sh _train.sh</code>.
the script <code>_clean.sh</code> cleans the checkpoints folder, which contains the network models saved during the training process, as well as removing <code>results.txt</code>, which is a log of the scores achieved during training.</p>
<blockquote><p>remember to save your trained models and scores in a different folder if needed before cleaning.</p>
</blockquote>
<p><code>_train.sh</code> launches the training procedure, following the parameters in <code>config.py</code>.
you can modify the training parameters directly in <code>config.py</code>, or pass them as argument to <code>_train.sh</code>.
e.g., launching <code>sh _train.sh learning_rate_start=0.001</code> overwrites the starting value of the learning rate in <code>config.py</code> with the one passed as argument (see below).
you may want to modify <code>_train.sh</code> for your particular needs.</p>
<p>the output should look like below:</p>
<p>...<br>
[time:       33] [episode:       26 score:   -19.0000] [rscore:   -20.5000 rpps:   822] [pps:   823 tps:   183] [nt:  2 np:  2 na: 32]<br>
[time:       33] [episode:       27 score:   -20.0000] [rscore:   -20.4815 rpps:   855] [pps:   856 tps:   183] [nt:  2 np:  2 na: 32]<br>
[time:       35] [episode:       28 score:   -20.0000] [rscore:   -20.4643 rpps:   854] [pps:   855 tps:   185] [nt:  2 np:  2 na: 32]<br>
[time:       35] [episode:       29 score:   -19.0000] [rscore:   -20.4138 rpps:   877] [pps:   878 tps:   185] [nt:  2 np:  2 na: 32]<br>
[time:       36] [episode:       30 score:   -20.0000] [rscore:   -20.4000 rpps:   899] [pps:   900 tps:   186] [nt:  2 np:  2 na: 32]<br>
...</p>
<p><strong>pps</strong> (predictions per second) demonstrates the speed of processing frames, while <strong>score</strong> shows the achieved score.<br>
<strong>rpps</strong> and <strong>rscore</strong> are the rolling average of the above values.</p>
<p>to stop the training procedure, adjuts <code>episodes</code> in <code>config.py</code> propoerly, or simply use ctrl + c.</p>
<h2>how to continue training a model?</h2>
<p>if you want to continue training a model, set <code>load_checkpoints=true</code> in <code>config.py</code>, and set <code>load_episode</code> to the episode number you want to load.
be sure that the corresponding model has been saved in the checkpoints folder (the model name includes the number of the episode).</p>
<blockquote><p>be sure not to use <code>_clean.sh</code> if you want to stop and then continue training!</p>
</blockquote>
<h2>how to play a game with a trained agent?</h2>
<p>run <code>_play.sh</code>
you may want to modify this script for your particular needs.</p>
<h2>how to change the game, configurations, etc.?</h2>
<p>all the configurations are in <code>config.py</code><br>
as mentioned before, one useful way of modifying a config is to pass it as an argument to <code>_train.sh</code>. for example, to save the models while training, just run: <code>train.sh trainers=4</code>.</p>
<h2>sample learning curves</h2>
<p>typical learning curves for pong and boxing are shown here. these are easily obtained from the results.txt file.
<img src=""http://mb2.web.engr.illinois.edu/images/pong_boxing.png"" alt=""convergence curves""></p>
<h3>references</h3>
<p>if you use this code, please refer to our <a href=""https://openreview.net/forum?id=r1vgvbcxl"">iclr 2017 paper</a>:</p>
<pre><code>@conference{babaeizadeh2017ga3c,
  title={reinforcement learning thorugh asynchronous advantage actor-critic on a gpu},
  author={babaeizadeh, mohammad and frosio, iuri and tyree, stephen and clemons, jason and kautz, jan},
  booktitle={iclr},
  biurl={https://openreview.net/forum?id=r1vgvbcxl},
  year={2017}
}
</code></pre>
<p>this work was first presented in an oral talk at the <a href=""http://allenai.org/plato/emdnn/papers.html"">the 1st international workshop on efficient methods for deep neural networks</a>, nips workshop, barcelona (spain), dec. 9, 2016:</p>
<pre><code>@article{babaeizadeh2016ga3c,
  title={{ga3c:} {gpu}-based {a3c} for deep reinforcement learning},
  author={babaeizadeh, mohammad and frosio, iuri and tyree, stephen and clemons, jason and kautz, jan},
  journal={nips workshop},
  biurl={arxiv preprint arxiv:1611.06256},
  year={2016}
}
</code></pre>
",1,2,21,0,0,FALSE,0,1,FALSE,FALSE,TRUE,1,2,FALSE,FALSE,TRUE,15,1047,0,623,11,FALSE,FALSE,FALSE,reinforcement learning through asynchronous advantage actor-critic on a gpu,1,reinforcement learning through asynchronous advantage actor-critic on a gpu,5,23,23,23,23,23,0,1523,1523,1523,1523,1523,0,11,11,11,11,11,0,23,1523,11,['no affiliation'],FALSE,23,1523,11,['no affiliation'],FALSE,TRUE,FALSE,624,15,112,1,5,1,1,3,1,1,unknow,9,89
https://paperswithcode.com/paper/learning-and-evaluating-emotion-lexicons-for,2005.05672,https://arxiv.org/abs/2005.05672v1,https://arxiv.org/pdf/2005.05672v1.pdf,https://github.com/JULIELab/MEmoLon,TRUE,TRUE,"Emotion lexicons describe the affective meaning of words and thus constitute a centerpiece for advanced sentiment and emotion analysis. Yet, manually curated lexicons are only available for a handful of languages, leaving most languages of the world without such a precious resource for downstream applications. Even worse, their coverage is often limited both in terms of the lexical units they contain and the emotional variables they feature. In order to break this bottleneck, we here introduce a methodology for creating almost arbitrarily large emotion lexicons for any target language. Our approach requires nothing but a source language emotion lexicon, a bilingual word translation model, and a target language embedding model. Fulfilling these requirements for 91 languages, we are able to generate representationally rich high-coverage lexicons comprising eight emotional variables with more than 100k lexical entries each. We evaluated the automatically generated lexicons against human judgment from 26 datasets, spanning 12 typologically diverse languages, and found that our approach produces results in line with state-of-the-art monolingual approaches to lexicon creation and even surpasses human reliability for some languages and variables. Code and data are available at https://github.com/JULIELab/MEmoLon archived under DOI https://doi.org/10.5281/zenodo.3779901.",ACL 2020 6,"['Sven Buechel', 'Susanna Rücker', 'Udo Hahn']","['Emotion Recognition', 'Translation', 'Word Translation']",2020-05-12,[],bcd35ec9bda0cfa306cae6201fd3e50c543e232c,"[{'name': 'Sven  Buechel', 'ids': ['3449424']}, {'name': 'Susanna  Rücker', 'ids': ['1698101293']}, {'name': 'Udo  Hahn', 'ids': ['50007548']}]","['581286607c371fc0aca22369ae5438cc6afd4797', 'ea2c0caebb858e0971c9cb2921d52c7bd33bcc5f', '4006d500b17e9089378658900fd3fa587531b352', 'ce3b364b7e6358940ce97d8d5887a65e5024ca21', '9c9bc902a92b37d17a9482d74bc1b45ca281f12d']","['a5d814e37cbcac9e646517ac258b1b0347b2e256', '9cd9610e7a11ac1a8f7fccad34a87f1e8e1a93fe', '4ebe5a18f1bef09680da77086c12fa996b818c80', '0d98d48fbbe86d3d52898e3866d14de34cddb847', 'd42116156c5ed5a581a38c7b4142fea5b6323914', 'ddc1280a7b08009bb2c5be6462a5e06e9e0b60e1', '83d6c58bb598b5c0d2799a99c91849b3c69d795f', '8bb7899709a224ff84c02c72c19cf803ddc78035', '723f744455914bf1ead6ad267976a1500b7dee4a', '65b12e3900bc765553a670ae41ad0f9434aea3d9', 'ae47d53a9d712d655f88362bddfae226c651742a', '6a6f1b8ef3eefaf8027a76134859b44886a5f795', '541efb0b2d025911fcb0e4afd66115e50b7693e3', 'e30fdcb603afbdfb38d336b57df9a7e7c3b85a61', 'c5353f31b03f7c3c387bbdb5da1656148b51b766', 'ba049d218ed32c1553af8c4705ce517f3fa60aba', '235e255462446d7364d9a5df7dc6fe736a7249ad', '7f90ef42f22d4f9b86d33b0ad7f16261273c8612', 'c5dea5267e0519916951312a5c05fd1adac39723', '54227c063bb04489caffd65ff9fc6218788ddb25', '52df920cf33f61f08b332a51cb2686699945c599', 'bcdc102c04fb0e7d4652e8bcc7edd2983bb9576d', 'f2c41c6632d55c601f000e35cf0cc4db524ef20e', '20b0d2490f41f47cc1cfc2b2fd521132b65d64d7', 'bf9db8ca2dce7386cbed1ae0fd6465148cdb2b98', '2417633f64b6226083c0cd6e738a3c3e6da8e871', 'c765eb0a31849361d829b24e173a37bab0919892', 'eb8d32388012889ed438c3b0191f4ab614c5b80a', 'f551c0e6cf7c5e57b6782d48d4efbe638083615c', 'ae8164f43f1eeeb25c5b8127b0e4064cb8002ddf', 'b3b7b374eee3ea3c6fe35c1e7bdf9bf24c9456b0', '9576d1573688a05e45ad30e783e7d0c10a10d6d9', '400c28a134dec53af1de657894416a57b41215b7', 'a3dea344ae208dbdcbc96e47c7930e46c7a18c92', '147925d056f3218b0ce9a1c87bd3bed47444d9d7', '6f4b674758435c6dd5e72c986542a756f79147dd', 'b62b80384f402d38c3425db6a99899d1cb9c50c6', 'ecda65ac34c7394b309660d092b894d6278ed27e', '3a6e8f22a33fa9f090af1c0b8bbdfac7c134e0c8', '9ea1ddfd4cb62bc88a40f12bc488ca753fc27fd9', 'c6fe37beac50446012a87aac9fc2f8c4f34891e1', '033d76386add395d51a750de2e9c062668568638', '5607fcc4a83de6ef489d4b0e4c94689001f07d69', '7e6e7407d062fad9a0ca8b6654099bd395ce30ea', '0efb3bddce9053f71f802ed8631dfa0ea355af3a', '73fb260c8b8a050c77bebbd6a7645df692a6c3fe', 'dc914dcb8f04027613ec16e851703d7b5f7b60a5', '332b05ff46f779754a8fefbfe3695f98aed9ed81', 'd98110feaa663d163b97e3bcdcfd2a5fa543c0cf', 'a457755cf3b96503c58cb38bb6dcd64a7be038e3', '7bb7e7174f6b1ee36eefe9acdd1f74067f6e7750', '3d430904bb1d2eb5993e890697ac4554a3e23a8c', '732176cc598d80ecef68611145a04ddf7f882158', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', 'b834433e8cbfc6b1af69e1b10e145ef1c2732690', 'f3c793d8bff8863ee0353f1d9c7a84182217c773', '4a61f3bb90d2c921fe7c6aca6dfb589d0ad69ce7', '3d28cf1780ff21b87f2fa0b47ebaf0021b07bbf3', '5c8d05e27e36ebd64ee43fe1670262cdcc2123ba', '502c1e8bba1578241c354a54926ca913d5fa9c8a', '4751e41973198cc145804d8dee85cbc511b71980', 'cda4facddeadd5e35d21b00fe196a1ad1edead46', '097001c25f90552246b5adbf06444268d917dac5', '7eab920586a88564d26b7d70e270c61c3d06a4ce', '409168d24d1c33f02d9c45365bc20a962da18135', '31e46995440fe661ca53fcf751536742855a59a6', '58e00cde75cf11589ca53e7d2e62179a80cad19a', '47d04b0a9e63567e351098507f40107295fdbba7', '94a178bc81d045bbc7ff6bb83738c2491c3c9985', 'bfdbfe3bf703594b884ae69f505f94ce7e98141e', '88de375c54f6bcf1182bc56d5502679a4ea335b7', '168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74', '66c8ab0b3b7698cf5333c3f7350a6eaa5302294b', '21ea974bc645962f91d071984ea9c04ccf476c8e', '0d1444970774aa59c763cdf3a39ff547f1b7dd96', '516a27d5dd06622f872f5ef334313350745eadc3', '67a7dfdb903e534294e1ffacef7bebc981120f28', 'c5e3b065e352a93d8754b86baaf8ec20bf81a5c3', '04b334619290b8428a531f32003dd472d1644c86', '9acdb73cbfd48f80309f419eb3217e4f124a04a7', '52b3d9fcc8d0d3beb0357c598358f0435784d517', '25dfd1d6b8912c44134984ed4de93a32e7dcea17', '3411c8d58493f0ee094155ff3ee8b6c3d6cf5923', '37f25b301a7bc6c6b088093b026333c2e46eeb63']",2020,https://semanticscholar.org/paper/bcd35ec9bda0cfa306cae6201fd3e50c543e232c,['DBLP'],"['https://www.aclweb.org/anthology/2020.acl-main.112/', 'https://www.aclweb.org/anthology/2020.acl-main.112.pdf', 'https://arxiv.org/abs/2005.05672', 'https://arxiv.org/pdf/2005.05672v1.pdf']",ACL,['Computer Science'],3035323028,161285790,https://api.github.com/repos/JULIELab/MEmoLon,4221065,MEmoLon,"Repository for our ACL 2020 paper \Learning and Evaluating Emotion Lexicons for 91 Languages\""""",\N,2020-05-01 9:04:53,\N,0,2021-02-13 23:19:17,\N,https://github.com/JULIELab/MEmoLon,9,1,Jupyter Notebook,2020-05-01 9:04:53,2022-04-06 19:06:19,7,TRUE,TRUE,no homepageUrl,6,[],"# MEmoLon – The Multilingual Emotion Lexicon

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3779901.svg)](https://doi.org/10.5281/zenodo.3779901)


This is the main repository for our ACL 2020 paper [Learning and Evaluating Emotion Lexicons for 91 Languages](https://www.aclweb.org/anthology/2020.acl-main.112/).

## Overview
Data and code for this research project are distributed across different places. This github repository serves as landing page linking to other relevant sites. It also contains the code necessary to re-run our experiments and analyses.  Releases of this repository are archived as Zenodo records under [DOI 10.5281/zenodo.3779901](https://doi.org/10.5281/zenodo.3779901). While this repository contains our codebase and experimental results, the generated lexicon is archived in an second Zenodo record under [DOI 10.5281/zenodo.3756606](https://doi.org/10.5281/zenodo.3756606) due to its size.

### Links 
* [Zenodo record of this repository](https://doi.org/10.5281/zenodo.3779901)

* [Zenodo record of the lexicon](https://doi.org/10.5281/zenodo.3756606)

* [arXiv version of the paper](https://arxiv.org/abs/2005.05672) 

* [ACL Anthology version of the paper](https://www.aclweb.org/anthology/2020.acl-main.112/)

  


## The Lexicon
We created emotion lexicons for 91 languages, each one covers eight emotional variables and  comprises over 100k word entries. There are several versions of the lexicons, the difference being the choice of the expansion model: There is a linear regression baseline and three  versions of neural network models. The *main* version of our lexicons (the version we refer to in the main experiments of our paper and the one we would recommend to use) is referred to as as **MTL_grouped** (applying multi-task learning within two groups of our target variables).   **If you are mainly interested in our lexicons,  download [this](https://zenodo.org/record/3756607/files/MTL_grouped.zip?download=1) zip file (2.2GB).**  It contains 91 tsv files which are named `<iso language code>.tsv`. Please refer to the [description of the Zenodo record](https://doi.org/10.5281/zenodo.3756606) for more details.



## The Experimental Results

The analyses and results we present in the paper can be found in `/memolon/analyses` in form of jupyter notebooks and csv / json files. The names of the notebooks follow the section names in the paper. 



## The Codebase

If you are interested in the implementation of our methodology, replicating the lexicon creation or re-running our analyses, this section describes how to work with our code.



### Set-Up 

Make sure you have `conda` installed on your machine.  We ran the code on Debian 9. Necessary steps may differ across operating systems. 

Clone this repository, `cd` into the project's root directory, and run the following commands.

```
conda create --name ""memolon2020"" python=3.8 pip
conda activate memolon2020
pip install -r requirements.txt
source activate.src
```

The last line configures your `PYTHONTPATH`.



### Re-Running the Lexicon Generation

Recreating the lexicons from scratch requires the Source lexicon, data splits, and the translation tables for all 91 languages. The data split (word lists in `train.txt`, `dev.txt`, and `test.txt`  in `/memolon/data/Source`)   as well as the translation tables (see content of `/memolon/data/TranslationTables`)  are already included in this repository. So, you only have to download the source lexicon. There are two files:

 * Get the file [Ratings_Warriner_et_al.csv](https://github.com/JULIELab/XANEW/blob/master/Ratings_Warriner_et_al.csv)  (commit b1ed97e from 11 Nov 2019) and place it in `/memolon/data/Source`.
 * Get the file [Warriner_BE.tsv](https://github.com/JULIELab/EmoMap/blob/master/coling18/main/lexicon_creation/lexicons/Warriner_BE.tsv) (commit dbfa3b9 from 15 Jun 2018) and place it in ``/memolon/data/Source``.

 

The python scripts for creating the lexicons can be found in  `/memolon/src`.  You can either `cd` there and simply run `run_all.sh` or follow the more detailed instructions below. Please take note that the whole process may take several hours. **You do not have to have a GPU to run our code in a reasonable amount of time.**

 * To download the fastText embedding models run `download_embeddings.py` which will download the vec.gz files and place them into `/memolon/data/Embeddings`.
 * To train and use our models to create all four different versions of the target lexicons (`TargetPred`) run the following scripts (or just the one you want to use). They will create the lexicons and place them into the respective subfolder of  `/memolon/data/TargetPred`:
     * `TargetPred_MTLgrouped.py`: Multi-task learning within the two groups (VAD and BE5) but not across both. This is the version we mainly refer to in the paper and **recommend to use**.
     * `TargetPred_MTLall.py`: Multi-task learning among all 8 target variables.
     * `TargetPred_STL.py`: Single-task learning all 8 variables separately.
     * `TargetPred_ridge.py`: The ridge regression baseline.



### Re-Running the Analyses

As stated above, analyses are organized as jupyter notebooks in the folder `/memolon/analyses`. Please note that running some of the notebooks requires data files from other notebooks.  The recommended order of running the notebooks is the following, although other orders are possible as well. 

1. `overview-gold-lexica.ipynb`
2. `silver-evaluation.ipynb`
3. `gold-evaluation.ipynb`
4. `comparison_against_human_reliability.ipynb`
5. `translation_vs_prediction.ipynb`
6. `gold_vs_silver_evaluation.ipynb`
7. `overview-generated-lexicons.ipynb`

Running the silver evaluation is quite simple. You can either generate our lexicons from scratch (see above), or, much easier, download our lexicons from the  [Zenodo record](https://doi.org/10.5281/zenodo.3756606) (see above). Unzip all four versions of the lexicons and place the tsv files in the respective subfolders of `/memolon/data/TargetPred`.

Running the gold evaluation and related analyses requires you to manually collect all the gold datasets listed in the paper. This is a tedious process because they all have different copyright and access restrictions. Please find more detailed instructions below.

 * en1. This is our Source lexicon, see the above section on lexicon generation.
 * en2. Either request the **1999**-version of the Affective Norms for English Words (ANEW) from the [Center for the Study of Emotion and Attention](https://csea.phhp.ufl.edu/Media.html#bottommedia) at the University of Florida, or copy-paste/parse the data from the Techreport  *Bradley, M. M., & Lang, P. J. (1999). Affective Norms for English Words (Anew): Stimuli, Instruction Manual and Affective Ratings (C–1). The Center for Research in Psychophysiology, University of Florida.* Format the data as an tsv file with column headers `word`, `valence`, `arousal`, `dominance` and save it under `/memolon/data/TargetGold/ANEW1999.tsv`.
 * en3. Get the file `Stevenson(2007)-ANEW_emotional_categories.xls` from [Stevenson et al. (2007)](https://doi.org/10.3758/BF03192999) 
 and place it in `/memolon/data/TargetGold`.
 * es1. Get the file `Redondo(2007).xls`  from [Redondo et al. (2007)](https://doi.org/10.3758/BF03193031) and place it `/memolon/data/TargetGold`.
 * es2. Get the file `13428_2015_700_MOESM1_ESM.csv` from [Stadthagen-Gonzalez et al. (2017)](https://doi.org/10.3758/BF03192999) and save it as `/memolon/data/TargetGold/Stadthagen_VA.csv`
 * es3. Get the file `Hinojosa et al_Supplementary materials.xlsx` from [Hinojosa et al. (2015)](https://link.springer.com/article/10.3758%2Fs13428-015-0572-5) and place it in `/memolon/data/TargetGold`.
 * es4. Included in the download for es3.
 * es5. Get the file `13428_2017_962_MOESM1_ESM.csv` from [Stadthagen-Gonzalez et al. (2018)](https://doi.org/10.3758/s13428-017-0962-y) and save it as `/memolon/data/TargetGold/Stadthagen_BE.csv`.
 * es6. Get the file `13428_2016_768_MOESM1_ESM.xls` from [Ferré et al. (2017)](https://doi.org/10.3758/s13428-016-0768-3) ad save it as `/memolon/data/TargetGold/Ferre.xlsx`.
 * de1. Get the file `13428_2013_426_MOESM1_ESM.xlsx` from [Schmidtke et al. (2014)](https://doi.org/10.3758/s13428-013-0426-y) and save it as `/memolon/data/TargetGold/Schmidtke.xlsx`
 * de2. Get the file `BAWL-R.xls` from [Vo et al. (2009)](https://doi.org/10.3758/BRM.41.2.534) which is currently available 
 [here](https://www.ewi-psy.fu-berlin.de/einrichtungen/arbeitsbereiche/allgpsy/Download/BAWL/index.html). You will need to request a password from the authors. Save the file **without password** as `/memolon/data/TargetGold/BAWL-R.xls`. We had to run an automatic file repair when oping it with Excel for the first time.
 * de3. Get the file `LANG_database.txt` from [Kaske and Kotz (2010)](https://doi.org/10.3758/BRM.42.4.987) and place it `/memolon/data/TargetGold`.
 * de4. Get de2 (see above). Then, get the file  `13428_2011_59_MOESM1_ESM.xls` from [Briesemeister et al. (2011)](https://doi.org/10.3758/s13428-011-0059-y) and save it as `/memolon/data/TargetGold/Briesemeister.xls`.
 * pl1. Get the file `data sheet 1.xlsx` from [Imbir (2016)](https://doi.org/10.3389/fpsyg.2016.01081) and save it as  `/memolon/data/TargetGold/Imbir.xlsx`.
 * pl2. Get the file `13428_2014_552_MOESM1_ESM.xlsx` from [Riegel et al. (2015)](https://doi.org/10.3758/s13428-014-0552-1) and save it as `/memolon/data/TargetGold/Riegel.xlsx`
 * pl3. Get pl2 (see above). Then, get the file `S1 Dataset` from [Wierzba et al. (2015)](https://doi.org/10.1371/journal.pone.0132305) 
 and save it as `/memolon/data/TargetGold/Wierzba.xlsx`.
 * zh1. Get CVAW 2.0 from [Yu et al. (2016)](https://doi.org/10.18653/v1/N16-1066) which is distributed via 
 [this website](http://nlp.innobic.yzu.edu.tw/resources/cvaw.html). Use Google Translate to  'translate' the words in `cvaw2.csv` 
 from traditional to simplified Chinese characters (you can batch-translate by copy-pasting multiple words separated by newline directly from the file). Save the modified file as `/memolon/data/TargetGold/cvaw2_simplied.csv`.
 * zh2. Get the file `13428_2016_793_MOESM2_ESM.pdf` from [Yao et al. (2017)](https://doi.org/10.3758/s13428-016-0793-2).  Convert PDF to Excel (there are online tools for that but check the results for correctness) and save as `/memolon/data/TargetGold/Yao.xlsx`.
 * it. Get the data from [Montefinese et al. (2014)](https://doi.org/10.3758/s13428-013-0405-3). The website offers a PDF version
 of the ratings. However, the formatting makes it very difficult to process automatically. Instead, the first author Maria Montefinese provided us with an Excel version.  Save the ratings as `/memolon/data/TargetGold/Montefinese.xls`.
 * pt. Get the file  `13428_2011_131_MOESM1_ESM.xls`  from [Soares et al. (2012)](https://doi.org/10.3758/s13428-011-0131-7). 
 Save it as  `/memolon/data/TargetGold/Soares.xls`.
 * nl. Get the file `13428_2012_243_MOESM1_ESM.xlsx`  from [Moors et al. (2013)](https://doi.org/10.3758/s13428-012-0243-8).
 Save it as  `/memolon/data/TargetGold/Moors.xlsx`.
 * id. Get the file `Data Sheet 1.XLSX` from [Sianipar et al. (2016)](https://doi.org/10.3389/fpsyg.2016.01907). Save it as `/memolon/data/TargetGold/Sianipar.xlsx`
 * el. Get the data from [Palogiannidi et al. (2016)](https://www.aclweb.org/anthology/L16-1458): We downloaded the ratings via the [link](www.telecom.tuc.gr/~epalogiannidi/docs/resources/greek_affective_lexicon.zip)
 provided in the paper on March 13, 2018. The link pointed to zip containing a single file `greek_affective_lexicon.csv`  which we saved under  `/memolon/data/TargetGold`. However, the original link does not work anymore (as of April 22, 2020). We recommend contacting the authors for a replacement. 
 * tr1. Get the file `TurkishEmotionalWordNorms.csv` from [Kapucu et al. (2018)](https://doi.org/10.1177/0033294118814722) 
 which is available [here](https://osf.io/rxtdm/). Place it under `/memolon/data/TargetGold`.
 * tr2. Included in the download for tr1. 
 * hr. Get the file `Supplementary material_Ćoso et al.xlsx` from [Coso et al. (2019)](https://doi.org/10.1177/1747021819834226)
 which is available [here](https://www.ucace.com/links/). Save it as `/memolon/data/TargetGold/Coso.xlsx`.



 ## Citation

If you find this work useful, please cite our paper:

```bib
@inproceedings{buechel-etal-2020-learning-evaluating,
    title = ""Learning and Evaluating Emotion Lexicons for 91 Languages"",
    author = {Buechel, Sven  and
      R{\""u}cker, Susanna  and
      Hahn, Udo},
    booktitle = ""Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"",
    month = jul,
    year = ""2020"",
    address = ""Online"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://www.aclweb.org/anthology/2020.acl-main.112"",
    doi = ""10.18653/v1/2020.acl-main.112"",
    pages = ""1202--1217"",
    abstract = ""Emotion lexicons describe the affective meaning of words and thus constitute a centerpiece for advanced sentiment and emotion analysis. Yet, manually curated lexicons are only available for a handful of languages, leaving most languages of the world without such a precious resource for downstream applications. Even worse, their coverage is often limited both in terms of the lexical units they contain and the emotional variables they feature. In order to break this bottleneck, we here introduce a methodology for creating almost arbitrarily large emotion lexicons for any target language. Our approach requires nothing but a source language emotion lexicon, a bilingual word translation model, and a target language embedding model. Fulfilling these requirements for 91 languages, we are able to generate representationally rich high-coverage lexicons comprising eight emotional variables with more than 100k lexical entries each. We evaluated the automatically generated lexicons against human judgment from 26 datasets, spanning 12 typologically diverse languages, and found that our approach produces results in line with state-of-the-art monolingual approaches to lexicon creation and even surpasses human reliability for some languages and variables. Code and data are available at https://github.com/JULIELab/MEmoLon archived under DOI 10.5281/zenodo.3779901."",
}
```





## Contact

Please get in touch via sven dot buechel at uni-jena dot de. 

",10,109,8,0,425,2014-04-12 12:17:27,https://github.com/JULIELab/MEmoLon,1,10831,FALSE,no mirror url,2,2,2,Organization,no company,julielab@listserv.uni-jena.de,no twitter_username or private twitter_username,88,2022-04-06 12:29:49,"MIT License

Copyright (c) 2020 JULIE Lab

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""Software""), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
",8,2.096200767,2.06711979,2,2,MEmoLon,pytorch,"<h1>memolon – the multilingual emotion lexicon</h1>
<p><a href=""https://doi.org/10.5281/zenodo.3779901""><img src=""https://zenodo.org/badge/doi/10.5281/zenodo.3779901.svg"" alt=""doi""></a></p>
<p>this is the main repository for our acl 2020 paper <a href=""https://www.aclweb.org/anthology/2020.acl-main.112/"">learning and evaluating emotion lexicons for 91 languages</a>.</p>
<h2>overview</h2>
<p>data and code for this research project are distributed across different places. this github repository serves as landing page linking to other relevant sites. it also contains the code necessary to re-run our experiments and analyses.  releases of this repository are archived as zenodo records under <a href=""https://doi.org/10.5281/zenodo.3779901"">doi 10.5281/zenodo.3779901</a>. while this repository contains our codebase and experimental results, the generated lexicon is archived in an second zenodo record under <a href=""https://doi.org/10.5281/zenodo.3756606"">doi 10.5281/zenodo.3756606</a> due to its size.</p>
<h3>links</h3>
<ul>
<li><p><a href=""https://doi.org/10.5281/zenodo.3779901"">zenodo record of this repository</a></p>
</li>
<li><p><a href=""https://doi.org/10.5281/zenodo.3756606"">zenodo record of the lexicon</a></p>
</li>
<li><p><a href=""https://arxiv.org/abs/2005.05672"">arxiv version of the paper</a></p>
</li>
<li><p><a href=""https://www.aclweb.org/anthology/2020.acl-main.112/"">acl anthology version of the paper</a></p>
</li>
</ul>
<h2>the lexicon</h2>
<p>we created emotion lexicons for 91 languages, each one covers eight emotional variables and  comprises over 100k word entries. there are several versions of the lexicons, the difference being the choice of the expansion model: there is a linear regression baseline and three  versions of neural network models. the <em>main</em> version of our lexicons (the version we refer to in the main experiments of our paper and the one we would recommend to use) is referred to as as <strong>mtl_grouped</strong> (applying multi-task learning within two groups of our target variables).   <strong>if you are mainly interested in our lexicons,  download <a href=""https://zenodo.org/record/3756607/files/mtl_grouped.zip?download=1"">this</a> zip file (2.2gb).</strong>  it contains 91 tsv files which are named <code><iso language code>.tsv</code>. please refer to the <a href=""https://doi.org/10.5281/zenodo.3756606"">description of the zenodo record</a> for more details.</p>
<h2>the experimental results</h2>
<p>the analyses and results we present in the paper can be found in <code>/memolon/analyses</code> in form of jupyter notebooks and csv / json files. the names of the notebooks follow the section names in the paper.</p>
<h2>the codebase</h2>
<p>if you are interested in the implementation of our methodology, replicating the lexicon creation or re-running our analyses, this section describes how to work with our code.</p>
<h3>set-up</h3>
<p>make sure you have <code>conda</code> installed on your machine.  we ran the code on debian 9. necessary steps may differ across operating systems.</p>
<p>clone this repository, <code>cd</code> into the project's root directory, and run the following commands.</p>
<pre><code>conda create --name ""memolon2020"" python=3.8 pip
conda activate memolon2020
pip install -r requirements.txt
source activate.src
</code></pre>
<p>the last line configures your <code>pythontpath</code>.</p>
<h3>re-running the lexicon generation</h3>
<p>recreating the lexicons from scratch requires the source lexicon, data splits, and the translation tables for all 91 languages. the data split (word lists in <code>train.txt</code>, <code>dev.txt</code>, and <code>test.txt</code>  in <code>/memolon/data/source</code>)   as well as the translation tables (see content of <code>/memolon/data/translationtables</code>)  are already included in this repository. so, you only have to download the source lexicon. there are two files:</p>
<ul>
<li>get the file <a href=""https://github.com/julielab/xanew/blob/master/ratings_warriner_et_al.csv"">ratings_warriner_et_al.csv</a>  (commit b1ed97e from 11 nov 2019) and place it in <code>/memolon/data/source</code>.</li>
<li>get the file <a href=""https://github.com/julielab/emomap/blob/master/coling18/main/lexicon_creation/lexicons/warriner_be.tsv"">warriner_be.tsv</a> (commit dbfa3b9 from 15 jun 2018) and place it in <code>/memolon/data/source</code>.</li>
</ul>
<p>the python scripts for creating the lexicons can be found in  <code>/memolon/src</code>.  you can either <code>cd</code> there and simply run <code>run_all.sh</code> or follow the more detailed instructions below. please take note that the whole process may take several hours. <strong>you do not have to have a gpu to run our code in a reasonable amount of time.</strong></p>
<ul>
<li>to download the fasttext embedding models run <code>download_embeddings.py</code> which will download the vec.gz files and place them into <code>/memolon/data/embeddings</code>.</li>
<li>to train and use our models to create all four different versions of the target lexicons (<code>targetpred</code>) run the following scripts (or just the one you want to use). they will create the lexicons and place them into the respective subfolder of  <code>/memolon/data/targetpred</code>:<ul>
<li><code>targetpred_mtlgrouped.py</code>: multi-task learning within the two groups (vad and be5) but not across both. this is the version we mainly refer to in the paper and <strong>recommend to use</strong>.</li>
<li><code>targetpred_mtlall.py</code>: multi-task learning among all 8 target variables.</li>
<li><code>targetpred_stl.py</code>: single-task learning all 8 variables separately.</li>
<li><code>targetpred_ridge.py</code>: the ridge regression baseline.</li>
</ul>
</li>
</ul>
<h3>re-running the analyses</h3>
<p>as stated above, analyses are organized as jupyter notebooks in the folder <code>/memolon/analyses</code>. please note that running some of the notebooks requires data files from other notebooks.  the recommended order of running the notebooks is the following, although other orders are possible as well.</p>
<ol>
<li><code>overview-gold-lexica.ipynb</code></li>
<li><code>silver-evaluation.ipynb</code></li>
<li><code>gold-evaluation.ipynb</code></li>
<li><code>comparison_against_human_reliability.ipynb</code></li>
<li><code>translation_vs_prediction.ipynb</code></li>
<li><code>gold_vs_silver_evaluation.ipynb</code></li>
<li><code>overview-generated-lexicons.ipynb</code></li>
</ol>
<p>running the silver evaluation is quite simple. you can either generate our lexicons from scratch (see above), or, much easier, download our lexicons from the  <a href=""https://doi.org/10.5281/zenodo.3756606"">zenodo record</a> (see above). unzip all four versions of the lexicons and place the tsv files in the respective subfolders of <code>/memolon/data/targetpred</code>.</p>
<p>running the gold evaluation and related analyses requires you to manually collect all the gold datasets listed in the paper. this is a tedious process because they all have different copyright and access restrictions. please find more detailed instructions below.</p>
<ul>
<li>en1. this is our source lexicon, see the above section on lexicon generation.</li>
<li>en2. either request the <strong>1999</strong>-version of the affective norms for english words (anew) from the <a href=""https://csea.phhp.ufl.edu/media.html#bottommedia"">center for the study of emotion and attention</a> at the university of florida, or copy-paste/parse the data from the techreport  <em>bradley, m. m., &amp; lang, p. j. (1999). affective norms for english words (anew): stimuli, instruction manual and affective ratings (c–1). the center for research in psychophysiology, university of florida.</em> format the data as an tsv file with column headers <code>word</code>, <code>valence</code>, <code>arousal</code>, <code>dominance</code> and save it under <code>/memolon/data/targetgold/anew1999.tsv</code>.</li>
<li>en3. get the file <code>stevenson(2007)-anew_emotional_categories.xls</code> from <a href=""https://doi.org/10.3758/bf03192999"">stevenson et al. (2007)</a> 
and place it in <code>/memolon/data/targetgold</code>.</li>
<li>es1. get the file <code>redondo(2007).xls</code>  from <a href=""https://doi.org/10.3758/bf03193031"">redondo et al. (2007)</a> and place it <code>/memolon/data/targetgold</code>.</li>
<li>es2. get the file <code>13428_2015_700_moesm1_esm.csv</code> from <a href=""https://doi.org/10.3758/bf03192999"">stadthagen-gonzalez et al. (2017)</a> and save it as <code>/memolon/data/targetgold/stadthagen_va.csv</code></li>
<li>es3. get the file <code>hinojosa et al_supplementary materials.xlsx</code> from <a href=""https://link.springer.com/article/10.3758%2fs13428-015-0572-5"">hinojosa et al. (2015)</a> and place it in <code>/memolon/data/targetgold</code>.</li>
<li>es4. included in the download for es3.</li>
<li>es5. get the file <code>13428_2017_962_moesm1_esm.csv</code> from <a href=""https://doi.org/10.3758/s13428-017-0962-y"">stadthagen-gonzalez et al. (2018)</a> and save it as <code>/memolon/data/targetgold/stadthagen_be.csv</code>.</li>
<li>es6. get the file <code>13428_2016_768_moesm1_esm.xls</code> from <a href=""https://doi.org/10.3758/s13428-016-0768-3"">ferré et al. (2017)</a> ad save it as <code>/memolon/data/targetgold/ferre.xlsx</code>.</li>
<li>de1. get the file <code>13428_2013_426_moesm1_esm.xlsx</code> from <a href=""https://doi.org/10.3758/s13428-013-0426-y"">schmidtke et al. (2014)</a> and save it as <code>/memolon/data/targetgold/schmidtke.xlsx</code></li>
<li>de2. get the file <code>bawl-r.xls</code> from <a href=""https://doi.org/10.3758/brm.41.2.534"">vo et al. (2009)</a> which is currently available 
<a href=""https://www.ewi-psy.fu-berlin.de/einrichtungen/arbeitsbereiche/allgpsy/download/bawl/index.html"">here</a>. you will need to request a password from the authors. save the file <strong>without password</strong> as <code>/memolon/data/targetgold/bawl-r.xls</code>. we had to run an automatic file repair when oping it with excel for the first time.</li>
<li>de3. get the file <code>lang_database.txt</code> from <a href=""https://doi.org/10.3758/brm.42.4.987"">kaske and kotz (2010)</a> and place it <code>/memolon/data/targetgold</code>.</li>
<li>de4. get de2 (see above). then, get the file  <code>13428_2011_59_moesm1_esm.xls</code> from <a href=""https://doi.org/10.3758/s13428-011-0059-y"">briesemeister et al. (2011)</a> and save it as <code>/memolon/data/targetgold/briesemeister.xls</code>.</li>
<li>pl1. get the file <code>data sheet 1.xlsx</code> from <a href=""https://doi.org/10.3389/fpsyg.2016.01081"">imbir (2016)</a> and save it as  <code>/memolon/data/targetgold/imbir.xlsx</code>.</li>
<li>pl2. get the file <code>13428_2014_552_moesm1_esm.xlsx</code> from <a href=""https://doi.org/10.3758/s13428-014-0552-1"">riegel et al. (2015)</a> and save it as <code>/memolon/data/targetgold/riegel.xlsx</code></li>
<li>pl3. get pl2 (see above). then, get the file <code>s1 dataset</code> from <a href=""https://doi.org/10.1371/journal.pone.0132305"">wierzba et al. (2015)</a> 
and save it as <code>/memolon/data/targetgold/wierzba.xlsx</code>.</li>
<li>zh1. get cvaw 2.0 from <a href=""https://doi.org/10.18653/v1/n16-1066"">yu et al. (2016)</a> which is distributed via 
<a href=""http://nlp.innobic.yzu.edu.tw/resources/cvaw.html"">this website</a>. use google translate to  'translate' the words in <code>cvaw2.csv</code> 
from traditional to simplified chinese characters (you can batch-translate by copy-pasting multiple words separated by newline directly from the file). save the modified file as <code>/memolon/data/targetgold/cvaw2_simplied.csv</code>.</li>
<li>zh2. get the file <code>13428_2016_793_moesm2_esm.pdf</code> from <a href=""https://doi.org/10.3758/s13428-016-0793-2"">yao et al. (2017)</a>.  convert pdf to excel (there are online tools for that but check the results for correctness) and save as <code>/memolon/data/targetgold/yao.xlsx</code>.</li>
<li>it. get the data from <a href=""https://doi.org/10.3758/s13428-013-0405-3"">montefinese et al. (2014)</a>. the website offers a pdf version
of the ratings. however, the formatting makes it very difficult to process automatically. instead, the first author maria montefinese provided us with an excel version.  save the ratings as <code>/memolon/data/targetgold/montefinese.xls</code>.</li>
<li>pt. get the file  <code>13428_2011_131_moesm1_esm.xls</code>  from <a href=""https://doi.org/10.3758/s13428-011-0131-7"">soares et al. (2012)</a>. 
save it as  <code>/memolon/data/targetgold/soares.xls</code>.</li>
<li>nl. get the file <code>13428_2012_243_moesm1_esm.xlsx</code>  from <a href=""https://doi.org/10.3758/s13428-012-0243-8"">moors et al. (2013)</a>.
save it as  <code>/memolon/data/targetgold/moors.xlsx</code>.</li>
<li>id. get the file <code>data sheet 1.xlsx</code> from <a href=""https://doi.org/10.3389/fpsyg.2016.01907"">sianipar et al. (2016)</a>. save it as <code>/memolon/data/targetgold/sianipar.xlsx</code></li>
<li>el. get the data from <a href=""https://www.aclweb.org/anthology/l16-1458"">palogiannidi et al. (2016)</a>: we downloaded the ratings via the <a href=""www.telecom.tuc.gr/~epalogiannidi/docs/resources/greek_affective_lexicon.zip"">link</a>
provided in the paper on march 13, 2018. the link pointed to zip containing a single file <code>greek_affective_lexicon.csv</code>  which we saved under  <code>/memolon/data/targetgold</code>. however, the original link does not work anymore (as of april 22, 2020). we recommend contacting the authors for a replacement. </li>
<li>tr1. get the file <code>turkishemotionalwordnorms.csv</code> from <a href=""https://doi.org/10.1177/0033294118814722"">kapucu et al. (2018)</a> 
which is available <a href=""https://osf.io/rxtdm/"">here</a>. place it under <code>/memolon/data/targetgold</code>.</li>
<li>tr2. included in the download for tr1. </li>
<li>hr. get the file <code>supplementary material_ćoso et al.xlsx</code> from <a href=""https://doi.org/10.1177/1747021819834226"">coso et al. (2019)</a>
which is available <a href=""https://www.ucace.com/links/"">here</a>. save it as <code>/memolon/data/targetgold/coso.xlsx</code>.</li>
</ul>
<h2>citation</h2>
<p>if you find this work useful, please cite our paper:</p>
<pre><code class=""lang-bib"">@inproceedings{buechel-etal-2020-learning-evaluating,
    title = &quot;learning and evaluating emotion lexicons for 91 languages&quot;,
    author = {buechel, sven  and
      r{\&quot;u}cker, susanna  and
      hahn, udo},
    booktitle = &quot;proceedings of the 58th annual meeting of the association for computational linguistics&quot;,
    month = jul,
    year = &quot;2020&quot;,
    address = &quot;online&quot;,
    publisher = &quot;association for computational linguistics&quot;,
    url = &quot;https://www.aclweb.org/anthology/2020.acl-main.112&quot;,
    doi = &quot;10.18653/v1/2020.acl-main.112&quot;,
    pages = &quot;1202--1217&quot;,
    abstract = &quot;emotion lexicons describe the affective meaning of words and thus constitute a centerpiece for advanced sentiment and emotion analysis. yet, manually curated lexicons are only available for a handful of languages, leaving most languages of the world without such a precious resource for downstream applications. even worse, their coverage is often limited both in terms of the lexical units they contain and the emotional variables they feature. in order to break this bottleneck, we here introduce a methodology for creating almost arbitrarily large emotion lexicons for any target language. our approach requires nothing but a source language emotion lexicon, a bilingual word translation model, and a target language embedding model. fulfilling these requirements for 91 languages, we are able to generate representationally rich high-coverage lexicons comprising eight emotional variables with more than 100k lexical entries each. we evaluated the automatically generated lexicons against human judgment from 26 datasets, spanning 12 typologically diverse languages, and found that our approach produces results in line with state-of-the-art monolingual approaches to lexicon creation and even surpasses human reliability for some languages and variables. code and data are available at https://github.com/julielab/memolon archived under doi 10.5281/zenodo.3779901.&quot;,
}
</code></pre>
<h2>contact</h2>
<p>please get in touch via sven dot buechel at uni-jena dot de.</p>
",6,1,80,0,0,FALSE,0,0,FALSE,FALSE,FALSE,1,4,FALSE,FALSE,FALSE,18,1545,3,1736,14,FALSE,TRUE,FALSE,learning and evaluating emotion lexicons for 91 languages,0,learning and evaluating emotion lexicons for 91 languages,3,30,30,30,30,30,0,439,439,439,439,439,0,11,11,11,11,11,0,30,439,11,['no affiliation'],FALSE,30,439,11,['no affiliation'],FALSE,FALSE,FALSE,20,84,5,3,0,1,1,4,1,1,A*,8,190
https://paperswithcode.com/paper/drill-down-interactive-retrieval-of-complex,1911.03826,https://arxiv.org/abs/1911.03826v1,https://arxiv.org/pdf/1911.03826v1.pdf,https://github.com/uvavision/DrillDown,TRUE,TRUE,"This paper explores the task of interactive image retrieval using natural language queries, where a user progressively provides input queries to refine a set of retrieval results. Moreover, our work explores this problem in the context of complex image scenes containing multiple objects. We propose Drill-down, an effective framework for encoding multiple queries with an efficient compact state representation that significantly extends current methods for single-round image retrieval. We show that using multiple rounds of natural language queries as input can be surprisingly effective to find arbitrarily specific images of complex scenes. Furthermore, we find that existing image datasets with textual captions can provide a surprisingly effective form of weak supervision for this task. We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.",NeurIPS 2019 12,"['Fuwen Tan', 'Paola Cascante-Bonilla', 'Xiaoxiao Guo', 'Hui Wu', 'Song Feng', 'Vicente Ordonez']",['Image Retrieval'],2019-11-10,[],5ec1906fbd2f34af88a987d381294e22a10c9165,"[{'name': 'Fuwen  Tan', 'ids': ['2055882010']}, {'name': 'Paola  Cascante-Bonilla', 'ids': ['1399431057']}, {'name': 'Xiaoxiao  Guo', 'ids': ['121433787']}, {'name': 'Hui  Wu', 'ids': ['47987329']}, {'name': 'Song  Feng', 'ids': ['145480862']}, {'name': 'Vicente  Ordonez', 'ids': ['2004053']}]","['f7e3443f9a7e15c0d0871903df9a6b5cec104a8d', '424656e7192c656766d213ced2eaf1f7e7fdf254', 'f8b37494ca210740942a27802bf3c68a2911cfb4']","['afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d', '92c0de4636985a141f12f0555202edbca43e5b4d', '6fa15f525a9814247ecd7cd93636f278d6d9ab3b', '2231f44be9a8472a46d8e8a628b4e52b9a8f44e0', 'cc00536941717ddd69b855afb31eb10a7c0043b5', '93499a7c7f699b6630a86fad964536f9423bb6d0', '098a1ccc13b8d6409aa333c8a1079b2c9824705b', '4c4bafe76cb69987d436a407c1e14fb94a7cea71', '71b7178df5d2b112d07e45038cb5637208659ff7', 'c18df1edc0a45891806d44896a8f666944e93d01', '3a0a3fbae91d98597d3d7bf5c33ff3eb818dc0a9', '424561d8585ff8ebce7d5d07de8dbf7aae5e7270', 'a6cb366736791bcccc5c8639de5a8f9636bf87e8', '45dd2a3cd7c27f2e9509b023d702408f5ac11c9d', '1b47265245e8db53a553049dcb27ed3e495fd625', 'ac81e14ab88a47d39e80b8927b3f37a1a0e588ed', '71ae756c75ac89e2d731c9c79649562b5768ff39', 'e95fe0ef21ed52007f93077d9e023f871056df53', 'b27e791e843c924ef052981b79490ab59fc0433d', '4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e', '4d8f2d14af5991d4f0d050d22216825cac3157bd', 'f7ab6c52be9351ac3f6cf8fe6ad5efba1c1595e8', 'a452560d99af7b9dc19fa2db8373237f0f6e9f99', '2c03df8b48bf3fa39054345bafabfeff15bfd11d', 'bb0d19804fd8d5be6deaf0059e0b7888e64205c3', 'f7158bd1635bf7bb87c557c429774d5236703e64', 'd1a73b5516b59fa28f7630da78ed18fb547d777c', '2e36ea91a3c8fbff92be2989325531b4002e2afc', 'ad5dc94b28bee087a34f52114c52bd09d2acd8cb', 'bace1415203a957eebb39b2ba9f44d48e1c77100', 'a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8', '7f1b111f0bb703b0bd97aba505728a9b0d9b2a54', '1686a73010fadf9c78b2d617a74d8e981fd7a290', '08a302f0bb8dc360ae3a0a20fa7b7555920380d4', '0612745dbd292fc0a548a16d39cd73e127faedde', '8384387a3739280b15d38f39429aadb7c9bd620f', '17f5c7411eeeeedf25b0db99a9130aa353aee4ba']",2019,https://semanticscholar.org/paper/5ec1906fbd2f34af88a987d381294e22a10c9165,['DBLP'],"['http://papers.nips.cc/paper/8533-drill-down-interactive-retrieval-of-complex-scenes-using-natural-language-queries.pdf', 'http://www.cs.virginia.edu/~ft3ex/projects/drilldown/DrillDown_slides.pdf', 'https://arxiv.org/pdf/1911.03826v1.pdf', 'http://arxiv.org/abs/1911.03826', 'http://papers.nips.cc/paper/8533-drill-down-interactive-retrieval-of-complex-scenes-using-natural-language-queries']",NeurIPS,['Computer Science'],2970944013,145620451,https://api.github.com/repos/uvavision/DrillDown,15444027,DrillDown,\N,Python,2019-10-10 15:32:59,\N,0,2021-02-24 10:36:40,\N,https://github.com/uvavision/DrillDown,5,4,Python,2019-10-10 15:32:59,2022-04-15 12:14:25,0,TRUE,TRUE,no homepageUrl,6,[],"# [Drill-down: Interactive Retrieval of Complex Scenes using Natural Language Queries.](https://arxiv.org/abs/1911.03826)
Fuwen Tan, Paola Cascante-Bonilla, Xiaoxiao Guo, Hui Wu, Song Feng, Vicente Ordonez. NeurIPS 2019


## Overview
This paper explores the task of interactive image retrieval using natural language queries, where a user progressively provides input queries to refine a set of retrieval results. Moreover, our work explores this problem in the context of complex image scenes containing multiple objects. We propose Drill-down, an effective framework for encoding multiple queries with an efficient compact state representation that significantly extends current methods for single-round image retrieval. We show that using multiple rounds of natural language queries as input can be surprisingly effective to find arbitrarily specific images of complex scenes. Furthermore, we find that existing image datasets with textual captions can provide a surprisingly effective form of weak supervision for this task. We compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.

## Requirements
- Setup a conda environment and install some prerequisite packages like this
```bash
conda create -n retrieval python=3.6    # Create a virtual environment
source activate retrieval         	    # Activate virtual environment
conda install jupyter scikit-image cython opencv seaborn nltk pycairo h5py  # Install dependencies
python -m nltk.downloader all				    # Install NLTK data
```
- Please also install [pytorch](http://pytorch.org/) 1.0 (or higher), torchVision, and torchtext


## Data 
- Download the images of the Visual Genome dataset if you have not done so
```Shell
./experiments/scripts/fetch_images.sh
```
This will populate the `DrillDown/data` folder with `vg/VG_100K` and `vg/VG_100K_2`.

- Download the annotations of the images
```Shell
./experiments/scripts/fetch_annotations.sh
```
This will populate the `DrillDown/data` folder with `vg/sg_xmls` and `vg/rg_jsons`, which are per-image scene-graph and region-graph annotations.

- Download the global image features and region features
```Shell
./experiments/scripts/fetch_features.sh
```
This will populate the `DrillDown/data` folder with `vg/global_features` and `vg/region_36_final`, which are the global features and region features of the images. The global features were extracted from a pretrained ResNet101 model. The region features were extracted from a pretrained FasterRCNN model provided by https://github.com/peteanderson80/bottom-up-attention. Please see `tools/save_image_features.py`, the FasterRCNN repo, and `tools/save_region_features.py` for more details.

- Download the pretrained models
```Shell
./experiments/scripts/fetch_pretrained_models.sh
```
This will populate the `DrillDown/data` folder with `caches/image_ckpts` and `caches/region_ckpts`


## Training/evaluation scripts
The training/evaluation scripts of different models are also included in `./experiments/scripts`. 
The results will appear in `DrillDown/logs`
Please note that, when finetuning the supervisedly pretrained DrillDown model, e.g. runing the script
```Shell
./experiments/scripts/train_drill_down_3x128_reinforce.sh
```
the default pretrained model is `DrillDown/caches/region_ckpts/vg_f128_i3_sl_ckpt.pkl`.



## Citing

If you find our paper/code useful, please consider citing:

	@InProceedings{drilldown,
    author={Fuwen Tan and Paola Cascante-Bonilla and Xiaoxiao Guo and Hui Wu and Song Feng and Vicente Ordonez},
    title={Drill-down: Interactive Retrieval of Complex Scenes using Natural Language Queries},
    booktitle = {Neural Information Processing Systems (NeurIPS)},
    month = {December},
    year = {2019}
    }


    
# License

This project is licensed under the [MIT license](https://opensource.org/licenses/MIT):

Copyright (c) 2019 University of Virginia, Fuwen Tan, Paola Cascante-Bonilla, Xiaoxiao Guo, Hui Wu, Song Feng, Vicente Ordonez.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""Software""), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.







",52,138,7,0,311,2016-06-14 8:00:38,https://github.com/uvavision/DrillDown,4,972,FALSE,no mirror url,1,0,0,Organization,no company,no email or private email,no twitter_username or private twitter_username,14,2021-12-02 17:48:56,no License,0,2.653995893,2.57089468,2,2,DrillDown,pytorch,"<h1><a href=""https://arxiv.org/abs/1911.03826"">drill-down: interactive retrieval of complex scenes using natural language queries.</a></h1>
<p>fuwen tan, paola cascante-bonilla, xiaoxiao guo, hui wu, song feng, vicente ordonez. neurips 2019</p>
<h2>overview</h2>
<p>this paper explores the task of interactive image retrieval using natural language queries, where a user progressively provides input queries to refine a set of retrieval results. moreover, our work explores this problem in the context of complex image scenes containing multiple objects. we propose drill-down, an effective framework for encoding multiple queries with an efficient compact state representation that significantly extends current methods for single-round image retrieval. we show that using multiple rounds of natural language queries as input can be surprisingly effective to find arbitrarily specific images of complex scenes. furthermore, we find that existing image datasets with textual captions can provide a surprisingly effective form of weak supervision for this task. we compare our method with existing sequential encoding and embedding networks, demonstrating superior performance on two proposed benchmarks: automatic image retrieval on a simulated scenario that uses region captions as queries, and interactive image retrieval using real queries from human evaluators.</p>
<h2>requirements</h2>
<ul>
<li>setup a conda environment and install some prerequisite packages like this<pre><code class=""lang-bash"">conda create -n retrieval python=3.6    # create a virtual environment
source activate retrieval               # activate virtual environment
conda install jupyter scikit-image cython opencv seaborn nltk pycairo h5py  # install dependencies
python -m nltk.downloader all                   # install nltk data
</code></pre>
</li>
<li>please also install <a href=""http://pytorch.org/"">pytorch</a> 1.0 (or higher), torchvision, and torchtext</li>
</ul>
<h2>data</h2>
<ul>
<li><p>download the images of the visual genome dataset if you have not done so</p>
<pre><code class=""lang-shell"">./experiments/scripts/fetch_images.sh
</code></pre>
<p>this will populate the <code>drilldown/data</code> folder with <code>vg/vg_100k</code> and <code>vg/vg_100k_2</code>.</p>
</li>
<li><p>download the annotations of the images</p>
<pre><code class=""lang-shell"">./experiments/scripts/fetch_annotations.sh
</code></pre>
<p>this will populate the <code>drilldown/data</code> folder with <code>vg/sg_xmls</code> and <code>vg/rg_jsons</code>, which are per-image scene-graph and region-graph annotations.</p>
</li>
<li><p>download the global image features and region features</p>
<pre><code class=""lang-shell"">./experiments/scripts/fetch_features.sh
</code></pre>
<p>this will populate the <code>drilldown/data</code> folder with <code>vg/global_features</code> and <code>vg/region_36_final</code>, which are the global features and region features of the images. the global features were extracted from a pretrained resnet101 model. the region features were extracted from a pretrained fasterrcnn model provided by <a href=""https://github.com/peteanderson80/bottom-up-attention"">https://github.com/peteanderson80/bottom-up-attention</a>. please see <code>tools/save_image_features.py</code>, the fasterrcnn repo, and <code>tools/save_region_features.py</code> for more details.</p>
</li>
<li><p>download the pretrained models</p>
<pre><code class=""lang-shell"">./experiments/scripts/fetch_pretrained_models.sh
</code></pre>
<p>this will populate the <code>drilldown/data</code> folder with <code>caches/image_ckpts</code> and <code>caches/region_ckpts</code></p>
</li>
</ul>
<h2>training/evaluation scripts</h2>
<p>the training/evaluation scripts of different models are also included in <code>./experiments/scripts</code>. 
the results will appear in <code>drilldown/logs</code>
please note that, when finetuning the supervisedly pretrained drilldown model, e.g. runing the script</p>
<pre><code class=""lang-shell"">./experiments/scripts/train_drill_down_3x128_reinforce.sh
</code></pre>
<p>the default pretrained model is <code>drilldown/caches/region_ckpts/vg_f128_i3_sl_ckpt.pkl</code>.</p>
<h2>citing</h2>
<p>if you find our paper/code useful, please consider citing:</p>
<pre><code>@inproceedings{drilldown,
author={fuwen tan and paola cascante-bonilla and xiaoxiao guo and hui wu and song feng and vicente ordonez},
title={drill-down: interactive retrieval of complex scenes using natural language queries},
booktitle = {neural information processing systems (neurips)},
month = {december},
year = {2019}
}
</code></pre>
<h1>license</h1>
<p>this project is licensed under the <a href=""https://opensource.org/licenses/mit"">mit license</a>:</p>
<p>copyright (c) 2019 university of virginia, fuwen tan, paola cascante-bonilla, xiaoxiao guo, hui wu, song feng, vicente ordonez.</p>
<p>permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""software""), to deal
in the software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the software, and to permit persons to whom the software is
furnished to do so, subject to the following conditions:</p>
<p>the above copyright notice and this permission notice shall be included in all
copies or substantial portions of the software.</p>
<p>the software is provided ""as is"", without warranty of any kind, express or
implied, including but not limited to the warranties of merchantability,
fitness for a particular purpose and noninfringement. in no event shall the
authors or copyright holders be liable for any claim, damages or other
liability, whether in an action of contract, tort or otherwise, arising from,
out of or in connection with the software or the use or other dealings in the
software.</p>
",2,1,17,0,0,FALSE,0,1,TRUE,TRUE,TRUE,4,4,TRUE,FALSE,FALSE,56,5883,20,675,1,FALSE,FALSE,FALSE,drill-down: interactive retrieval of complex scenes using natural language queries,0,drill-down: interactive retrieval of complex scenes using natural language queries,6,15,15,15,15,15,0,196,196,196,196,196,0,7,7,7,7,7,0,15,196,7,['no affiliation'],FALSE,15,196,7,['no affiliation'],FALSE,FALSE,FALSE,11,37,3,1,0,1,1,5,1,1,A*,10,155
https://paperswithcode.com/paper/defense-vae-a-fast-and-accurate-defense,1812.0657,https://arxiv.org/abs/1812.06570v3,https://arxiv.org/pdf/1812.06570v3.pdf,https://github.com/lxuniverse/defense-vae,TRUE,TRUE,"Deep neural networks (DNNs) have been enormously successful across a variety of prediction tasks. However, recent research shows that DNNs are particularly vulnerable to adversarial attacks, which poses a serious threat to their applications in security-sensitive systems. In this paper, we propose a simple yet effective defense algorithm Defense-VAE that uses variational autoencoder (VAE) to purge adversarial perturbations from contaminated images. The proposed method is generic and can defend white-box and black-box attacks without the need of retraining the original CNN classifiers, and can further strengthen the defense by retraining CNN or end-to-end finetuning the whole pipeline. In addition, the proposed method is very efficient compared to the optimization-based alternatives, such as Defense-GAN, since no iterative optimization is needed for online prediction. Extensive experiments on MNIST, Fashion-MNIST, CelebA and CIFAR-10 demonstrate the superior defense accuracy of Defense-VAE compared to Defense-GAN, while being 50x faster than the latter. This makes Defense-VAE widely deployable in real-time security-sensitive systems. Our source code can be found at https://github.com/lxuniverse/defense-vae.",no proceeding,"['Xiang Li', 'Shihao Ji']",[],2018-12-17,"[{'name': 'AutoEncoder', 'full_name': 'AutoEncoder', 'description': 'An **Autoencoder** is a bottleneck architecture that turns a high-dimensional input into a latent low-dimensional code (encoder), and then performs a reconstruction of the input with this latent code (the decoder).\r\n\r\nImage: [Michael Massi](https://en.wikipedia.org/wiki/Autoencoder#/media/File:Autoencoder_schema.png)', 'introduced_year': 2000, 'source_url': 'https://science.sciencemag.org/content/313/5786/504', 'source_title': 'Reducing the Dimensionality of Data with Neural Networks', 'code_snippet_url': 'https://github.com/L1aoXingyu/pytorch-beginner/blob/9c86be785c7c318a09cf29112dd1f1a58613239b/08-AutoEncoder/simple_autoencoder.py#L38', 'main_collection': {'name': 'Generative Models', 'description': '**Generative Models** aim to model data generatively (rather than discriminatively), that is they aim to approximate the probability distribution of the data. Below you can find a continuously updating list of generative models for computer vision.', 'parent': None, 'area': 'Computer Vision'}}]",56599a6e1a3b552081f82f0153e85bfc3144e07b,"[{'name': 'Xiang  Li', 'ids': ['50879544']}, {'name': 'Shihao  Ji', 'ids': ['144576567']}]","['03361ebe6479eeca6e92ca5b462f6f721d1517ae', '3da83ec72d9d84be345f28052cb2ff96b4f4843f', 'a4b06da631e6f348b02315c316c767258ee245b9', '4a4864e05ac6bc5ba70f67da60a4bd38418993c7', '5609e1c4208b1b82f4138bfb6b8a68a94029515f', '1c7c369483a8f8c85d732c1f999bb7eb11c32a2f']","['6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4', 'abf77aff13ccd0301a3e693f5d1247a924791bad', '5f5dc5b9a2ba710937e2c413b37b053cd673df02', '5d90f06bb70a0a3dced62413346235c02b1aa086', '63a010c69f00e65c946a68b546bbd42cbed03564', '484ad17c926292fbe0d5211540832a8c8a8e958b', '819167ace2f0caae7745d2f25a803979be5fbfae', '301aa261120059b0a7b62103c49504c48ead5de0', 'e225dd59ef4954db21479cdcbee497624b2d6d0f', '4f9f7434f06cbe31e54a0bb118975340b9e0a4c9', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '7178bf9576ea3c8094953cbad5cfa950502a3dc0', '53b047e503f4c24602f376a774d653f7ed56c024', '0f237ff1802c8d594829139602710114dc65205e', '78aa018ee7d52360e15d103390ea1cdb3a0beb41', '5e9ed7e980634562a329e85ad225d23ea77f3d1f', '033c08ca48aaed2d5ab0a17d668d410538678ed8', '9fec45e1ff97ffb0e0cf9f039e39b46043430301', '162d958ff885f1462aeda91cd72582323fd6a1f4', 'c4413dd4a51ab86d09c165d4b2fe1dc2168fc1ff', 'df40ce107a71b770c9d0354b78fdd8989da80d2f', 'ee48b932a60085d7fd5540637540509144b07030', 'f9c602cc436a9ea2f9e7db48c77d924e09ce3c32', 'e2a85a6766b982ff7c8980e57ca6342d22493827', 'ac8e45a0451ac578f17f631fc2663ee4b98b83a9', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', 'c00f744f103a528f5b45bf0482f54b5e6a9f7740', '600a5d60cb96eda2a9849413e747547d70dfb00a', '6adf016e7531c91100d3cf4a74f5d4c87b26b528', 'f7bb1636ced9036b3d0edafc7d82ad43164d41a3', '52693bcf4627bdbc31734e7f3dbcf5ea7eef4d35']",2019,https://semanticscholar.org/paper/56599a6e1a3b552081f82f0153e85bfc3144e07b,['DBLP'],"['http://arxiv.org/abs/1812.06570', 'https://doi.org/10.1007/978-3-030-43887-6_15', 'https://arxiv.org/pdf/1812.06570v2.pdf']",PKDD/ECML Workshops,['Computer Science'],3013135147,151090120,https://api.github.com/repos/lxuniverse/defense-vae,13335239,defense-vae,"Implementation for \Defense-VAE: A Fast and Accurate Defense against Adversarial Attacks\""""",Python,2019-06-19 19:19:41,\N,0,2021-01-06 23:17:46,\N,https://github.com/lxuniverse/defense-vae,5,2,Python,2019-06-19 19:19:41,2020-12-23 16:59:45,1,TRUE,TRUE,no homepageUrl,2,[],"# Defense-VAE
This repository contains the implementation of [defense-vae](https://arxiv.org/abs/1812.06570).

## Dependences
Python 2.7, Pytorch, Tensorflow 1.7, Cleverhans 2.1.0

## Repo Structure

* white_box: scripts to generate adversarial images from training and test data, train VAE, reconstruct from adversarial images, and test white-box defense accuracy. 
* black_box: scripts to generate adversarial images from test data, reconstruct from adversarial images, and test black-box defense accuracy.
* data: folder to place raw image data.
* cnn_models: pretrained CNN models as the attacking targets and pretrained substitute models for black-box attacks. 

## Quick Start

* Clone the Repo

* Download the datasets:
    * cd data
    * sh download_datasets.sh

* For the white-box defense, please run:
    * cd white_box
    * python main_script.py --gpu 0 --dataset 1 --cnn_model 1 
        * --gpu: choose GPU index, use 0 if you only have one.
        * --dataset: 1 or 2, where 1 is for MNIST and 2 is for FMNIST.
        * --cnn_model: 1 ~ 4 corresponding to model a~d. See the paper for details.

* For the black-box defense, please run:
    * cd black_box
    * python main_script.py --gpu 0 --dataset 1 --cnn_model_bb 1 --cnn_model_sub 1 
        * --gpu: choose GPU index, use 0 if you only have one.
        * --dataset: 1 or 2, where 1 is for MNIST and 2 is for FMNIST.
        * --cnn_model_bb: 1 ~ 4, choose the model to be attacked, which corresponds to model a~d. See the paper for details.
        * --cnn_model_sub: 1 or 2 corresponding to model b or e. See the paper for details.

* The final results will be written into results.txt in the white_box and black_box folders. The attack index 1,2,3 corresponds to the FGSM, RANDFGSM and CW attack.

* Please run the black-box experiments after the white-box experiments because the black-box experiments need the VAE models trained in the white-box experiments. 

## Citation

If you found this paper useful, please cite it.
    
    @inproceedings{defense-vae19,
      title     = {Defense-VAE: A Fast and Accurate Defense Against Adversarial Attacks},
      author    = {Xiang Li and Shihao Ji},
      booktitle = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
      publisher = {Springer},
      pages     = {191--207},
      year      = {2019}
    }

## Contact

Xiang Li, xli62@student.gsu.edu

",10,154,75,3,366,2015-05-29 13:15:31,https://github.com/lxuniverse/defense-vae,2,742135,FALSE,no mirror url,2,0,0,User,no company,xli62@gsu.edu,no twitter_username or private twitter_username,7,2022-03-23 3:40:55,no License,0,2.962948354,3.468928178,2,3,defense-vae,tf,"<h1>defense-vae</h1>
<p>this repository contains the implementation of <a href=""https://arxiv.org/abs/1812.06570"">defense-vae</a>.</p>
<h2>dependences</h2>
<p>python 2.7, pytorch, tensorflow 1.7, cleverhans 2.1.0</p>
<h2>repo structure</h2>
<ul>
<li>white_box: scripts to generate adversarial images from training and test data, train vae, reconstruct from adversarial images, and test white-box defense accuracy. </li>
<li>black_box: scripts to generate adversarial images from test data, reconstruct from adversarial images, and test black-box defense accuracy.</li>
<li>data: folder to place raw image data.</li>
<li>cnn_models: pretrained cnn models as the attacking targets and pretrained substitute models for black-box attacks. </li>
</ul>
<h2>quick start</h2>
<ul>
<li><p>clone the repo</p>
</li>
<li><p>download the datasets:</p>
<ul>
<li>cd data</li>
<li>sh download_datasets.sh</li>
</ul>
</li>
<li><p>for the white-box defense, please run:</p>
<ul>
<li>cd white_box</li>
<li>python main_script.py --gpu 0 --dataset 1 --cnn_model 1 <ul>
<li>--gpu: choose gpu index, use 0 if you only have one.</li>
<li>--dataset: 1 or 2, where 1 is for mnist and 2 is for fmnist.</li>
<li>--cnn_model: 1 ~ 4 corresponding to model a~d. see the paper for details.</li>
</ul>
</li>
</ul>
</li>
<li><p>for the black-box defense, please run:</p>
<ul>
<li>cd black_box</li>
<li>python main_script.py --gpu 0 --dataset 1 --cnn_model_bb 1 --cnn_model_sub 1 <ul>
<li>--gpu: choose gpu index, use 0 if you only have one.</li>
<li>--dataset: 1 or 2, where 1 is for mnist and 2 is for fmnist.</li>
<li>--cnn_model_bb: 1 ~ 4, choose the model to be attacked, which corresponds to model a~d. see the paper for details.</li>
<li>--cnn_model_sub: 1 or 2 corresponding to model b or e. see the paper for details.</li>
</ul>
</li>
</ul>
</li>
<li><p>the final results will be written into results.txt in the white_box and black_box folders. the attack index 1,2,3 corresponds to the fgsm, randfgsm and cw attack.</p>
</li>
<li><p>please run the black-box experiments after the white-box experiments because the black-box experiments need the vae models trained in the white-box experiments.</p>
</li>
</ul>
<h2>citation</h2>
<p>if you found this paper useful, please cite it.</p>
<pre><code>@inproceedings{defense-vae19,
  title     = {defense-vae: a fast and accurate defense against adversarial attacks},
  author    = {xiang li and shihao ji},
  booktitle = {joint european conference on machine learning and knowledge discovery in databases},
  publisher = {springer},
  pages     = {191--207},
  year      = {2019}
}
</code></pre>
<h2>contact</h2>
<p>xiang li, xli62@student.gsu.edu</p>
",7,1,0,0,0,FALSE,0,0,FALSE,FALSE,TRUE,4,4,TRUE,FALSE,FALSE,16,1788,13,357,11,FALSE,TRUE,FALSE,defense-vae: a fast and accurate defense against adversarial attacks,0,defense-vae: a fast and accurate defense against adversarial attacks,2,44,44,44,44,44,0,3111,3111,3111,3111,3111,0,14,14,14,14,14,0,44,3111,14,['no affiliation'],FALSE,0,0,0,['no affiliation'],FALSE,TRUE,FALSE,11,31,6,0,1,1,1,3,1,1,unknow,9,164
https://paperswithcode.com/paper/is-bert-really-robust-natural-language-attack,1907.11932,https://arxiv.org/abs/1907.11932v6,https://arxiv.org/pdf/1907.11932v6.pdf,https://github.com/jind11/TextFooler,TRUE,TRUE,"Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate natural adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate the advantages of this framework in three ways: (1) effective---it outperforms state-of-the-art attacks in terms of success rate and perturbation rate, (2) utility-preserving---it preserves semantic content and grammaticality, and remains correctly classified by humans, and (3) efficient---it generates adversarial text with computational complexity linear to the text length. *The code, pre-trained target models, and test examples are available at https://github.com/jind11/TextFooler.",no proceeding,"['Di Jin', 'Zhijing Jin', 'Joey Tianyi Zhou', 'Peter Szolovits']","['Adversarial Text', 'General Classification', 'Natural Language Inference', 'Text Classification']",2019-07-27,"[{'name': 'Weight Decay', 'full_name': 'Weight Decay', 'description': '**Weight Decay**, or **$L_{2}$ Regularization**, is a regularization technique applied to the weights of a neural network. We minimize a loss function compromising both the primary loss function and a penalty on the $L\\_{2}$ Norm of the weights:\r\n\r\n$$L\\_{new}\\left(w\\right) = L\\_{original}\\left(w\\right) + \\lambda{w^{T}w}$$\r\n\r\nwhere $\\lambda$ is a value determining the strength of the penalty (encouraging smaller weights). \r\n\r\nWeight decay can be incorporated directly into the weight update rule, rather than just implicitly by defining it through to objective function. Often weight decay refers to the implementation where we specify it directly in the weight update rule (whereas L2 regularization is usually the implementation which is specified in the objective function).\r\n\r\nImage Source: Deep Learning, Goodfellow et al', 'introduced_year': 1943, 'source_url': None, 'source_title': None, 'code_snippet_url': '', 'main_collection': {'name': 'Regularization', 'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.', 'parent': None, 'area': 'General'}}, {'name': 'Residual Connection', 'full_name': 'Residual Connection', 'description': '**Residual Connections** are a type of skip-connection that learn residual functions with reference to the layer inputs, instead of learning unreferenced functions. \r\n\r\nFormally, denoting the desired underlying mapping as $\\mathcal{H}({x})$, we let the stacked nonlinear layers fit another mapping of $\\mathcal{F}({x}):=\\mathcal{H}({x})-{x}$. The original mapping is recast into $\\mathcal{F}({x})+{x}$.\r\n\r\nThe intuition is that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.', 'introduced_year': 2000, 'source_url': 'http://arxiv.org/abs/1512.03385v1', 'source_title': 'Deep Residual Learning for Image Recognition', 'code_snippet_url': 'https://github.com/pytorch/vision/blob/7c077f6a986f05383bcb86b535aedb5a63dd5c4b/torchvision/models/resnet.py#L118', 'main_collection': {'name': 'Skip Connections', 'description': '**Skip Connections** allow layers to skip layers and connect to layers further up the network, allowing for information to flow more easily up the network. Below you can find a continuously updating list of skip connection methods.', 'parent': None, 'area': 'General'}}, {'name': 'Adam', 'full_name': 'Adam', 'description': '**Adam** is an adaptive learning rate optimization algorithm that utilises both momentum and scaling, combining the benefits of [RMSProp](https://paperswithcode.com/method/rmsprop) and [SGD w/th Momentum](https://paperswithcode.com/method/sgd-with-momentum). The optimizer is designed to be appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. \r\n\r\nThe weight updates are performed as:\r\n\r\n$$ w_{t} = w_{t-1} - \\eta\\frac{\\hat{m}\\_{t}}{\\sqrt{\\hat{v}\\_{t}} + \\epsilon}  $$\r\n\r\nwith\r\n\r\n$$ \\hat{m}\\_{t} = \\frac{m_{t}}{1-\\beta^{t}_{1}} $$\r\n\r\n$$ \\hat{v}\\_{t} = \\frac{v_{t}}{1-\\beta^{t}_{2}} $$\r\n\r\n$$ m_{t} = \\beta_{1}m_{t-1} + (1-\\beta_{1})g_{t} $$\r\n\r\n$$ v_{t} = \\beta_{2}v_{t-1} + (1-\\beta_{2})g_{t}^{2}  $$\r\n\r\n\r\n$ \\eta $ is the step size/learning rate, around 1e-3 in the original paper. $ \\epsilon $ is a small number, typically 1e-8 or 1e-10, to prevent dividing by zero. $ \\beta_{1} $ and $ \\beta_{2} $ are forgetting parameters, with typical values 0.9 and 0.999, respectively.', 'introduced_year': 2000, 'source_url': 'http://arxiv.org/abs/1412.6980v9', 'source_title': 'Adam: A Method for Stochastic Optimization', 'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/optim/adam.py#L6', 'main_collection': {'name': 'Stochastic Optimization', 'description': ""**Stochastic Optimization** methods are used to optimize neural networks. We typically take a mini-batch of data, hence 'stochastic', and perform a type of gradient descent with this minibatch. Below you can find a continuously updating list of stochastic optimization algorithms."", 'parent': 'Optimization', 'area': 'General'}}, {'name': 'Layer Normalization', 'full_name': 'Layer Normalization', 'description': 'Unlike [batch normalization](https://paperswithcode.com/method/batch-normalization), **Layer Normalization** directly estimates the normalization statistics from the summed inputs to the neurons within a hidden layer so the normalization does not introduce any new dependencies between training cases. It works well for [RNNs](https://paperswithcode.com/methods/category/recurrent-neural-networks) and improves both the training time and the generalization performance of several existing RNN models. More recently, it has been used with [Transformer](https://paperswithcode.com/methods/category/transformers) models.\r\n\r\nWe compute the layer normalization statistics over all the hidden units in the same layer as follows:\r\n\r\n$$ \\mu^{l} = \\frac{1}{H}\\sum^{H}\\_{i=1}a\\_{i}^{l} $$\r\n\r\n$$ \\sigma^{l} = \\sqrt{\\frac{1}{H}\\sum^{H}\\_{i=1}\\left(a\\_{i}^{l}-\\mu^{l}\\right)^{2}}  $$\r\n\r\nwhere $H$ denotes the number of hidden units in a layer. Under layer normalization, all the hidden units in a layer share the same normalization terms $\\mu$ and $\\sigma$, but different training cases have different normalization terms. Unlike batch normalization, layer normalization does not impose any constraint on the size of the mini-batch and it can be used in the pure online regime with batch size 1.', 'introduced_year': 2000, 'source_url': 'http://arxiv.org/abs/1607.06450v1', 'source_title': 'Layer Normalization', 'code_snippet_url': 'https://github.com/CyberZHG/torch-layer-normalization/blob/89f405b60f53f85da6f03fe685c190ef394ce50c/torch_layer_normalization/layer_normalization.py#L8', 'main_collection': {'name': 'Normalization', 'description': '**Normalization** layers in deep learning are used to make optimization easier by smoothing the loss surface of the network. Below you will find a continuously updating list of normalization  methods.', 'parent': None, 'area': 'General'}}, {'name': 'Softmax', 'full_name': 'Softmax', 'description': ""The **Softmax** output function transforms a previous layer's output into a vector of probabilities. It is commonly used for multiclass classification.  Given an input vector $x$ and a weighting vector $w$ we have:\r\n\r\n$$ P(y=j \\mid{x}) = \\frac{e^{x^{T}w_{j}}}{\\sum^{K}_{k=1}e^{x^{T}wk}} $$"", 'introduced_year': 2000, 'source_url': None, 'source_title': None, 'code_snippet_url': None, 'main_collection': {'name': 'Output Functions', 'description': '**Output functions** are layers used towards the end of a network to transform to the desired form for a loss function. For example, the softmax relies on logits to construct a conditional probability. Below you can find a continuously updating list of output functions.', 'parent': None, 'area': 'General'}}, {'name': 'Scaled Dot-Product Attention', 'full_name': 'Scaled Dot-Product Attention', 'description': '**Scaled dot-product attention** is an attention mechanism where the dot products are scaled down by $\\sqrt{d_k}$. Formally we have a query $Q$, a key $K$ and a value $V$ and calculate the attention as:\r\n\r\n$$ {\\text{Attention}}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^{T}}{\\sqrt{d_k}}\\right)V $$\r\n\r\nIf we assume that $q$ and $k$ are $d_k$-dimensional vectors whose components are independent random variables with mean $0$ and variance $1$, then their dot product, $q \\cdot k = \\sum_{i=1}^{d_k} u_iv_i$, has mean $0$ and variance $d_k$.  Since we would prefer these values to have variance $1$, we divide by $\\sqrt{d_k}$.', 'introduced_year': 2000, 'source_url': 'http://arxiv.org/abs/1706.03762v5', 'source_title': 'Attention Is All You Need', 'code_snippet_url': 'https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/5c0264915ab43485adc576f88971fc3d42b10445/transformer/Modules.py#L7', 'main_collection': {'name': 'Attention Mechanisms', 'description': '**Attention Mechanisms** are a component used in neural networks to model long-range interaction, for example across a text in NLP. The key idea is to build shortcuts between a context vector and the input, to allow a model to attend to different parts. Below you can find a continuously updating list of attention mechanisms.', 'parent': 'Attention', 'area': 'General'}}, {'name': 'Dropout', 'full_name': 'Dropout', 'description': '**Dropout** is a regularization technique for neural networks that drops a unit (along with connections) at training time with a specified probability $p$ (a common value is $p=0.5$). At test time, all units are present, but with weights scaled by $p$ (i.e. $w$ becomes $pw$).\r\n\r\nThe idea is to prevent co-adaptation, where the neural network becomes too reliant on particular connections, as this could be symptomatic of overfitting. Intuitively, dropout can be thought of as creating an implicit ensemble of neural networks.', 'introduced_year': 2000, 'source_url': 'http://jmlr.org/papers/v15/srivastava14a.html', 'source_title': 'Dropout: A Simple Way to Prevent Neural Networks from Overfitting', 'code_snippet_url': 'https://github.com/google/jax/blob/7f3078b70d0ed9bea6228efa420879c56f72ef69/jax/experimental/stax.py#L271-L275', 'main_collection': {'name': 'Regularization', 'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.', 'parent': None, 'area': 'General'}}, {'name': 'GELU', 'full_name': 'Gaussian Error Linear Units', 'description': ""The **Gaussian Error Linear Unit**, or **GELU**,  is an activation function. The GELU activation function is $x\\Phi(x)$, where $\\Phi(x)$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their percentile, rather than gates inputs by their sign as in [ReLUs](https://paperswithcode.com/method/relu) ($x\\mathbf{1}_{x>0}$). Consequently the GELU can be thought of as a smoother ReLU.\r\n\r\n$$\\text{GELU}\\left(x\\right) = x{P}\\left(X\\leq{x}\\right) = x\\Phi\\left(x\\right) = x \\cdot \\frac{1}{2}\\left[1 + \\text{erf}(x/\\sqrt{2})\\right],$$\r\nif $X\\sim \\mathcal{N}(0,1)$.\r\n\r\nOne can approximate the GELU with\r\n$0.5x\\left(1+\\tanh\\left[\\sqrt{2/\\pi}\\left(x + 0.044715x^{3}\\right)\\right]\\right)$ or $x\\sigma\\left(1.702x\\right),$\r\nbut PyTorch's exact implementation is sufficiently fast such that these approximations may be unnecessary. (See also the [SiLU](https://paperswithcode.com/method/silu) $x\\sigma(x)$ which was also coined in the paper that introduced the GELU.)\r\n\r\nGELUs are used in [GPT-3](https://paperswithcode.com/method/gpt-3), [BERT](https://paperswithcode.com/method/bert), and most other Transformers."", 'introduced_year': 2000, 'source_url': 'https://arxiv.org/abs/1606.08415v4', 'source_title': 'Gaussian Error Linear Units (GELUs)', 'code_snippet_url': 'https://github.com/pytorch/pytorch/blob/96aaa311c0251d24decb9dc5da4957b7c590af6f/torch/nn/modules/activation.py#L584', 'main_collection': {'name': 'Activation Functions', 'description': '**Activation functions** are functions that we apply in neural networks after (typically) applying an affine transformation combining weights and input features. They are typically non-linear functions. The rectified linear unit, or ReLU, has been the most popular in the past decade, although the choice is architecture dependent and many alternatives have emerged in recent years. In this section, you will find a constantly updating list of activation functions.', 'parent': None, 'area': 'General'}}, {'name': 'Multi-Head Attention', 'full_name': 'Multi-Head Attention', 'description': '**Multi-head Attention** is a module for attention mechanisms which runs through an attention mechanism several times in parallel. The independent attention outputs are then concatenated and linearly transformed into the expected dimension. Intuitively, multiple attention heads allows for attending to parts of the sequence differently (e.g. longer-term dependencies versus shorter-term dependencies). \r\n\r\n$$ \\text{MultiHead}\\left(\\textbf{Q}, \\textbf{K}, \\textbf{V}\\right) = \\left[\\text{head}\\_{1},\\dots,\\text{head}\\_{h}\\right]\\textbf{W}_{0}$$\r\n\r\n$$\\text{where} \\text{ head}\\_{i} = \\text{Attention} \\left(\\textbf{Q}\\textbf{W}\\_{i}^{Q}, \\textbf{K}\\textbf{W}\\_{i}^{K}, \\textbf{V}\\textbf{W}\\_{i}^{V} \\right) $$\r\n\r\nAbove $\\textbf{W}$ are all learnable parameter matrices.\r\n\r\nNote that [scaled dot-product attention](https://paperswithcode.com/method/scaled) is most commonly used in this module, although in principle it can be swapped out for other types of attention mechanism.\r\n\r\nSource: [Lilian Weng](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms)', 'introduced_year': 2000, 'source_url': 'http://arxiv.org/abs/1706.03762v5', 'source_title': 'Attention Is All You Need', 'code_snippet_url': 'https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/fec78a687210851f055f792d45300d27cc60ae41/transformer/SubLayers.py#L9', 'main_collection': {'name': 'Attention Modules', 'description': '**Attention Modules** refer to modules that incorporate attention mechanisms. For example, multi-head attention is a module that incorporates multiple attention heads. Below you can find a continuously updating list of attention modules.', 'parent': 'Attention', 'area': 'General'}}, {'name': 'Attention Dropout', 'full_name': 'Attention Dropout', 'description': '**Attention Dropout** is a type of [dropout](https://paperswithcode.com/method/dropout) used in attention-based architectures, where elements are randomly dropped out of the [softmax](https://paperswithcode.com/method/softmax) in the attention equation. For example, for scaled-dot product attention, we would drop elements from the first term:\r\n\r\n$$ {\\text{Attention}}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^{T}}{\\sqrt{d_k}}\\right)V $$', 'introduced_year': 2018, 'source_url': None, 'source_title': None, 'code_snippet_url': 'https://github.com/huggingface/transformers/blob/4dc65591b5c61d75c3ef3a2a883bf1433e08fc45/src/transformers/modeling_tf_bert.py#L271', 'main_collection': {'name': 'Regularization', 'description': 'Regularization strategies are designed to reduce the test error of a machine learning algorithm, possibly at the expense of training error. Many different forms of regularization exist in the field of deep learning. Below you can find a constantly updating list of regularization strategies.', 'parent': None, 'area': 'General'}}, {'name': 'WordPiece', 'full_name': 'WordPiece', 'description': '**WordPiece** is a subword segmentation algorithm used in natural language processing.  The vocabulary is initialized with individual characters in the language, then the most frequent combinations of symbols in the vocabulary are iteratively added to the vocabulary. The process is:\r\n\r\n1. Initialize the word unit inventory with all the characters in the text.\r\n2. Build a language model on the training data using the inventory from 1.\r\n3. Generate a new word unit by combining two units out of the current word inventory to increment the word unit inventory by one. Choose the new word unit out of all the possible ones that increases the likelihood on the training data the most when added to the model.\r\n4. Goto 2 until a predefined limit of word units is reached or the likelihood increase falls below a certain threshold.\r\n\r\nText: [Source](https://stackoverflow.com/questions/55382596/how-is-wordpiece-tokenization-helpful-to-effectively-deal-with-rare-words-proble/55416944#55416944)\r\n\r\nImage: WordPiece as used in [BERT](https://paperswithcode.com/method/bert)', 'introduced_year': 2000, 'source_url': 'http://arxiv.org/abs/1609.08144v2', 'source_title': ""Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"", 'code_snippet_url': '', 'main_collection': {'name': 'Subword Segmentation', 'description': '', 'parent': None, 'area': 'Natural Language Processing'}}, {'name': 'Linear Warmup With Linear Decay', 'full_name': 'Linear Warmup With Linear Decay', 'description': '**Linear Warmup With Linear Decay** is a learning rate schedule in which we increase the learning rate linearly for $n$ updates and then linearly decay afterwards.', 'introduced_year': 2000, 'source_url': None, 'source_title': None, 'code_snippet_url': None, 'main_collection': {'name': 'Learning Rate Schedules', 'description': '**Learning Rate Schedules** refer to schedules for the learning rate during the training of neural networks. Below you can find a continuously updating list of learning rate schedules.', 'parent': None, 'area': 'General'}}, {'name': 'Dense Connections', 'full_name': 'Dense Connections', 'description': '**Dense Connections**, or **Fully Connected Connections**, are a type of layer in a deep neural network that use a linear operation where every input is connected to every output by a weight. This means there are $n\\_{\\text{inputs}}*n\\_{\\text{outputs}}$ parameters, which can lead to a lot of parameters for a sizeable network.\r\n\r\n$$h\\_{l} = g\\left(\\textbf{W}^{T}h\\_{l-1}\\right)$$\r\n\r\nwhere $g$ is an activation function.\r\n\r\nImage Source: Deep Learning by Goodfellow, Bengio and Courville', 'introduced_year': 2000, 'source_url': None, 'source_title': None, 'code_snippet_url': None, 'main_collection': {'name': 'Feedforward Networks', 'description': '**Feedforward Networks** are a type of neural network architecture which rely primarily on dense-like connections. Below you can find a continuously updating list of feedforward network components.', 'parent': None, 'area': 'General'}}, {'name': 'BERT', 'full_name': 'BERT', 'description': '**BERT**, or Bidirectional Encoder Representations from Transformers, improves upon standard [Transformers](http://paperswithcode.com/method/transformer) by removing the unidirectionality constraint by using a *masked language model* (MLM) pre-training objective. The masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. Unlike left-to-right language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pre-train a deep bidirectional Transformer. In addition to the masked language model, BERT uses a *next sentence prediction* task that jointly pre-trains text-pair representations. \r\n\r\nThere are two steps in BERT: *pre-training* and *fine-tuning*. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they\r\nare initialized with the same pre-trained parameters.', 'introduced_year': 2000, 'source_url': 'https://arxiv.org/abs/1810.04805v2', 'source_title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'code_snippet_url': 'https://github.com/google-research/bert', 'main_collection': {'name': 'Language Models', 'description': '**Language Models** are models for predicting the next word or character in a document. Below you can find a continuously updating list of language models.\r\n\r\n', 'parent': None, 'area': 'Natural Language Processing'}}]",7c38ed29fab5a95a0e747b0053470808efabe7fc,"[{'name': 'Di  Jin', 'ids': ['1860892']}, {'name': 'Zhijing  Jin', 'ids': ['8752221']}, {'name': 'Joey Tianyi Zhou', 'ids': ['10638646']}, {'name': 'Peter  Szolovits', 'ids': ['1679873']}]","['cdfcb7a9de6a0cb37bf2110bef619555a96258bc', '4658662425f71a711024728db90dce0f40c9f0e6', '3ab9145d5134e4e89bcceb1c8a95f9f98c98c5ff', '7bc6eac5e11764204711169f36ab71631a8cd998', '95cb2b1eb7447dea243b8a0feb5844f30d7a15b6', 'b2ee8df9a69a48131aa68c8fc154785f1807e70f', '9db99e9284dee54ca9b617bdbd1ec8824567cc33', '438d88c5b78abfda513228d675b7e876aa1d32ed', 'e9264e8f5bd00cd1609e9a0946d691f635c25cdd', '335a05c16fcbc7a2c2bf5221299de608b08030f0', '90d02b73b7ce303c42cf5180d5041165468db95d', '03b164280ae44ae476e0615f0088b22e99ebb712', '44bd6111520ebb7323e834d14aa8b2fe4e4b3b79', '9c9fafc3105325428fe6f6ef58709be433510b2f', '291a491cc5005630918ef0c4449918e582c597a1', '06f7c468e3b926089615d84e6f970333bbb60e42', 'd3fbec6757f53ddd811430cf6dda931f04f4f429', 'af0b84d24764fc5751af75a8f68fe535ef10d08b', '399d2cd5521c690e1c33b1f526e53013423bcfef', 'a4201ef44bcc583ceac570f2b0006ee07f1372e7', '2ffcf8352223c95ae8cef4daaec995525ecc926b', 'f44c4602d7ff3677827c12cb6b6d35a2195fd003', '309b906fed883e5efe4acf676c655ead21f6c17b', '55f381a2d34f70eace04e9186722f82711928a2a', 'd85b849168698cd3c40e74273527ed573e437239', '521fc48f06ee907d6f400814e387290422517ebd', '46dfe93b284c929e7340e3f59303f25ad1a9df88', 'aa2cd7b5a202e995adddd8bfe3fc0536faee70ed', 'caf76c91acad2b3cae629052ab467fe05f43b957', 'cc1a5a76c96a3b440b13f946e5c9f84cc7290ce0', 'd753b091ef3b8828032e4bf0072265f94c333d29', '2dc7741c3cd3c7fc0d0ae7b60cf7358f612e175b', 'c94529aff09763b607b7594197f1bbf01c006759', '3c2f6c2cf7c9b121a0e46a660846bb0a5dad0ee8', 'bc41ca07f25c16d98697dd5cb348245e4c9089d4', 'e06b36f1076b3497992b558d7707f053161dc840', '3a55618358fe9f5e81500a75f1502c3011154ae8', 'f3e81b9df9f1a451f305c91144702b30df84a2c7', '42d376cdf2437769b9619aa38db64921772920ca', '2ef1b6d00a6c1fb018e591676d45ca8510e0729f', 'cc4db47a416aba22d1073e59a6867b6997928b7c', '51782ae310e404090dad8828ef5c11846f7fd257', '494c8566ebf7c7ba921063f36298868f06d1da0d', '19bd0019879bf0f98ee89da04143d81ed2e41f87', 'e1b9d5d4cf17f5ceaf2d6798ffd594fd1d017eaa', '8a88bb34e3d49e41af264dad946ce11c0c2b6150', '03a787b58a1fa92f712b99050fcc1e60d66cea94', '14bbbf78bfc500aa5d7bbc8db347ce98fc8f6aaa', '1551a502edf7d6d9bbf18ebecae1ca8c5de1caf0', '6228199a5744ae54e059c141f2972f0ba8581590', '4362f09eedff1b5093ae304048dc9694df845079', 'a2d03cfdf73cab647ca9e944db4ec616d1f0582c', '5b5a677bf371a4f357bb739bc954d376b41b0c9a', '981c7b3cd70e23fce8f8877ccb6ee92810b65e85', '4b1f7ada084eea68f28375922b4da70ef9eac624', 'f156ecbbb9243522275490d698c6825f4d2e01af', '6f509be31111ed828486198ad3ca41acb5c1c257', '167f52d369b0979f27282af0f3a1a4be9c9be84b', '389036b1366b64579725457993c1f63a4f3370ba', '879eaab2275a364549809560b42f0fef357ebbce', '91ac65431b2dc46919e1673fde67671c29446812', 'f630a8df256e3da2f2a5578c5e53496292bd3b66', '0daa4e407db5ee8e6c5cdaf47c10351d36881bf4', '39f8cac9c1810d6eb3c3990fc51c8917e3495312', '19ca0b4d2152a1e1cf215a018cb382ba272819ff', '504bee90f96bf29197e23d0f66826318a47e68a1', '80f82affbd34e712266e4bafa3d9bdebd0252f0e', '65d9ffedf32cf903e8bf05e476e4beb476a98578', 'a0c49ad11968dc2a1226fe387d04e1e46cd5f659', 'dda4341a8584b7ecb8e7a0d1e87a9ad6a04e5655', '8f7ff936f25a8849bbc57ee5e667bf5b0f6fe584', '4f328f1dd49f91dd35bbcb07fbab0e861546b005', 'df341594841ecc611169821efacffe1185e2902e', '9472e1d0de6eee6aca36a8961316e5178e396374', 'ea2e2d4ae607a35828ec5c51164fac3a557c88b5', 'ea6dc99c5996fbf3aa9288a27cc92683e935c7c6', 'a6cdf2516825a93314f5aab29ae70263aac856f0']","['472644c5f4155635cf9e9e37540bfa53c20e7610', 'ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c', '7a96765c147c9c814803c8c9de28a1dd069271da', 'e2a85a6766b982ff7c8980e57ca6342d22493827', 'f37e1b62a767a307c046404ca96bc140b3e68cb5', '7aa38b85fa8cba64d6a4010543f6695dbf5f1386', '16aa01ca0834a924c25faad5d8bfef3fd1acfcfe', '3502b5ef1afb16f76bcae33db17179195bbcdaae', '3febb2bed8865945e7fddc99efd791887bb7e14f', '4c41104e871bccbd56494350a71d77a7f1da5bb0', '83e7654d545fbbaaf2328df365a781fb67b841b4', 'a76706d350b8c483a3aff73e61b91d15b5687335', '74e9053d6f44f4507bd40bbea999ee65f0cbefb2', 'ca062c5e48d230d0ac51da96d492f3cb4bf82b39', '7f2d63a50a2f32caa95e20720eb310e524ff4a8d', 'c68fbc1f4aa72d30974f8a3071054e3b227137fd', 'f04df4e20a18358ea2f689b4c129781628ef7fc1', '6af58c061f2e4f130c3b795c21ff0c7e3903278f', '1f6ba0782862ec12a5ec6d7fb608523d55b0c6ba', 'd891dc72cbd40ffaeefdc79f2e7afe1e530a23ad', '53b047e503f4c24602f376a774d653f7ed56c024', '39d08fa8b028217384daeb3e622848451809a422', 'bee044c8e8903fb67523c1f8c105ab4718600cdb', '39b7fa66f34169832212ef062c58d20cb7cc5875', '6a5704ac5fdacb7121a0c02a9be4de2bdc5a40fc', 'a9487f431f87546ce36c9ffa98b6321872d9e7f1', 'fa12574c228542151ccd7d4e3f42cc4896cd274a', '5ded2b8c64491b4a67f6d39ce473d4b9347a672e', '44d2abe2175df8153f465f6c39b68b76a0d40ab9', 'b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b', 'df2b0e26d0599ce3e70df8a9da02e51594e0e992', '514e7fb769950dbe96eb519c88ca17e04dc829f6']",2020,https://semanticscholar.org/paper/7c38ed29fab5a95a0e747b0053470808efabe7fc,['DBLP'],"['https://zhijing-jin.com/files/papers/Is_BERT_Jin2020AAAI.pdf', 'https://aaai.org/ojs/index.php/AAAI/article/view/6311']",AAAI,['Computer Science'],2996851481,138048709,https://api.github.com/repos/jind11/TextFooler,27600507,TextFooler,A Model for Natural Language Attack on Text Classification and Inference,Python,2019-09-03 20:06:21,\N,0,2021-02-24 9:41:32,\N,https://github.com/jind11/TextFooler,269,68,Python,2019-09-03 20:06:21,2022-05-26 20:17:29,19,TRUE,TRUE,no homepageUrl,12,"['adversarial-attacks', 'bert', 'bert-model', 'text-classification', 'natural-language-inference', 'natural-language-processing']","# TextFooler
A Model for Natural Language Attack on Text Classification and Inference

This is the source code for the paper: [Jin, Di, et al. ""Is BERT Really Robust? Natural Language Attack on Text Classification and Entailment."" arXiv preprint arXiv:1907.11932 (2019)](https://arxiv.org/pdf/1907.11932.pdf). If you use the code, please cite the paper:

```
@article{jin2019bert,
  title={Is BERT Really Robust? Natural Language Attack on Text Classification and Entailment},
  author={Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Szolovits, Peter},
  journal={arXiv preprint arXiv:1907.11932},
  year={2019}
}
```

## Data
Our 7 datasets are [here](https://bit.ly/nlp_adv_data).

## Prerequisites:
Required packages are listed in the requirements.txt file:
```
pip install -r requirements.txt
```

## How to use

* Run the following code to install the **esim** package:

 ```
cd ESIM
python setup.py install
cd ..
```

* (Optional) Run the following code to pre-compute the cosine similarity scores between word pairs based on the [counter-fitting word embeddings](https://drive.google.com/open?id=1bayGomljWb6HeYDMTDKXrh0HackKtSlx).

```
python comp_cos_sim_mat.py [PATH_TO_COUNTER_FITTING_WORD_EMBEDDINGS]
```

* Run the following code to generate the adversaries for text classification:

```
python attack_classification.py
```

For Natural langauge inference:

```
python attack_nli.py
```

Examples of run code for these two files are in [run_attack_classification.py](https://github.com/jind11/TextFooler/blob/master/run_attack_classification.py) and [run_attack_nli.py](https://github.com/jind11/TextFooler/blob/master/run_attack_nli.py). Here we explain each required argument in details:

  * --dataset_path: The path to the dataset. We put the 1000 examples for each dataset we used in the paper in the folder [data](https://github.com/jind11/TextFooler/tree/master/data).
  * --target_model: Name of the target model such as ''bert''.
  * --target_model_path: The path to the trained parameters of the target model. For ease of replication, we shared the [trained BERT model parameters](https://drive.google.com/drive/folders/1wKjelHFcqsT3GgA7LzWmoaAHcUkP4c7B?usp=sharing), the [trained LSTM model parameters](https://drive.google.com/drive/folders/108myH_HHtBJX8MvhBQuvTGb-kGOce5M2?usp=sharing), and the [trained CNN model parameters](https://drive.google.com/drive/folders/1Ifowzfers0m1Aw2vE8O7SMifHUhkTEjh?usp=sharing) on each dataset we used.
  * --counter_fitting_embeddings_path: The path to the counter-fitting word embeddings.
  * --counter_fitting_cos_sim_path: This is optional. If given, then the pre-computed cosine similarity scores based on the counter-fitting word embeddings will be loaded to save time. If not, it will be calculated.
  * --USE_cache_path: The path to save the USE model file (Downloading is automatic if this path is empty).
  
Two more things to share with you:

1. In case someone wants to replicate our experiments for training the target models, we shared the used [seven datasets](https://drive.google.com/open?id=1N-FYUa5XN8qDs4SgttQQnrkeTXXAXjTv) we have processed for you!

2. In case someone may want to use our generated adversary results towards the benchmark data directly, [here it is](https://drive.google.com/drive/folders/12yeqcqZiEWuncC5zhSUmKBC3GLFiCEaN?usp=sharing).
",23,143,1,116,334,2016-01-07 21:33:14,https://github.com/jind11/TextFooler,68,2896,FALSE,no mirror url,2,0,0,User,MIT,jindi15@mit.edu,no twitter_username or private twitter_username,69,2021-12-17 17:34:47,"MIT License

Copyright (c) 2021 Di Jin

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the ""Software""), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
",15,2.754778693,2.861112822,2,2,TextFooler,pytorch,"<h1>textfooler</h1>
<p>a model for natural language attack on text classification and inference</p>
<p>this is the source code for the paper: <a href=""https://arxiv.org/pdf/1907.11932.pdf"">jin, di, et al. ""is bert really robust? natural language attack on text classification and entailment."" arxiv preprint arxiv:1907.11932 (2019)</a>. if you use the code, please cite the paper:</p>
<pre><code>@article{jin2019bert,
  title={is bert really robust? natural language attack on text classification and entailment},
  author={jin, di and jin, zhijing and zhou, joey tianyi and szolovits, peter},
  journal={arxiv preprint arxiv:1907.11932},
  year={2019}
}
</code></pre>
<h2>data</h2>
<p>our 7 datasets are <a href=""https://bit.ly/nlp_adv_data"">here</a>.</p>
<h2>prerequisites:</h2>
<p>required packages are listed in the requirements.txt file:</p>
<pre><code>pip install -r requirements.txt
</code></pre>
<h2>how to use</h2>
<ul>
<li><p>run the following code to install the <strong>esim</strong> package:</p>
<pre><code>cd esim
python setup.py install
cd ..
</code></pre>
</li>
<li><p>(optional) run the following code to pre-compute the cosine similarity scores between word pairs based on the <a href=""https://drive.google.com/open?id=1baygomljwb6heydmtdkxrh0hackktslx"">counter-fitting word embeddings</a>.</p>
</li>
</ul>
<pre><code>python comp_cos_sim_mat.py [path_to_counter_fitting_word_embeddings]
</code></pre>
<ul>
<li>run the following code to generate the adversaries for text classification:</li>
</ul>
<pre><code>python attack_classification.py
</code></pre>
<p>for natural langauge inference:</p>
<pre><code>python attack_nli.py
</code></pre>
<p>examples of run code for these two files are in <a href=""https://github.com/jind11/textfooler/blob/master/run_attack_classification.py"">run_attack_classification.py</a> and <a href=""https://github.com/jind11/textfooler/blob/master/run_attack_nli.py"">run_attack_nli.py</a>. here we explain each required argument in details:</p>
<ul>
<li>--dataset_path: the path to the dataset. we put the 1000 examples for each dataset we used in the paper in the folder <a href=""https://github.com/jind11/textfooler/tree/master/data"">data</a>.</li>
<li>--target_model: name of the target model such as ''bert''.</li>
<li>--target_model_path: the path to the trained parameters of the target model. for ease of replication, we shared the <a href=""https://drive.google.com/drive/folders/1wkjelhfcqst3gga7lzwmoaahcukp4c7b?usp=sharing"">trained bert model parameters</a>, the <a href=""https://drive.google.com/drive/folders/108myh_hhtbjx8mvhbquvtgb-kgoce5m2?usp=sharing"">trained lstm model parameters</a>, and the <a href=""https://drive.google.com/drive/folders/1ifowzfers0m1aw2ve8o7smifhuhktejh?usp=sharing"">trained cnn model parameters</a> on each dataset we used.</li>
<li>--counter_fitting_embeddings_path: the path to the counter-fitting word embeddings.</li>
<li>--counter_fitting_cos_sim_path: this is optional. if given, then the pre-computed cosine similarity scores based on the counter-fitting word embeddings will be loaded to save time. if not, it will be calculated.</li>
<li>--use_cache_path: the path to save the use model file (downloading is automatic if this path is empty).</li>
</ul>
<p>two more things to share with you:</p>
<ol>
<li><p>in case someone wants to replicate our experiments for training the target models, we shared the used <a href=""https://drive.google.com/open?id=1n-fyua5xn8qds4sgttqqnrketxxaxjtv"">seven datasets</a> we have processed for you!</p>
</li>
<li><p>in case someone may want to use our generated adversary results towards the benchmark data directly, <a href=""https://drive.google.com/drive/folders/12yeqcqziewuncc5zhsumkbc3glficean?usp=sharing"">here it is</a>.</p>
</li>
</ol>
",4,6,0,0,0,FALSE,0,0,FALSE,FALSE,TRUE,4,15,TRUE,FALSE,FALSE,39,8472,22,382,11,TRUE,TRUE,FALSE,is bert really robust? a strong baseline for natural language attack on text classification and entailment,1,is bert really robust? a strong baseline for natural language attack on text classification and entailment,4,21,79,50,50,100,29,1370,656,714,685,685,29,21,9,12,10.5,10.5,1.5,79,656,12,['MIT'],FALSE,79,656,12,['MIT'],FALSE,TRUE,FALSE,390,32,77,4,14,1,1,2,1,1,unknow,16,155